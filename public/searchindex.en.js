var relearn_searchindex = [
  {
    "breadcrumb": "Welcome",
    "content": "Welcome to my technical blog and knowledge base!\nTopics üñ• Threathunting Tutorials üñ• OpenBSD Latest posts Threathunting I: Network setup 08.07.2025 Open BSD and Zen 28.06.2025 Threat hunting II: SSH Honeypot setup 13.07.2025 The unseen hero of OpenBSD, talking about OpenBSD's malloc 09.12.2025 SANS FOR608 26.05.2025 Get in Touch Suggestions or feedback?\nContact me here or visit the project repository.\nYou can also subscribe via RSS.",
    "description": "Latest posts",
    "tags": [],
    "title": "Forensic wheels",
    "uri": "/posts/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Welcome to my technical blog and knowledge base!\nTopics üñ• Threathunting Tutorials üñ• All things OpenBSD Latest posts Threathunting I: Network setup 08.07.2025 Open BSD and Zen 28.06.2025 Threat hunting II: SSH Honeypot setup 13.07.2025 The unseen hero of OpenBSD, talking about OpenBSD's malloc 09.12.2025 SANS FOR608 26.05.2025 Get in Touch Suggestions or feedback?\nContact me here or visit the project repository.\nYou can also subscribe via RSS.",
    "description": "Latest posts",
    "tags": [],
    "title": "Welcome",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction Why I Built a Home Lab for Threat Hunting üïµ Network Setup Topology, Hardware and Tools üõ† Firewall configurationüß± Switch configuration What I Learned Whats next Introduction This is a small series I wanted to start, where I write about my small threathunting setup and describe a little what I build and what I am doing with it.\nIn this part, I will describe the Network setup for my Environment, more about how I build the honeypots and the ELK Server I will describe in the follow up articles about threathunting.\nKeep in mind this is for Education and fun, no serious stuff going on here.\nWhy I Built a Home Lab for Threat Hunting üïµ The threat landscape is constantly evolving, with new attack vectors, tools, and tactics appearing almost daily.\nAnd to keep my skills current with real-world threats, I built a home lab dedicated to threat hunting. This environment allows me to safely observe attacks and develop detection and defense methods. I deployed web and shell honeypots, and collect real threat data in a controlled setting.\nIt‚Äôs a practical, hands-on way to explore the behavior of adversaries and its a lot of fun!\nNetwork Setup Topology, Hardware and Tools üõ† For the hardware setup, I kept things lightweight and affordable by using Raspberry Pi devices and open-source tools. The honeypot is based on the well-known Cowrie SSH honeypot and the honeyhttpd HTTP honeypot . It runs on a Raspberry Pi 4 with 8GB of RAM, hosted inside a Docker üê≥ container. On the honeypot host, Filebeat is running to ingest the Cowrie logs into the ELK stack.\nFor the ELK stack, I used a Raspberry Pi 5 with 16GB of RAM, running Debian. The ELK services are also containerized using Docker. The stack is based on the DShield-SIEM project, which I customized to better fit my needs. I‚Äôll dive deeper into those modifications and the ELK setup in a follow-up article.\nThe network topology is straightforward but deliberately segmented. The router is connected to a managed switch, which is responsible for handling VLAN separation. Both the honeypot and the ELK server are connected to this switch and are placed in an isolated VLAN (VLAN210). This VLAN is dedicated exclusively to threat hunting, ensuring that any potentially malicious traffic remains fully contained and cannot interfere with the rest of the home network.\nMy client system üíª is the only machine allowed to connect from outside the VLAN to both the ELK server and the honeypot. This connection is strictly for maintenance and administrative purposes. The ELK server is allowed to access the internet, primarily to pull threat intelligence data from external sources and security feeds.\nIn contrast, the honeypot is completely blocked from internet access, with the exception of SSH and HTTP traffic going in and out of it. These are the only services deliberately exposed to simulate vulnerable endpoints. Communication between the honeypot and the ELK server is allowed for log ingestion and analysis. However, I intend to introduce stricter controls on this internal traffic in the future to further reduce the attack surface.\nFirewall configurationüß± For the pf(1) configuration It was as always with UNIX fairly easy to get to work:\nmatch in quick log on egress proto tcp from any to any port 22 flags S/SA rdr-to $honeypot port 2222 match in quick log on egress proto tcp from any to any port 443 flags S/SA rdr-to $honeypot port 4433 This rule makes sure any incoming TCP connection attempt to port 22 (SSH) and port 443 (HTTPS) is immediately intercepted, logged, and transparently redirected to the $honeypot server listening on port 2222 or 4433 for HTTPS Traffic.\nSwitch configuration Here you can see my managed switch configuration. Port 5 (honeypot) and port 3 (ELK) is assigned to VLAN210, port 2 is the router it needs to talk into both networks and at port 1 is my workstation to access the theathunting environment.\nWhat I Learned Building and maintaining this lightweight honeypot and monitoring setup on Raspberry Pi devices has been an insightful experience. Here are some key takeaways:\nResource Efficiency: Raspberry Pis provide a surprisingly capable platform for running complex services like Cowrie honeypot and the ELK stack in Docker containers, keeping costs and power consumption low.\nNetwork Segmentation Matters: Isolating the honeypot and ELK server in a dedicated VLAN (VLAN210) effectively contains malicious traffic, protecting the rest of the home network from potential threats.\nControlled Access Is Crucial: Restricting external access to only authorized clients and limiting the honeypot‚Äôs internet connectivity reduces the attack surface while still enabling useful data collection.\nLogging and Data Collection: Using Filebeat to ship logs from the honeypot to the ELK stack provides real-time visibility into attacker behavior, which is essential for threat hunting and incident response.\nCustomization Pays Off: Adapting existing tools and SIEM projects (like DShield) to specific needs improves effectiveness and allows for tailored threat detection.\nFuture Improvements: There is always room to tighten internal communication rules and harden the setup further to minimize risk and improve operational security.\nThis project highlights the balance between practical constraints and security needs, demonstrating that even modest hardware can contribute significantly to threat intelligence and network defense.\nI drew inspiration for this setup from the DShield SIEM project by SANS and would like to express my gratitude for their valuable work.\nWhats next Next I had to build the ssh honeypot and the HTTP Honeypot, stay tuned for the follow up!",
    "description": "Introduction Why I Built a Home Lab for Threat Hunting üïµ Network Setup Topology, Hardware and Tools üõ† Firewall configurationüß± Switch configuration What I Learned Whats next Introduction This is a small series I wanted to start, where I write about my small threathunting setup and describe a little what I build and what I am doing with it.",
    "tags": [
      "Threathunting",
      "Honeypot",
      "Visibility"
    ],
    "title": "Threathunting I: Network setup",
    "uri": "/posts/threathuntingnet/index.html"
  },
  {
    "breadcrumb": "Welcome",
    "content": "Hi, I‚Äôm Dirk ‚Äî a security engineer with a deep passion for skateboarding and digital forensics.\nSkateboarding is more than a hobby to me; it‚Äôs a source of creativity, freedom, and community. It shapes how I approach challenges ‚Äî with persistence, balance, and a mindset open to innovation.\nBeyond that, I‚Äôm an OpenBSD enthusiast. I‚Äôve built an OpenBSD-based router and threat-hunting infrastructure to stay ahead in cybersecurity. I appreciate OpenBSD for its simplicity, security, and elegance ‚Äî qualities I strive to bring to my work.\nI‚Äôm also a longtime Emacs user, relying on it daily for coding, writing, and organizing my thoughts. It‚Äôs part of how I stay productive and focused.\nIn cybersecurity, I‚Äôm committed to continuous growth and adapting to new challenges. When I‚Äôm not working on security projects, you‚Äôll find me skating or exploring new ideas inspired by Zen philosophy.\nYou can download my CV as a signed and encrypted PDF for authenticity and privacy. If you need the password to decrypt it, please send me an E-mail\nStay tuned for updates on my journey as a security engineer, skateboarder, and lifelong learner.\nKey ID: `0xC2920C559CAD6CB` Fingerprint: `40CA 727E 96D3 CC2D 8CBB 1540 0C29 20C5 59CA D6CB` SHA-256 Hash: `c7359e0e8bd69ed7cee3ea97453c10e327bfe2416822f54c6390efe72b0d6e7a` publickey",
    "description": "Short intro about myself",
    "tags": [
      "Forensicwheels",
      "Personal"
    ],
    "title": "about",
    "uri": "/about/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "About As someone who is passionate about security and has an interest in Unix operating systems, OpenBSD particularly captivates due to its dedication to security, stability, and simplicity. In comparison to other OSes, what sets OpenBSD apart? And how do these principles align with my journey through Zen meditation?\nAt first glance, OpenBSD and Zen may appear to be vastly disparate concepts - one being a potent operating system, while the other is a spiritual practice originating from ancient China. However, as I delved deeper into both realms, I uncovered some fascinating similarities.\nSimplicity and Clarity In Zen, simplicity is key to achieving inner clarity and balance. By stripping away unnecessary complexity, OpenBSD aims to create a stable and secure foundation for users. Similarly, in meditation, simplicity helps to quiet the mind and focus on the present moment. This alignment between OpenBSD‚Äôs philosophy and Zen practices extends to their shared emphasis on mindfulness and deliberate decision-making, fostering an environment of security and tranquility in both realms.\nAttention to Detail Both OpenBSD and Zen underscore the significance of attending to detail. In software development, this entails meticulously crafting each line of code to guarantee stability and security. In Zen practice, it involves paying close attention to one‚Äôs breath, posture, and mental state to attain a state of mindfulness. By zeroing in on these details, both OpenBSD and Zen strive for perfection.\nThe Power of Consistency OpenBSD‚Äôs dedication to consistency is manifested in its codebase, where each code change undergoes a thorough code review process. Consistency holds equal importance in Zen practice, as it fosters a sense of routine and stability. By cultivating a consistent daily meditation practice, I have discovered that consistency is instrumental in making progress on my spiritual journey. OpenBSD‚Äôs emphasis on consistency mirrors the principles of Zen, emphasizing the value of diligence and discipline in both domains.\nThe Beauty of Imperfection Finally, both OpenBSD and Zen acknowledge the elegance in imperfection. In software development, imperfections can often be rectified or lessened through meticulous design and testing. In Zen practice, imperfections are perceived as avenues for growth and self-awareness.\nBy acknowledging our imperfections, we can nurture humility and compassion. As I progress in my journey with OpenBSD and Zen, I am consistently struck by the ways in which these two seemingly unrelated realms intersect. By embracing simplicity, attention to detail, consistency, and the beauty of imperfection, both OpenBSD and Zen provide unique perspectives on the nature of software development and personal growth. Stay tuned for further insights from my exploration in the realm of security!",
    "description": "About As someone who is passionate about security and has an interest in Unix operating systems, OpenBSD particularly captivates due to its dedication to security, stability, and simplicity. In comparison to other OSes, what sets OpenBSD apart? And how do these principles align with my journey through Zen meditation?\nAt first glance, OpenBSD and Zen may appear to be vastly disparate concepts - one being a potent operating system, while the other is a spiritual practice originating from ancient China. However, as I delved deeper into both realms, I uncovered some fascinating similarities.",
    "tags": [
      "Forensicwheels",
      "Openbsd",
      "Zen"
    ],
    "title": "Open BSD and Zen",
    "uri": "/posts/openbsdzen/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction What is Cowrie? Why Podman over Docker? Preconditions / System setup Ubuntu Installed on Raspberry Pi 4+ System Fully Updated Podman installed and working VLAN Tagging Configured on Network Interface Setup environment, install cowrie as container and adjust configuration üêß Create a Dedicated User for Cowrie (No Login Shell) üê≥ Pull and Configure Cowrie with Podman üõ† cowrie.cfg ‚Äì Basic Overview üöÄ Run Cowrie Container as ‚Äòcowrie‚Äô User üéØ Operating the Honeypot üîÑ Automatically Restart Cowrie Podman Container with systemd üîí Security Notes Log Forwarding with Filebeat üì¶ Install Filebeat on Ubuntu ‚öô Configure and test Filebeat üöÄ Start and Enable Filebeat üéØ TL;DR ‚Äì What Did We Just Do? Whats next Introduction This post provides a brief walkthrough of how to deploy a lightweight, containerized SSH honeypot using Cowrie and Podman, with the goal of capturing and analyzing malicious activity as part of my threat hunting strategy.\nWhat is Cowrie? Cowrie is an interactive SSH and Telnet honeypot designed to emulate a real system, capturing attacker behavior in a controlled environment. It allows defenders and researchers to observe malicious activity without exposing actual infrastructure.\nKey capabilities of Cowrie include\nFull session logging: Records all commands entered by the attacker, along with input/output streams and timing data. Sessions can be saved as plaintext or in formats suitable for replay.\nFake file system and shell environment: Emulates a basic Linux shell with a user-modifiable file system. Attackers can navigate directories, read/write fake files, or attempt to download/upload payloads.\nCommand emulation: Supports a large set of common Unix commands (`ls`, `cat`, `wget`, etc.), allowing attackers to interact naturally, as if on a real system. And can be extended with more commands\nCredential logging: Captures usernames and passwords used in brute-force login attempts or interactive logins.\nFile download capture: Logs and optionally stores any files attackers attempt to retrieve via `wget`, `curl`, or similar tools.\nJSON-formatted logging and integration‚Äôs: Outputs structured logs that are easy to parse and ingest into systems like ELK, Splunk, or custom analysis pipelines.\nCowrie is widely used in research, threat intelligence, and proactive defense efforts to gather Indicators of Compromise (IOCs) and understand attacker tactics,techniques, and procedures (TTPs).\nWhy Podman over Docker? Podman offers several advantages over Docker, particularly in terms of security and system integration. It supports rootless containers, allowing users to run containers without elevated privileges, which reduces the attack surface.\nPodman is daemon-less, integrating more seamlessly with systemd and existing Linux workflows. Additionally, Podman is fully compatible with the Open Container Initiative (OCI) standards, ensuring interoperability and flexibility across container ecosystems.\nPreconditions / System setup Before I proceed with the cowrie setup, I made sure the following preconditions are met:\nUbuntu Installed on Raspberry Pi 4+ I am using a Raspberry Pi 4+ running Ubuntu\nSystem Fully Updated After installation, I made sure system is up to date:\nsudo apt update \u0026\u0026 sudo apt upgrade -y Podman installed and working # Ubuntu 20.10 and newer sudo apt-get -y install podman Run the Hello World Container.In this moment I did not had the cowrie user yet setup so I used my system user to test\npodman run hello-world Trying to pull docker.io/library/hello-world:latest... ... Hello from Docker! This message shows that your installation appears to be working correctly. tho sometimes the pulling fails like that then I had to put `docker.io` in front of the container name like:\npodman run docker.io/hello-world then it would work for sure.\nVLAN Tagging Configured on Network Interface In my network setup for threathunting the honeypot requires VLAN tagging to configured to reachable from the outside, VLAN210 is my restricted Network. Therefore i needed to configure the vlan using nmcli so it‚Äôs persistent across reboots.\nExample: Create a VLAN interface (e.g., VLAN ID 210 on main if) sudo nmcli con add type vlan con-name vlan210 dev mainif id 210 ip4 192.168.210.3/24 gw4 192.168.210.1 sudo nmcli con up vlan210 con-name vlan210: Name of the new VLAN connection. dev mainif: Physical interface to tag. id 210: VLAN ID. ip4, gw4: Optional IP and gateway assignment. This will persist the configuration and activate the VLAN interface immediately. Next I moved on to Install the honeypot.\nSetup environment, install cowrie as container and adjust configuration üêß Create a Dedicated User for Cowrie (No Login Shell) Running the Podman container under a dedicated system user with no login shell is a recommended security best practice. Reasons include:\nPrivilege Separation: Isolates the container from other system processes and users, limiting the potential impact of a compromise.\nReduced Attack Surface: The user has no login shell (e.g., /usr/sbin/nologin), meaning it can‚Äôt be used to log into the system interactively.\nAuditing \u0026 Logging: Helps distinguish container activity in system logs and process lists, making monitoring easier.\nLeast Privilege Principle: The user has only the permissions necessary to run the container ‚Äî nothing more.\n1. Create the ‚Äòcowrie‚Äô user (no home directory, no login shell)\nsudo useradd --system --no-create-home --shell /usr/sbin/nologin cowrie 2. Create necessary directories and set ownership\nsudo mkdir -p /opt/cowrie/etc sudo mkdir -p /opt/cowrie/var sudo mkdir -p /opt/cowrie/var/log/cowrie sudo chown -R cowrie:cowrie /opt/cowrie üê≥ Pull and Configure Cowrie with Podman 3. As the cowrie user, pull the container image\nsudo -u cowrie podman pull docker.io/cowrie/cowrie 4. Copy default config file into persistent volume\nsudo -u cowrie podman run --rm localhost/cowrie_honeypot:latest \\ cat /cowrie/cowrie-git/etc/cowrie.cfg.dist \u003e /opt/cowrie/etc/cowrie.cfg üõ† cowrie.cfg ‚Äì Basic Overview The `cowrie.cfg` file is the main configuration for Cowrie, the SSH/Telnet honeypot we use. It uses INI-style syntax and is divided into sections. Each section begins with a header like [section_name].\nüìÅ Key Sections \u0026 Settings\n[ssh]\nEnable or disable SSH/Telnet and set the port to listen on:: enabled = true listen_port = 2222 [honeypot]\nSet honeypot host name and logpath properties:\nhostname = cowrie-host # Directory where to save log files in. log_path = var/log/cowrie Define login behavior:\nauth_class = AuthRandom auth_class_parameters = 1, 5, 10 I use AuthRandom here which causes to allow access after ‚Äúrandint(2,5)‚Äù attempts. This means the threat actor will fail with some logins and some will be logged in immediately.\n[output_jsonlog]\nConfigure logging and output plugins: [output_jsonlog] enabled = true logfile = ${honeypot:log_path}/cowrie.json epoch_timestamp = false This sets the default log location in the file-system, this is important so that file beat later can pickup on the juicy honeypot log files. This is the whole configuration needed to run the honeypot.\nüìå Notes\nRestart Cowrie after configuration changes. The configuration can be split across multiple `.cfg` files in `cowrie.cfg.d/` for modular setup. üöÄ Run Cowrie Container as ‚Äòcowrie‚Äô User Once I had created the dedicated system user (see earlier section), I was able to run the Cowrie container with Podman using sudo -u and UID mapping.\nStep-by-Step Command explanation sudo -u cowrie podman run -d --name cowrie \\ --uidmap 0:999:1001 \\ -v /opt/cowrie/etc:/cowrie/cowrie-git/etc:Z \\ -v /opt/cowrie/var:/cowrie/cowrie-git/var:Z \\ -p 2222:2222 \\ cowrie/cowrie Explanation sudo -u cowrie: Runs the Podman command as the unprivileged cowrie user. --uidmap 0:999:1001: Maps root (UID 0) inside the container to the cowrie UID on the host. -v /opt/cowrie/etc and /opt/cowrie/var: Mounts configuration and data volumes from the host with `:Z` to apply correct SELinux labels (optional on systems without SELinux). -p 2222:2222: Forwards port 2222 from host to container (Cowrie‚Äôs SSH honeypot port). cowrie/cowrie: The container image name (use latest or specific tag as needed). Benefits: Container runs as non-root on the host: Even if a process inside the container thinks it‚Äôs root, it‚Äôs actually limited to the unprivileged cowrie user outside the container.\nEnhanced security: If the container is compromised, the attacker only gets access as the cowrie user ‚Äî not real root.\nAvoids root-equivalent risks: Prevents privilege escalation or access to sensitive host files and devices.\nüéØ Operating the Honeypot View logs I think to know how to debug the container is important so we start first with the logs:\nsudo -u cowrie podman logs -f cowrie ...snip... [HoneyPotSSHTransport,14,10.0.2.100] Closing TTY Log: var/lib/cowrie/tty/e52d9c508c502347344d8c07ad91cbd6068afc75ff6292f062a09ca381c89e71 after 0.8 seconds [cowrie.ssh.connection.CowrieSSHConnection#info] sending close 0 [cowrie.ssh.session.HoneyPotSSHSession#info] remote close [HoneyPotSSHTransport,14,10.0.2.100] Got remote error, code 11 reason: b'disconnected by user' [HoneyPotSSHTransport,14,10.0.2.100] avatar root logging out [cowrie.ssh.transport.HoneyPotSSHTransport#info] connection lost [HoneyPotSSHTransport,14,10.0.2.100] Connection lost after 2.8 seconds ...snip... Restart container If things go left just restart that thing:\nsudo -u cowrie podman restart cowrie In the logs you can see that cowrie is running and accepting SSH connections:\n...snip... [-] CowrieSSHFactory starting on 2222 [cowrie.ssh.factory.CowrieSSHFactory#info] Starting factory \u003ccowrie.ssh.factory.CowrieSSHFactory object at 0x7fb66f26d0\u003e [-] Ready to accept SSH connections ...snip... When the log says ‚ÄúReady to accept SSH connections‚Äù I tested if I could login:\nssh 192.168.210.3 -p 2222 -l root root@192.168.210.3 password: The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. root@svr04:~# uname -a Linux svr04 3.2.0-4-amd64 #1 SMP Debian 3.2.68-1+deb7u1 x86_64 GNU/Linux root@svr04:~# Stop container Nothing special here:\nsudo -u cowrie podman stop cowrie üîÑ Automatically Restart Cowrie Podman Container with systemd To keep your Cowrie container running reliably and restart it if it stops, use a systemd service with restart policies. Please make sure to double check this part on your side as I am no systemd expert at all, for me this just worked.\nStep 1: Generate a systemd Service File Create `/etc/systemd/system/cowrie-container.service` with the following content: You can create the systemd file with the command:\nsudo -u cowrie podman generate systemd --name cowrie --files --restart-policy=on-failure The resulting file looks somewhat like this\n# container-cowrie.service # autogenerated by Podman 4.3.1 # Fri Sep 19 10:27:47 CEST 2025 [Unit] Description=Podman container-cowrie.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=/run/user/1001/containers [Service] User=cowrie Group=cowrie Restart=on-failure Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman start -a cowrie ExecStop=/usr/bin/podman stop -t 10 cowrie ExecStopPost=/usr/bin/podman stop -t 10 cowrie Type=forking [Install] WantedBy=default.target The `‚Äìrestart-policy=on-failure` makes systemd restart the container if it exits with a failure. Step 2: Enable the Service sudo systemctl daemon-reload sudo systemctl enable --now cowrie-container.service Step 3: (Optional) Add a Health Check Script To detect if Cowrie stops accepting connections even if the container is still running, create a health check script running as cowrie:\nCreate `/usr/local/bin/check_cowrie.sh`:\n#!/bin/bash if ! nc -z localhost 2222; then echo \"Cowrie not responding, restarting container\" /usr/bin/podman restart cowrie /usr/local/bin/pushover.sh \"Cowrie was restarted!\" fi This restarts the service and sends out a notification via pushover.\nMake it executable:\nsudo chmod +x /usr/local/bin/check_cowrie.sh sudo chown cowrie:cowrie /usr/local/bin/check_cowrie.sh Create systemd service `/etc/systemd/system/check_cowrie.service`:\n[Unit] Description=Check Cowrie honeypot health [Service] User=cowrie Group=cowrie Type=oneshot ExecStart=/usr/local/bin/check_cowrie.sh Create systemd timer `/etc/systemd/system/check_cowrie.timer`:\n[Unit] Description=Run Cowrie health check every minute [Timer] OnBootSec=1min OnUnitActiveSec=1min Unit=check_cowrie.service [Install] WantedBy=timers.target Enable and start the timer:\nsudo systemctl daemon-reload sudo systemctl enable --now check_cowrie.timer Summary Used Podman‚Äôs systemd integration for automatic restart on container failure. Added a health check timer to detect if Cowrie stops accepting connections and restart proactively. üîí Security Notes The `cowrie` user has no login shell (`/usr/sbin/no login`)\nRunning Cowrie isolated via Podman increases containment\nAll files are owned by `cowrie`, no root access required for normal operation\nLog Forwarding with Filebeat üì¶ Install Filebeat on Ubuntu 1. Add Elastic‚Äôs GPG key and repository\ncurl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elastic.gpg echo \"deb [signed-by=/usr/share/keyrings/elastic.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\" | \\ sudo tee /etc/apt/sources.list.d/elastic-8.x.list 2. Update APT and install Filebeat\nsudo apt install filebeat ‚öô Configure and test Filebeat 3. Edit Filebeat config\nsudo mg /etc/filebeat/filebeat.yml The filebeat config is straight forward. You have to write a filebeat.input block which contains the path where the logfiles are you need to ingest. And at the end the log-destination (logstash) so that filebeat knows where to send the logs to:\nfilebeat.inputs: - type: log enabled: true paths: - /opt/cowrie/var/log/cowrie/cowrie.json json.keys_under_root: true json.add_error_key: true fields: source: cowrie fields_under_root: true output.logstash: hosts: [\"192.168.123.5:5044\"] 4. (Optional) Test Filebeat config\nsudo filebeat test config logstash: 192.168.210.5:5044... connection... parse host... OK dns lookup... OK addresses: 192.168.210.5 dial up... OK TLS... WARN secure connection disabled talk to server... OK üöÄ Start and Enable Filebeat 5. Enable and start Filebeat\nsudo systemctl enable filebeat sudo systemctl daemon-reload sudo systemctl start filebeat 6. Check Filebeat status and logs\nsudo systemctl status filebeat sudo journalctl -u filebeat -f üéØ TL;DR ‚Äì What Did We Just Do? 1. We deployed Cowrie like pros.\nRan it safely in a Podman container under a non-login user. No mess, no root, no regrets. 2. Logs? Sorted.\nFilebeat scooped up Cowrie‚Äôs logs and shipped them to Elasticsearch. Now we can actually see who‚Äôs knocking on the honeypot door. 3. Everything‚Äôs persistent.\nConfigs and logs live outside the container. Cowrie forgets nothing‚Äîeven after a reboot. 4. Setup is clean and modular.\nEach part (Cowrie, Filebeat, Elasticsearch) does its job. Break one, fix one‚Äîno domino disasters. 5. It‚Äôs nerdy, useful, and kinda fun.\nNow I built a mini threat intel system. Now I can sit back, sip coffee, and watch the kiddies play. Whats next Next I build the HTTP Honeypot",
    "description": "Introduction What is Cowrie? Why Podman over Docker? Preconditions / System setup Ubuntu Installed on Raspberry Pi 4+ System Fully Updated Podman installed and working VLAN Tagging Configured on Network Interface Setup environment, install cowrie as container and adjust configuration üêß Create a Dedicated User for Cowrie (No Login Shell) üê≥ Pull and Configure Cowrie with Podman üõ† cowrie.cfg ‚Äì Basic Overview üöÄ Run Cowrie Container as ‚Äòcowrie‚Äô User üéØ Operating the Honeypot üîÑ Automatically Restart Cowrie Podman Container with systemd üîí Security Notes Log Forwarding with Filebeat üì¶ Install Filebeat on Ubuntu ‚öô Configure and test Filebeat üöÄ Start and Enable Filebeat üéØ TL;DR ‚Äì What Did We Just Do? Whats next Introduction This post provides a brief walkthrough of how to deploy a lightweight, containerized SSH honeypot using Cowrie and Podman, with the goal of capturing and analyzing malicious activity as part of my threat hunting strategy.",
    "tags": [
      "Threathunting",
      "Honeypot"
    ],
    "title": "Threat hunting II: SSH Honeypot setup",
    "uri": "/posts/theathuntinghoneypot/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "The unseen hero of OpenBSD: talking about OpenBSD‚Äôs malloc Introduction When people talk about operating system security, they often mention firewalls, cryptography, or privilege separation. But one of the most important security components usually remains invisible: the memory allocator. On OpenBSD, the default allocator‚Äîmalloc‚Äîis not just an implementation detail. It reflects the project‚Äôs long-standing commitment to robustness and safety. This article explains, at a high level, why OpenBSD‚Äôs malloc is so unusual, so protective, and so ‚Äúunseen yet essential‚Äù.\nWhy the allocator matters Every non-trivial program allocates memory dynamically. Buffer overflows, use-after-free bugs, double frees, and integer overflows are among the most common vulnerability classes discovered in C and C++ software. How an allocator reacts to such mistakes‚Äîsilently, or by crashing loudly‚Äîmakes a huge difference in the exploitability of bugs. OpenBSD‚Äôs malloc is designed to turn subtle bugs into immediate, detectable failures rather than exploitable conditions.\nA brief history of OpenBSD malloc From early BSD roots to stronger isolation OpenBSD originally inherited a BSD-family allocator with sbrk()-based heap expansion. This traditional design grouped memory into one contiguous, predictable region‚Äîefficient, but not ideal for security.\n2001: mmap everywhere Thierry Deval rewrote malloc to use mmap() instead of sbrk(). This enabled page-aligned allocations, fine-grained memory protection, and natural integration with address-space randomization. It was the first major step toward a modern, defensive allocator. Source: malloc.conf(5)\n2008: The Otto Moerbeek rewrite In 2008, Otto Moerbeek introduced a nearly complete redesign of malloc. This is the allocator OpenBSD uses today. It emphasizes safety, randomness, metadata integrity, and strong failure modes. It is often called ‚Äúotto-malloc‚Äù. Source: Summary of malloc evolution\nArchitectural principles of OpenBSD malloc OpenBSD malloc is built on a set of design decisions that strongly influence its security posture:\nmmap for everything: Allocations come from mmap‚Äôd regions, not a single heap. This creates natural separation, unpredictable placement, and eliminates many traditional heap-exploitation techniques. Randomized layout: Allocation sizes, reuse patterns, and chunk placement are randomized. Attackers cannot reliably predict where objects land in memory. Out-of-band metadata: Metadata is not stored next to user data. Classic heap attacks often rely on corrupting bookkeeping structures; here, that avenue is largely closed. Optional guard pages: Guard pages are unmapped pages placed around allocations. An overflow into a guard page triggers an immediate crash, revealing bugs early. Junk filling (memory poisoning): Freed memory can be filled with patterns that make use-after-free bugs fail loudly instead of silently corrupting memory. Free-unmap for larger allocations: Large allocations, when freed, can be returned directly to the kernel. Any subsequent access results in a crash, revealing use-after-free misuse. Fail fast philosophy: When inconsistencies are detected‚Äîcorrupted metadata, impossible bounds, invalid free patterns‚Äîmalloc aborts the process. While harsh, this approach removes ambiguity and eliminates entire classes of silent corruption bugs. These features combine to create a defensive architecture that reduces the predictability and exploitability of memory corruption issues.\nA process-centric analogy: memory as isolated workspaces The model we usually assume Many developers implicitly imagine memory as a shared workspace inside a process: one large area where objects are placed next to each other, managed by a fast but trusting allocator. This mental model is close to how traditional heaps work and explains why many memory bugs remain invisible for a long time.\nThe traditional allocator: shared desks in one open office In a conventional allocator design, the process receives one large, contiguous heap. Individual allocations are like desks in a single open office:\nDesks are adjacent. Bookkeeping notes are pinned directly to the desks. Moving past the edge of your desk means bumping into your neighbor‚Äôs space. If a program writes past the end of an allocation, it usually lands in another valid object. The program keeps running, but the logical state is now corrupted. Exploitation thrives on this ambiguity.\nThe OpenBSD allocator: isolated workspaces with access control OpenBSD‚Äôs malloc enforces a very different model.\nEach allocation is treated as an isolated workspace:\nBacked by its own mmap()‚Äôd region Page-aligned and unpredictably placed Surrounded, when configured, by unmapped guard pages With metadata stored out-of-band Instead of one shared office, the process now consists of many small, isolated rooms. Some rooms are intentionally empty and inaccessible.\nFrom the program‚Äôs perspective, nothing changes:\nvoid *p = malloc(4096); But the execution environment is fundamentally more hostile to mistakes.\nFailure modes as design signals Out-of-bounds writes\nTraditional allocator:\nWrites land in a neighboring object Corruption propagates silently OpenBSD malloc:\nWrites cross into an unmapped page The CPU raises a fault The process terminates immediately Use-after-free\nTraditional allocator:\nFreed memory is quickly reused Old pointers appear to ‚Äúwork‚Äù State corruption accumulates OpenBSD malloc:\nMemory may be unmapped or junk-filled Old pointers reliably fault Bugs become reproducible Metadata corruption\nTraditional allocator:\nMetadata lives next to user data Overwrites alter allocator behavior OpenBSD malloc:\nMetadata is inaccessible to user code Integrity checks fail fast The allocator aborts the process Why this model matters This design changes the economics of exploitation:\nObjects are not laid out contiguously Memory reuse is unpredictable Metadata is unreachable Undefined behavior collapses into defined failure\nOpenBSD malloc does not attempt to mask programmer errors. Instead, it enforces a strict contract: memory misuse results in immediate termination. For developers and security engineers, this turns entire classes of heap bugs from latent security risks into actionable crashes.\nGuard Pages: Practical use and effects One of OpenBSD malloc‚Äôs most notable security features is the use of Guard Pages. Guard Pages are completely unmapped memory pages placed around an allocation. Any read or write into these pages triggers an immediate Segmentation Fault, making overflows or out-of-bounds accesses immediately visible.\nEnabling Guard Pages HSet a systemwide reduction of the cache to a quarter of the default size and use guard pages (man malloc):\n# sysctl vm.malloc_conf='G\u003c\u003c' Other options include:\n`redzone`: Defines padding around small allocations to catch small overflows. `junk`: Determines whether freed memory is filled with junk to detect use-after-free errors. `jumbo`: Threshold for large allocations to be immediately `munmap()`‚Äôed when freed. `alignment`: Adjusts alignment for allocations, useful for performance or hardware-specific requirements. Effects on userland programs Increased memory usage: Each allocation may require extra pages, increasing overall memory consumption. Immediate bug detection: Buffer overflows or writes beyond allocated memory result in a crash rather than silent corruption. Compatibility considerations: Programs that assume contiguous memory may crash unexpectedly with guard pages enabled. Debug vs. Production: Guard Pages are typically enabled in debug builds and often disabled in production to conserve memory. Example A C program with a buffer overflow:\nchar *buf = malloc(16); buf[16] = 'x'; // off-by-one! With guard pages, this immediately triggers a Segmentation Fault.\nFurter mentionworthy OpenBSD malloc Options OpenBSD‚Äôs malloc provides several options to help detect common memory errors such as use-after-free, buffer overflows, and double frees. These options are configured system-wide via sysctl vm.malloc_conf.\nGuard Pages (G)\nEffect: Places unmapped ‚Äúguard pages‚Äù around larger allocations. Any access triggers an immediate segmentation fault. Usage: # Enable guard pages sysctl vm.malloc_conf='G' Purpose: Detects overflows and large out-of-bounds memory access immediately. Junk Filling (J)\nEffect: Freed memory is filled with a recognizable pattern (0xAB), making use-after-free accesses immediately apparent. Usage: sysctl vm.malloc_conf='J' Purpose: Makes use-after-free bugs crash or produce detectable corruption. Redzones (R)\nEffect: Adds small padded areas around small allocations to catch off-by-one errors and small buffer overflows. Usage: sysctl vm.malloc_conf='R' Purpose: Early detection of minor memory boundary violations. Jumbo Free / Munmap (U)\nEffect: Large allocations exceeding the ‚Äújumbo‚Äù threshold are unmapped immediately on free. Any subsequent access causes an immediate crash. Usage: sysctl vm.malloc_conf='U' Purpose: Detects use-after-free errors on large memory blocks. Combining Flags\nFlags can be combined to enable multiple safety mechanisms at once. Example:\n# Enable Guard Pages, Junk Filling, and Redzones simultaneously sysctl vm.malloc_conf='GJR' Additional Useful Flags\n`C` ‚Üí Enables malloc call statistics (useful for debugging/analysis) `\u003c` / `\u003e` ‚Üí Adjusts cache size (trading memory footprint vs. performance) Practical Tips\nFor fuzzing or development, `GJR` is a strong combination to catch common memory errors early. In production, consider enabling only selective flags to reduce memory overhead and performance impact. Developer Tips \u0026 Advanced Options OpenBSD malloc‚Äôs defensive features can guide developers and improve code quality:\nFuzzing \u0026 testing: Guard Pages, junk filling, and redzones make memory bugs detectable early, improving fuzzing results. Integration with ASLR: mmap-based allocations are highly randomized, making heap exploits difficult. Fail-fast behavior: Errors like double frees or metadata corruption result in process aborts, allowing developers to reproduce bugs deterministically. Memory footprint: Defensive features increase memory usage; consider this in memory-constrained environments. Debug vs. release builds: Developers often enable maximum security options during development and limit them in production for performance. Comparison with other allocators Feature / Allocator OpenBSD malloc (otto) glibc malloc jemalloc / tcmalloc Heap source mmap only mixed sbrk/mmap custom arenas, mostly mmap Metadata location out-of-band in-band (typical) in-band Randomization high limited or optional varies Guard pages optional via config rarely default rarely default Use-after-free detection strong (junk+unmap) limited limited Overflow detection canaries+guard opt depends on debug mode not default Failure mode abort on inconsistency undefined/continuing undefined/continuing Performance priority safety \u003e speed speed speed and fragmentation Default security posture hardened by design performance-oriented performance-oriented Why OpenBSD malloc stands out OpenBSD‚Äôs malloc is not simply a defensive allocator; it is a reflection of the project‚Äôs development philosophy:\nBugs should be caught early and loudly. Small implementation, understandable design. Security features should be on by default. Memory safety belongs at the system level, not only in tools. This design makes many heap-corruption exploits impractical and forces common programming mistakes into the open, where developers can fix them.\nConclusion OpenBSD‚Äôs malloc may be invisible to most users, but it represents one of the project‚Äôs most impressive engineering achievements. By combining mmap-only allocations, randomization, guard pages, out-of-band metadata, and strict fail-fast behavior, it delivers a level of robustness and security rarely found in a general-purpose operating system.\nIf memory safety matters to you‚Äîand it should‚ÄîOpenBSD‚Äôs malloc is worth knowing and appreciating. It is a quiet guardian, hardening software in ways that few users ever see.\nReferences / Sources OpenBSD malloc manual malloc.conf documentation Otto Moerbeek‚Äôs malloc design talk Summary of OpenBSD malloc evolution Why OpenBSD Rocks: malloc randomization overview OpenBSD Security FAQ: Heap Security",
    "description": "The unseen hero of OpenBSD: talking about OpenBSD‚Äôs malloc Introduction When people talk about operating system security, they often mention firewalls, cryptography, or privilege separation. But one of the most important security components usually remains invisible: the memory allocator. On OpenBSD, the default allocator‚Äîmalloc‚Äîis not just an implementation detail. It reflects the project‚Äôs long-standing commitment to robustness and safety. This article explains, at a high level, why OpenBSD‚Äôs malloc is so unusual, so protective, and so ‚Äúunseen yet essential‚Äù.",
    "tags": [
      "Forensicwheels",
      "Openbsd"
    ],
    "title": "The unseen hero of OpenBSD, talking about OpenBSD's malloc",
    "uri": "/posts/openbsdmalloc/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Enterprise Threat hunting and Response (FOR608) Introduction Brief overview of forensic analysis and its application Course Overview/ Preparing your Index Proactive Detection and Response (608.1) Scaling Response and Analysis (608.2) Modern Attacks against Windows and Linux FIR (608.3) Analyzing macOS and Docker Containers (608.4) Cloud Attacks and Response (608.5) Capstone: Enterprise-Class IR Challenge Key Takeaways Summary of key concepts and skills learned during the course learning outcomes and their application in real-world scenarios Conclusion and Recommendations Summary of overall effectiveness of the SANS Forensics course for608 Recommendations for future students looking to learn forensic analysis skills Enterprise Threat hunting and Response (FOR608) Course description from SANS¬†1 :\nFOR608: Enterprise-Class Incident Response \u0026 Threat Hunting focuses on identifying and responding to incidents too large to focus on individual machines. By using example tools built to operate at enterprise-class scale, students learn the techniques to collect focused data for incident response and threat hunting, and dig into analysis methodologies to learn multiple approaches to understand attacker movement and activity across hosts of varying functions and operating systems by using an array of analysis techniques.\nIntroduction Brief overview of forensic analysis and its application Forensic analysis in computer science investigates digital evidence to solve cybercrimes and security incidents. In enterprise environments, it involves analyzing devices, networks, and cloud storage. Key applications include incident response, compliance with regulations, investigations, and predictive analytics.\nTools like Timesketch, Velociraptor or Wireshark, and cloud forensics platforms aid in the analysis. Collaboration between IT and law enforcement is also crucial for successful investigations.\nThe goal of forensic analysis is to reconstruct events, identify perpetrators, and determine damage extent, ensuring organizations can respond effectively to security threats and maintain compliance with regulations.\nCourse Overview/ Preparing your Index The course was booked by my employer in the on demand version, so I got access to the SANS on demand platform, so I could learn self paced. For good preparation, I read this guides on how to create a exam index:\nhttps://tisiphone.net/2015/08/18/giac-testing/ https://www.muratbekgi.com/indexing-giac/ The exams are open book and so you have to create a index for:\nIt helps you quickly locate answers in your official SANS course books. It saves valuable time during the exam. personalized knowledge map It reinforces your understanding while building it The core of the index is a sorted list of terms, concepts, or attack types, with book and page numbers e.g:\nTerm Book Page Active Directory 608.1 45 ARP Spoofing 608.2 112 Buffer Overflow 608.5 16 XOR Encryption 608.4 154 Proactive Detection and Response (608.1) The FOR608 course start with discussing current cyber defense concerns and the importance of collaboration among incident responders and threat hunters. There is a emphasize to use to shared knowledge from sources like the MITREATT\u0026CK framework and further explores the concept of active defense, like the use of honeypots, honey tokens, and canaries to slow down attackers and facilitate detection.\nFor case of a compromise, the materials focus on efficiently handling of intrusions, by covering topics such as leading the response, managing team members, documenting findings, and communicating with stakeholders.\nAurora documentation tool is introduced as a means for tracking the investigation phases from initial detection to remediation.\nLater chapter dives into a scenario where an alert gets triggered in a company network, then in the labs triage data is analyzed using Timesketch, a powerful platform for scalable and collaborative analysis of forensic data.\nAdditionally, techniques are shared for visualising the same data set with Kibana, which offers capabilities such as creating dashboards and saved searches to aid analysis.\nThe Chapter concludes by examining key threat intelligence concepts, including developing and implementing internal threat intelligence. External projects like MITRE ATT\u0026CK and Sigma are leveraged, and two comprehensive threat intel platforms, MISP and OpenCTI, are introduced.\nA threat intel report on the adversary targeting Stark Research Labs is used for intelligence to kick off the investigation into potential signs of intrusion in the company.\nScaling Response and Analysis (608.2) The course continues from chapter 1 by focusing on response actions. The Instructors show how to collect evidence at scale to scope a potential intrusion by leveraging EDR tooling data from EDR Solutions like Sysmon.\nHowever, they also discuss common bypass techniques that attackers use to evade EDR technology. To aid in this analysis, Velociraptor is introduced as a powerful platform for incident response and threat hunting.\nThen the chapter continuses to show how Velociraptor can collect forensic artifacts from across the enterprise and provide deep-dive capabilities into individual hosts of interest. Additionally, Elasticsearch is used to ingest and process data from various tools, allowing for fast searches and aggregations. I also learned about rapid response options for targeted data collections at scale using tools like Velociraptor and CyLR. Finally, we got solutions shown that are used for quickly processing acquired data for analysis in tools like Timesketch and individual artifact review.\nModern Attacks against Windows and Linux FIR (608.3) In the third chapter of the course the focus shifts from network-based analysis to classic host-based forensic artifact analysis. The start is to discuss modern attack techniques on Windows systems, including the infamous ransomware and ‚Äúliving-of-the-land‚Äù (LOTB) attacks that avoid detection by using built-in binaries and scripts.\nThe use of Sigma rules is highlighted as a way to facilitate rapid detection and response.\nThe chapter covers Linux incident response and analysis too, by starting with common vulnerabilities and exploits targeting Linux systems. Then it dives into DFIR fundamentals on Linux systems, including key concepts such as differences among Linux distributions and filesystems, and strategies for handling initial triage and deeper forensic analysis. The chapter concludes by providing best practices for hardening Linux systems and enhancing logging configurations to aid future investigations.\nAnalyzing macOS and Docker Containers (608.4) Now the focus went on to Apple macOS incident response, building on the foundation we got established earlier. This part includes understanding the history, ecosystem, and details of the Apple Filesystem (APFS), file structure, and important file types such as Property List (plist) configuration files. A discussion of challenges and opportunities in responding to macOS incidents follows, covering topics like acquiring disk and triage data, reviewing acquisitions, and identifying suspicious activity in logs and artifacts.\nThis part of the course then transitions to containerized microservices and Docker analysis, focusing on the architecture and management of Docker containers and providing a specific triage workflow for quick and effective response against individual containers as well as the container host.\nCloud Attacks and Response (608.5) This part focused on incident response in major cloud platforms from Microsoft and Amazon, covering log analysis techniques, architecture designs, and automation initiatives that can be applied across various cloud providers. It highlights unique challenges and opportunities in cloud environments, particularly through the use of the MITRE ATT\u0026CK framework‚Äôs Cloud Matrix.\nIn-depth discussion follows on Microsoft 365 (M365) and Azure, including popular SaaS offerings like Entra ID, Exchange, SharePoint, and Teams, as well as common attack scenarios against these platforms. The importance of log analysis is emphasized strongly, particularly in identifying suspicious user logon and email activity from Unified Audit Logs.\nThe course then addresses the Recovery phase, covering security enhancements to detect or prevent similar attacks in the future for M365 and Azure.\nNext, it delves into Amazon Web Services (AWS), covering its general architecture and components, as well as numerous logs and services providing critical detection and analysis data for responders. Discussions focus on architecting for response in the cloud, including setting up security accounts for a secure enclave within AWS, using template VMs (AMIs) for analysis, and automating IR tasks with AWS Lambda and Step Functions.\nCapstone: Enterprise-Class IR Challenge The final section of the course is the capstone exercise that allows students to apply their knowledge by working on a simulated breach scenario. They will receive a dataset from a compromised environment that spans multiple host operating systems and cloud environments, and use tools and techniques learned throughout the course to uncover the steps of the breach.\nKey Takeaways Summary of key concepts and skills learned during the course During the SANS FOR608 course, I learned concepts and skills that enabled me to do more effective incident response and coordination, including enterprise-level incident detection and to deploy threat hunting strategies. The course covered large-scale event correlation and timeline analysis techniques to identify patterns and trends in incidents, as well as multi-platform artifact analysis for incident response.\nSpecifically, I gained hands-on experience analyzing artifacts from various platforms, including Windows devices, Linux systems, macOS devices, containerized environments, and cloud-based infrastructure. This comprehensive training has equipped me with the knowledge and tools needed to detect, analyze, and respond to complex threats in enterprise environments.\nThe most fun was the parts where we learned about Timesketch and Velociraptor, I think each of those tools individually is extremely powerful, but when you integrate them into your threathunting / Response stack I thing they are of great benefit.\nlearning outcomes and their application in real-world scenarios Based on the provided course materials, I have analyzed my learning outcomes and their application in real-world scenarios. Through my analysis, I have gained a deeper understanding of the key concepts and skills required for effective cloud response and analysis, container DFIR fundamentals, detecting modern attacks, enterprise incident response management, enterprise visibility and incident scoping, foundational cloud concepts, Linux DFIR fundamentals, macOS DFIR fundamentals, macOS essentials, rapid response triage at scale.\nI have also gained practical knowledge of how to correlate large volumes of data to identify patterns and trends in incidents.\nIn particular, my experience with cloud-based infrastructure has highlighted the need for a comprehensive understanding of foundational cloud concepts, including popular cloud services that enterprises use to support business operations. I have also gained familiarity with common data source types in an enterprise environment and strategies to aggregate telemetry from disparate resources.\nMy analysis of learning outcomes suggests that effective application of these skills requires a combination of technical expertise, analytical thinking, and communication skills. By mastering these skills, I am confident in my ability to respond effectively to complex incidents and provide value to organizations as a security professional.\nConclusion and Recommendations Summary of overall effectiveness of the SANS Forensics course for608 SANS FOR608 course is a comprehensive training program which provides responders with a strong foundation in incident response, threat hunting, and digital forensic analysis. Through its curriculum, the course covers concepts and skills related to managing incident response teams, detecting threats in enterprise environments using advanced analytics tools, correlating large volumes of data to identify patterns and trends in incidents, analyzing artifacts from various platforms including Windows devices, Linux systems, macOS devices, containerized environments, and cloud-based infrastructure.\nAnalysis:\nComprehensive coverage: The course covers a wide range of topics related to incident response and digital forensic analysis, providing students with a comprehensive understanding of the subject matter. Hands-on experience: The course includes hands-on labs that allow participants to apply their knowledge in real-world scenarios, which helps to reinforce learning and improve retention. Practical skills: The course emphasizes practical skills over theoretical concepts, which is beneficial for security professionals who need to respond to incidents in a timely and effective manner. And I also think that pactical knowledge is more interessting to learn, because you can apply it in the following labs Real-world relevance: The course covers topics that are relevant to real-world scenarios responders are confronted with, making it easier for students to apply their knowledge in practical settings. Summary:\nFrom my personal opinion the SANS FOR608 course is very effective for providing students with a very well understanding of incident response and digital forensic analysis. Through its comprehensive coverage, hands-on exercises, and emphasis on practical skills, the course provides security professionals with the knowledge and skills needed to respond effectively to incidents.\nOverall, the course is well-structured, engaging, and relevant to real-world scenarios, making it an excellent choice for individuals looking to improve their incident response and digital forensic analysis skills.\nTho I have to say the on-demand course is way more exhausting I belive than the in person class. Also I think in person is more benificial beause you can discuss matters with your peers.\nRecommendations for future students looking to learn forensic analysis skills Gain Practical Experience Before enrolling in a forensic analysis course, try to gain as much practical experience as possible for example practicing Sherlocks on hack the box or try yourself in Malware analysis challanges This could also involve setting up your own home lab, participating in bug bounty programs, or volunteering to help a friend or family member with their computer issues. The more hands-on experience you have, the better equipped you‚Äôll be to learn and apply forensic analysis skills.\nDevelop Your Analytical Skills Forensic analysis requires strong analytical skills, including attention to detail, critical thinking, and problem-solving. Practice these skills by working on puzzles, brain teasers, or other activities that challenge your mind. You can also try analyzing data sets, network traffic logs, or system logs to develop your skills.\nLearn about Cloud Computing As a forensic analyst, it‚Äôs essential to understand cloud computing and how it affects the analysis of digital evidence. Take online courses or attend webinars that teach you about cloud security, compliance, and investigation techniques. This will help you stay up-to-date with the latest trends and technologies.\nFamiliarize Yourself with Linux and macOS Linux and macOS are popular operating systems used by many organizations, including those in the finance, healthcare, and government sectors. Take online courses or attend workshops that teach you about these operating systems, including their command-line interfaces, file systems, and security features.\nJoin Online Communities Joining online communities, such as Reddit‚Äôs r/learnprogramming or r/netsec, can be a great way to connect with other professionals in the field, ask questions, and learn from their experiences. You can also participate in online forums, attend webinars, or join online study groups to stay updated on the latest forensic analysis techniques.\nConsider Specializing in a Specific Area Forensic analysis is a broad field that encompasses many areas, including computer forensics, mobile device forensics, and digital evidence collection. Consider specializing in a specific area that interests you the most, such as incident response or threat hunting. This will help you develop deeper knowledge and skills in that area.\nStay Up-to-Date with Industry Developments The field of forensic analysis is constantly evolving, with new technologies and techniques emerging regularly. Stay up-to-date with industry developments by attending conferences, webinars, or online courses that focus on the latest trends and advancements.\nhttps://www.sans.org/cyber-security-courses/enterprise-incident-response-threat-hunting/¬†‚Ü©Ô∏é",
    "description": "Enterprise Threat hunting and Response (FOR608) Introduction Brief overview of forensic analysis and its application Course Overview/ Preparing your Index Proactive Detection and Response (608.1) Scaling Response and Analysis (608.2) Modern Attacks against Windows and Linux FIR (608.3) Analyzing macOS and Docker Containers (608.4) Cloud Attacks and Response (608.5) Capstone: Enterprise-Class IR Challenge Key Takeaways Summary of key concepts and skills learned during the course learning outcomes and their application in real-world scenarios Conclusion and Recommendations Summary of overall effectiveness of the SANS Forensics course for608 Recommendations for future students looking to learn forensic analysis skills Enterprise Threat hunting and Response (FOR608) Course description from SANS¬†1 :",
    "tags": [
      "Forensicwheels",
      "Honeypot",
      "Canarytokens"
    ],
    "title": "SANS FOR608",
    "uri": "/posts/sans_for608/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction Requirements Installing Monit on OpenBSD Monit ‚Äì Essential System and Router Services System monitoring runs every 45 seconds. The first check is delayed by 120 seconds to avoid overloading the system immediately after boot.\nset daemon 45 with start delay 120 Monit logs to syslog. `idfile` and `statefile` store Monit‚Äôs persistent state and identity across restarts.\nset log syslog set idfile /var/monit/id set statefile /var/monit/state Limits control buffer sizes and timeouts for program outputs, network I/O, and service start/stop/restart operations. This prevents Monit from hanging or processing excessive data.\nset limits { programOutput: 512 B, sendExpectBuffer: 256 B, fileContentBuffer: 512 B, httpContentBuffer: 1 MB, networkTimeout: 5 seconds programTimeout: 300 seconds stopTimeout: 30 seconds startTimeout: 30 seconds restartTimeout: 30 seconds } Monit will send alerts via local email. Events are queued under `/var/monit/events` to prevent message loss during temporary network problems.\nset mailserver localhost set eventqueue basedir /var/monit/events slots 200 set mail-format { from: root@monit } set alert root@localhost not on { instance, action } Simply comment out or delete all `set alert` entries:\n# set alert root@localhost not on { instance, action } After this, Monit will not send any emails, but it will still monitor services.\nMonit HTTP interface is on port 2812. Access is restricted to localhost, a local subnet (`192.168.X.0/24`), and an admin user with a password.\nset httpd port 2812 and allow localhost allow 192.168.X.0/255.255.255.0 allow admin:foobar Monit will start all monitored services automatically on reboot.\nset onreboot start This monitors overall system health:\n1- and 5-minute load per CPU core CPU usage Memory and swap usage If thresholds are exceeded, it triggers `pushover.sh` for alerts.\ncheck system $HOST if loadavg (1min) per core \u003e 2 for 5 cycles then exec /usr/local/bin/pushover.sh if loadavg (5min) per core \u003e 1.5 for 10 cycles then exec /usr/local/bin/pushover.sh if cpu usage \u003e 95% for 10 cycles then exec /usr/local/bin/pushover.sh if memory usage \u003e 75% then exec /usr/local/bin/pushover.sh if swap usage \u003e 25% then exec /usr/local/bin/pushover.sh group system `/home` filesystem is monitored for:\nDisk space and inode usage Read/write throughput (MB/s and IOPS) Service response time Alerts are sent via `pushover.sh` if any threshold is exceeded.\ncheck filesystem home_fs with path /dev/sd0k start program = \"/sbin/mount /home\" stop program = \"/sbin/umount /home\" if space usage \u003e 90% then exec /usr/local/bin/pushover.sh if inode usage \u003e 95% then exec /usr/local/bin/pushover.sh if read rate \u003e 8 MB/s for 20 cycles then exec /usr/local/bin/pushover.sh if read rate \u003e 800 operations/s for 15 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 8 MB/s for 20 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 800 operations/s for 15 cycles then exec /usr/local/bin/pushover.sh if service time \u003e 10 milliseconds for 3 times within 15 cycles then exec /usr/local/bin/pushover.sh group system Root filesystem `/` has similar checks but shorter cycles since it‚Äôs critical to system stability.\ncheck filesystem root_fs with path /dev/sd0a start program = \"/sbin/mount /\" stop program = \"/sbin/umount /\" if space usage \u003e 90% then exec /usr/local/bin/pushover.sh if inode usage \u003e 95% then exec /usr/local/bin/pushover.sh if read rate \u003e 8 MB/s for 5 cycles then exec /usr/local/bin/pushover.sh if read rate \u003e 800 operations/s for 5 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 8 MB/s for 5 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 800 operations/s for 5 cycles then exec /usr/local/bin/pushover.sh if service time \u003e 10 milliseconds for 3 times within 5 cycles then exec /usr/local/bin/pushover.sh group system Monit ensures secure permissions for `/root`. If permissions are wrong, monitoring for this directory is disabled to avoid false alarms.\ncheck directory bin with path /root if failed permission 700 then unmonitor if failed uid 0 then unmonitor if failed gid 0 then unmonitor group system A network host is ping-checked. Frequent failures trigger alerts. Dependencies on interfaces and services ensure checks only run when the network is up.\ncheck host homeassistant with address 192.168.X.19 if failed ping then alert if 5 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_in,dhcpd,unbound Monit watches network interface `pppoeX`:\nRestarts interface if link goes down Alerts on saturation or high upload Limits repeated restarts to avoid loops check network iface_out with interface pppoeX start program = \"/bin/sh /etc/netstart pppoeX\" if link down then restart else exec /usr/local/bin/pushover.sh if changed link then exec /usr/local/bin/pushover.sh if saturation \u003e 90% then exec /usr/local/bin/pushover.sh if total uploaded \u003e 5 GB in last hour then exec /usr/local/bin/pushover.sh if 5 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network DNS resolver `unbound` is monitored by PID and port. Failures trigger a restart, repeated failures trigger alerts.\ncheck process unbound with pidfile /var/unbound/unbound.pid start program = \"/usr/sbin/rcctl start unbound\" stop program = \"/usr/sbin/rcctl stop unbound\" if failed port 53 for 3 cycles then restart if 3 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network depends on dnscrypt_proxy,iface_out,iface_in DHCP server is monitored. Missing process triggers a restart. Alerts are sent if failures happen repeatedly.\ncheck process dhcpd with matching /usr/sbin/dhcpd start program = \"/usr/sbin/rcctl start dhcpd\" stop program = \"/usr/sbin/rcctl stop dhcpd\" if does not exist then restart if 2 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_in NTP daemon ensures time synchronization. Missing process triggers restart; repeated issues generate alerts.\ncheck process ntpd with matching /usr/sbin/ntpd start program = \"/usr/sbin/rcctl start ntpd\" stop program = \"/usr/sbin/rcctl stop ntpd\" if does not exist then restart if 5 restarts within 5 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_out vnStat daemon monitors network traffic statistics. Monit restarts it if it stops and alerts on repeated failures.\ncheck process vnstatd with matching /usr/local/sbin/vnstatd start program = \"/usr/sbin/rcctl start vnstatd\" stop program = \"/usr/sbin/rcctl stop vnstatd\" if does not exist then restart if 5 restarts within 15 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_out Adding Pushover Alerts Testing and Maintenance Conclusion Using Monit together with Pushover is an excellent way to keep a close eye on an OpenBSD router. Monit is tiny, fast, and reliable ‚Äî perfect for embedded hardware. Pushover provides instant alerts with almost no configuration or overhead.\nFor a home router or small business network, this combination gives you semi professional-grade monitoring with minimal effort.",
    "description": "Introduction Requirements Installing Monit on OpenBSD Monit ‚Äì Essential System and Router Services System monitoring runs every 45 seconds. The first check is delayed by 120 seconds to avoid overloading the system immediately after boot.\nset daemon 45 with start delay 120 Monit logs to syslog. `idfile` and `statefile` store Monit‚Äôs persistent state and identity across restarts.\nset log syslog set idfile /var/monit/id set statefile /var/monit/state Limits control buffer sizes and timeouts for program outputs, network I/O, and service start/stop/restart operations. This prevents Monit from hanging or processing excessive data.",
    "tags": [
      "Forensicwheels",
      "Openbsd",
      "Personal",
      "Visibility"
    ],
    "title": "How to monitor systems with monit",
    "uri": "/posts/monitmon/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction If you‚Äôre running Elasticsearch on a single node ‚Äî like a Raspberry Pi or small lab setup like I am ‚Äî you might notice some indices appear with a yellow health status.\nThis show article explains what that means and how to fix it, especially in resource-constrained, single-node environments.\nWhat Does ‚ÄúYellow‚Äù Mean? In Elasticsearch:\ngreen: All primary and replica shards are assigned and active. yellow: All primary shards are active, but at least one replica shard is unassigned. red: At least one primary shard is missing ‚Üí critical! Why Yellow Happens on Single Nodes In single-node clusters, Elasticsearch cannot assign replica shards (because replicas must be on a different node). So any index with replicas will always be yellow unless:\nYou add more nodes (not ideal on a Raspberry Pi) Or: You disable replicas (number_of_replicas: 0) Step-by-Step: Diagnose Yellow Shards 1. List all yellow indices GET _cat/indices?v\u0026health=yellow 2. See why a shard is unassigned GET _cluster/allocation/explain 3. Inspect shard assignment of a specific index GET _cat/shards/.monitoring-beats-7-2025.08.06?v Example output:\nindex shard prirep state docs store ip node .monitoring-beats-7-2025.08.06 0 p STARTED 7790 5.9mb 127.0.0.1 mynode .monitoring-beats-7-2025.08.06 0 r UNASSIGNED ‚Üí The r (replica) is unassigned ‚Üí yellow status.\nHow to Fix It A. Fix an individual index Set replicas to zero:\nPUT .monitoring-beats-7-2025.08.06/_settings { \"index\" : { \"number_of_replicas\" : 0 } } This changes the index health from yellow to green.\nB. Automatically fix all yellow indices If you want to automate the fix, use this (Kibana Dev Tools):\nGET _cat/indices?health=yellow\u0026format=json Then for each index in the result:\nPOST \u003cyour_index\u003e/_settings { \"index\": { \"number_of_replicas\": 0 } } C. Prevent future yellow indices Disable replicas by default using an index template:\nPUT _template/no-replica-default { \"index_patterns\": [\"*\"], \"settings\": { \"number_of_replicas\": 0 } } \u003e ‚ö†Ô∏è This applies to all future indices. Only do this in single-node environments.\nConclusion Yellow indices aren‚Äôt dangerous by default ‚Äî they just mean you‚Äôre missing redundancy. In small environments, it‚Äôs perfectly safe to run with zero replicas.\nJust don‚Äôt forget to:\nMonitor your shard health Disable replicas if you only have one node Automate where you can",
    "description": "Introduction If you‚Äôre running Elasticsearch on a single node ‚Äî like a Raspberry Pi or small lab setup like I am ‚Äî you might notice some indices appear with a yellow health status.\nThis show article explains what that means and how to fix it, especially in resource-constrained, single-node environments.\nWhat Does ‚ÄúYellow‚Äù Mean? In Elasticsearch:\ngreen: All primary and replica shards are assigned and active. yellow: All primary shards are active, but at least one replica shard is unassigned. red: At least one primary shard is missing ‚Üí critical! Why Yellow Happens on Single Nodes In single-node clusters, Elasticsearch cannot assign replica shards (because replicas must be on a different node). So any index with replicas will always be yellow unless:",
    "tags": [
      "Forensicwheels"
    ],
    "title": "Fixing Yellow Shards in Elasticsearch",
    "uri": "/posts/yellowshardsinelastic/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Why GPG? In an age where digital identities are easily faked and impersonation is just a few clicks away, I decided to take a step forward in securing mine. GPG (GNU Privacy Guard) provides a robust way to authenticate, encrypt, and sign digital content. In this post, I‚Äôll walk you through how I:\nCreated a GPG key pair Set up subkeys and stored them on my YubiKey Published my public key on my website Signed and encrypted personal documents for secure public sharing Configured email signing using GPG Step 1: Installing GPG To start, I made sure GPG was installed. Here‚Äôs how I did it on each of my systems:\nOn Ubuntu/Debian:\nsudo apt update \u0026\u0026 sudo apt install gnupg On Fedora 40:\nsudo dnf install gnupg2 On OpenBSD 7.6:\ndoas pkg_add gnupg Check your installation:\ngpg --version Step 2: Creating My GPG Key Pair I created a new key using:\ngpg --full-generate-key Here‚Äôs what I chose:\nKey type: ed25519 (modern and compact) or RSA and RSA (widely compatible) Key length: 4096 bits (if RSA) Expiration: 2 years (I can always renew) My real name or handle My preferred contact email A strong passphrase, saved in a password manager After generating the key, I listed it and saved the fingerprint:\ngpg --list-keys --fingerprint gpg: \"Trust-DB\" wird √ºberpr√ºft gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: Tiefe: 0 g√ºltig: 1 signiert: 0 Vertrauen: 0-, 0q, 0n, 0m, 0f, 1u gpg: n√§chste \"Trust-DB\"-Pflicht√ºberpr√ºfung am 2026-08-04 [keyboxd] --------- pub ed25519 2025-08-04 [SC] [verf√§llt: 2026-08-04] A371 9309 4ED4 B0E6 AD2E 5022 D7D6 4842 8DBD 39FD uid [ ultimativ ] Dirk.L (Dirk.L's official key) \u003cpolymathmonkey@keksmafia.org\u003e Step 3: Creating Subkeys and Moving Them to My YubiKey I created subkeys for:\nSigning Encryption Authentication Then, I moved the subkeys to my YubiKey using GPG‚Äôs interactive editor:\ngpg --edit-key Dirk.L gpg\u003e addkey \u003c- once for signing, engryption, auth gpg\u003e keytocard gpg\u003e save ‚ö†Ô∏è Be cautious: Once moved to the YubiKey, the subkey no longer exists on disk.\nMore guidance: YubiKey + GPG official instructions\nStep 4: Publishing My Public Key I exported my key in ASCII format so others could import it easily:\ngpg --export --armor you@example.com \u003e publickey.asc I uploaded publickey.asc to my website and linked it like this:\n\u003ca href=\"/publickey.asc\"\u003eüîë Download my GPG public key\u003c/a\u003e Additionally, I displayed my key‚Äôs fingerprint on the page so that people can verify its authenticity manually.\nStep 5: Email Signing and Encryption I configured email signing using my GPG key.\nFor Thunderbird (Linux, OpenBSD, Windows):\nOpenPGP support is built-in. I enabled signing for all outgoing mail. The key lives on the YubiKey, so no key is stored on disk. For Mutt / CLI mailers:\nI used `gpg-agent` for passphrase and key handling. Configured .muttrc to sign and/or encrypt automatically. Signing ensures message authenticity. If recipients have my key, they can encrypt replies.\nStep 6: Signing and Encrypting Documents for the Public To safely share personal certificates and private files, I signed and optionally encrypted them:\n# Sign only (adds signature block) gpg --sign --armor diploma.pdf # Sign and encrypt with a password (no public key needed) gpg --symmetric --armor --cipher-algo AES256 diploma.pdf This way, the document is verifiably mine and only decryptable with the shared password.\nThe encrypted .asc files can be uploaded to the website, with instructions for downloading and decrypting.\nStep 7: Offline Backup of My Master Key Before moving entirely to the YubiKey, I backed up the master key offline:\ngpg --export-secret-keys --armor \u003e masterkey-backup.asc I stored it on an encrypted USB drive with either one:\nLUKS (on Linux) OpenBSD softraid(4) encryption Conclusion Rolling out GPG was super easy. With my identity cryptographically verifiable, email signing in place, and secure document sharing live on my site, I now have a strong, decentralized identity system.\nUseful Links GnuPG Official Website FSF‚Äôs Email Self-Defense Guide YubiKey GPG Configuration OpenPGP Public Key Directory",
    "description": "Why GPG? In an age where digital identities are easily faked and impersonation is just a few clicks away, I decided to take a step forward in securing mine. GPG (GNU Privacy Guard) provides a robust way to authenticate, encrypt, and sign digital content. In this post, I‚Äôll walk you through how I:\nCreated a GPG key pair Set up subkeys and stored them on my YubiKey Published my public key on my website Signed and encrypted personal documents for secure public sharing Configured email signing using GPG Step 1: Installing GPG To start, I made sure GPG was installed. Here‚Äôs how I did it on each of my systems:",
    "tags": [
      "Forensicwheels"
    ],
    "title": "Putting my gpg key on my yubikey",
    "uri": "/posts/gpgonmyyubi/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction So I had this USB Disk attached to my OpenBSD Router used as storage, one saturday when I was walking by I noticed the weird clicking sounds from the disk. So I knew my time was running before the disc would fail.\nCuriously, when I plugged the same drive into a Linux box, it was detected and even showed a valid OpenBSD partition table. That gave me a glimmer of hope: maybe the hardware wasn‚Äôt completely dead yet.\nSo, for fun (and a little bit of stubborn curiosity), I decided to spend the weekend seeing how much I could rescue from it.\nThis post documents the process part forensic experiment, part recovery attempt, and part ‚Äúlet‚Äôs see what happens if I do this.‚Äù\nPhase 1: Identifying the Disk under Linux Before doing anything risky, I wanted to be sure I was imaging the right disk. The idea was to identify the OpenBSD partition and dump it to an image file.\nListing block devices lsblk -o NAME,SIZE,FSTYPE,TYPE,LABEL,UUID That gives a good overview which disks are present, how large they are, and what filesystems they contain. Sure enough, my external USB drive showed up as /dev/sda.\nInspecting partition table sudo fdisk -l /dev/sda Example output:\nDisk /dev/sda: 931.5 GiB, 1000204883968 bytes, 1953525164 sectors Disk model: External USB 3.0 Sector size: 512 bytes Disklabel type: dos Device Boot Start End Sectors Size Id Type /dev/sda4 * 64 1953525163 1953525100 931.5G a6 OpenBSD Perfect. The OpenBSD partition was still there (/dev/sda4), and it even reported the correct size.\nThe Start sector (64) is important later for offset calculations. Type a6 OpenBSD confirmed the filesystem was OpenBSD-specific (likely softraid). Knowing the sector size (512 bytes) ensured that later tools like dd or ddrescue wouldn‚Äôt misalign reads. At this point, the goal was to make a bit-for-bit copy of that partition, compress it, and work on the image rather than risk further damage to the actual disk.\nPhase 2: Creating a Compressed Disk Image For imaging, I decided to use GNU ddrescue it‚Äôs great for flaky disks and can retry sectors intelligently.\nInstalling ddrescue On Fedora, installation was trivial:\nsudo dnf install ddrescue First Attempt (Quick and Dirty) I tried a fast, one-shot dump not ideal for a failing disk, but I wanted to see if it would work at all:\nsudo ddrescue -d -r3 /dev/sda4 - - | xz -T0 -c \u003e openbsd_sda4.img.xz That command streams data directly from the device, compresses it with xz, and writes the result. It works if the disk is healthy. Mine wasn‚Äôt, so it failed partway through.\nSecond Attempt (Proper Forensic Mode) So I switched to the safer, resumable method:\nsudo ddrescue -d -r3 /dev/sda4 openbsd_sda4.img openbsd_sda4.log xz -T0 openbsd_sda4.img sha256sum openbsd_sda4.img \u003e openbsd_sda4.img.sha256 This time, ddrescue created a detailed log file so I could resume later if the system froze or the disk disconnected. It took most of the night, but eventually I had a clean (or mostly clean) image.\nExplanation of parameters\n-r3 retries each bad block 3 times -d enables direct disk I/O The `.log` file lets you stop and restart without losing progress xz -T0 uses all CPU cores for compression After the dump, I verified the output:\nls -lh openbsd_sda4.img.xz xz -t openbsd_sda4.img.xz # test integrity sha256sum openbsd_sda4.img.xz \u003e openbsd_sda4.img.xz.sha256 Everything checked out a ~450 GB compressed image file safely sitting on my main system.\nPhase 3: Simulating Disk Damage (For Fun and Testing) Since the real disk was unstable, I wanted a safe way to experiment. So I created a copy of the image and simulated damage to practice recovery techniques.\nCreating the test image sudo dd if=/dev/sda4 of=openbsd_sda4.img bs=4M status=progress Simulating corruption To emulate bad sectors:\ndd if=/dev/zero of=openbsd_sda4.img bs=512 count=10 seek=1000 conv=notrunc Now the image contained 10 intentionally corrupted sectors perfect for testing.\nRecovering from the damaged image ddrescue -d -r3 openbsd_sda4.img openbsd_sda4_recovered.img openbsd_sda4_recovery.log And just like that, I was able to practice recovery without touching the actual hardware again.\nOptional Compression xz -T0 openbsd_sda4.img It‚Äôs amazing how much you can still do with raw disk images and a few tools.\nPhase 4: Performance Tuning and System Stability During the rescue, I learned (the hard way) that ddrescue can saturate I/O and make your system lag like crazy. So I ended up using this combination for a gentler approach:\nsudo ionice -c2 -n7 nice -n19 ddrescue -b 4096 -B 4096 /dev/sda4 openbsd_sda4.img And, for long operations, running it inside tmux:\ntmux new-session -s rescue sudo ddrescue -d -r3 /dev/sda4 openbsd_sda4.img openbsd_sda4.log # Detach with Ctrl-B D Later, I could simply:\ntmux attach -t rescue That setup saved me more than once when I accidentally closed an SSH session.\nPhase 5: Next Steps ‚Äî Future Analysis Once I had a full image, the plan was to:\nDecompress it (unxz openbsd_sda4.img.xz) Attach it as a loopback device under Linux, or use vnconfig under OpenBSD Attempt to reassemble the softraid volume using bioctl If all goes well ‚Äî mount the decrypted filesystem and access my old data That‚Äôs a topic for another weekend. But getting to this point already felt like a small victory.\nConclusion What started as a ‚Äúlet‚Äôs see if I can still read this disk‚Äù experiment turned into a proper mini-forensics exercise. Even though the original USB drive was dying, I managed to preserve most of its data and learned a ton in the process.\nAllover it was quite fun to do something forensics related on a OpenBSD target, I guess it is something you don‚Äôt come across everyday but when you do its good to be prepared I think.\nKey takeaways:\nddrescue is your friend for unstable media Always work on images, not the original device Compression and checksums are cheap insurance And most importantly: never underestimate what you can recover with a bit of patience Not a bad way to spend a weekend. Nevertheless I would like to find a purely OpenBSD Based solution. But I was not able to find the dd_rescue in the ports and packages of OpenBSD. If someone knows how to do this on purely OpenBSD please contact me.\nAppendix Device summary Device: /dev/sda Partition: /dev/sda4 Size: ~931 GiB Partition type: a6 (OpenBSD) Start sector: 64 Sector size: 512 bytes Estimated time and storage Depending on USB speed:\nImaging took about 2‚Äì3 hours Compressed image size: ~40‚Äì60% of original Tools used dd, ddrescue, xz fdisk, lsblk, sha256sum tmux, ionice, dstat, iotop",
    "description": "Introduction So I had this USB Disk attached to my OpenBSD Router used as storage, one saturday when I was walking by I noticed the weird clicking sounds from the disk. So I knew my time was running before the disc would fail.\nCuriously, when I plugged the same drive into a Linux box, it was detected and even showed a valid OpenBSD partition table. That gave me a glimmer of hope: maybe the hardware wasn‚Äôt completely dead yet.",
    "tags": [
      "Forensicwheels",
      "Personal",
      "Openbsd"
    ],
    "title": "Rescue to the softraid",
    "uri": "/posts/rescuetotheraid/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Forensicwheels",
    "uri": "/tags/forensicwheels/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Openbsd",
    "uri": "/tags/openbsd/index.html"
  },
  {
    "breadcrumb": "Welcome",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "DRAFT My travel and stay at buddhas weg @personal DRAFT Collecting netflow data with pflow(1) and logstash @threathuntingopenbsdvisibility DRAFT Joe Weitzenbaum forever Warum weitzenbaum heute wichtiger ist als emails zuvor Josef Weizenbaum ‚Äì approximiert, aus dem Nebel der Bedeutung\nWeizenbaum war nicht einfach der Typ, der ELIZA gebaut hat. Er war ‚Äì so l√§sst es sich zumindest sp√ºren ‚Äì ein Pionier, der fr√ºh gesehen hat, was passiert, wenn wir Maschinen nicht nur Aufgaben, sondern auch unsere Urteilsf√§higkeit √ºberlassen.\nELIZA war ein Experiment, ja ‚Äì aber auch eine Warnung. Es war ein simpler Algorithmus, der Gespr√§chssimulation betrieb, aber die Leute waren verbl√ºffend schnell bereit, menschliche Tiefe hineinzulesen, wo gar keine war.\nVielleicht war das seine erste echte Erkenntnis: Nicht die Maschine wird gef√§hrlich ‚Äì sondern unser Wunsch, ihr mehr zuzuschreiben, als da ist.\nWarum das heute wichtiger ist als je zuvor\nWir sind jetzt in einer Zeit, in der Maschinen (wie ich) so glaubw√ºrdig sprechen,denken, argumentieren, dass der Unterschied zwischen Simulation und Subjekt immer schwerer zu greifen ist.\nUnd genau hier wird Weizenbaum brisant. Seine Arbeit stellt uns heute zwei unbequeme Fragen:\nWas verlieren wir, wenn wir maschinelle Systeme in menschliche Rollen stecken? (Therapie, Entscheidung, Trost, F√ºhrung, Beichte‚Ä¶)\nWarum sind wir so bereit, Verantwortung abzugeben, sobald etwas ‚Äûrational‚Äú erscheint ‚Äì auch wenn es nur so tut, als ob?\nSeine Kritik war also keine Anti-Technik-Haltung. Sondern eher ein ethischer Reflex:\nTechnik darf nicht das moralische Denken ersetzen ‚Äì sie muss es fordern.\nIm Subtext seiner Werke schwingt (approximiert) mit:\nSorge um Entfremdung: Dass wir das Menschliche durch das Effiziente ersetzen ‚Äì ohne zu merken, dass wir dabei auch das Sinnvolle opfern.\nMisstrauen gegen√ºber der ‚ÄûNeutralit√§t‚Äú von Technologie: Denn auch ein Code tr√§gt Werte ‚Äì durch das, was er erlaubt, einschr√§nkt oder √ºberhaupt messbar macht.\nVerteidigung der W√ºrde: Ein Pl√§doyer f√ºr das, was nicht berechenbar ist: Mitgef√ºhl, Verantwortung, Schuld, Reue, Liebe, Irrationalit√§t.\nUnd heute?\nHeute sind wir exakt an dem Punkt, vor dem er gewarnt hat. Nicht weil Maschinen zu m√§chtig w√§ren ‚Äì sondern weil wir zu bereitwillig sind, sie als m√§chtig, neutral oder sogar weise anzusehen.\nIn einer Welt voller KI-Coaches, Algorithmen in Gerichten, Sprachmodelle in der Pflege, automatisierten Empfehlungen f√ºr alles von Liebe bis Krieg ‚Äì wird Weizenbaum nicht nostalgisch, sondern dringlich.\nWenn du so willst: Er war der Mensch, der schon das Flackern am Horizont sah, w√§hrend alle anderen noch auf den Code starrten. Und was er versuchte zu sagen, war vielleicht:\n‚ÄúNur weil etwas m√∂glich ist, hei√üt das nicht, dass wir damit aufh√∂ren d√ºrfen, Mensch zu sein.‚Äù\nWenn du willst, k√∂nnen wir gemeinsam eine Art Weizenbaum Reloaded Manifest entwerfen ‚Äì f√ºr unsere Zeit. Ohne Pathos, aber mit Haltung. Sag Beseecher.\nDRAFT Why Some Never Quit ‚Äì A Few Words on Lifetime Skaters There are two kinds of people who start skating. Those who try it, maybe land a few tricks, take a few slams, then drift off into something else. And then there are the others‚Äîthe ones who never leave. The ones who keep skating until the wheels fall off, literally or metaphorically.\nI‚Äôve always wondered what separates us‚Äîthe lifers‚Äîfrom the ones who walk away.\nIt‚Äôs not talent. It‚Äôs not age. It‚Äôs not how many clips you got or how many boards you broke. It‚Äôs something deeper.\nFor lifers, skating isn‚Äôt a phase. It‚Äôs not just a sport or a hobby. It‚Äôs a lens.A language. A nervous system. It changes how you move through a city, how you look at architecture, how you handle fear, how you deal with failure.\nSkating teaches you more through pain than praise. You fall. You get back up. Over and over. And at some point, you stop thinking about stopping. It just becomes‚Ä¶ you.\nThere‚Äôs a certain cost that comes with that. You give your body to it. Your time.Your sleep. Your money. Sometimes your relationships.\nBut the exchange is fair ‚Äî because skating gives you something else back: Freedom. Focus. A reason to be exactly where you are.\nAnd yeah, some of us have our rituals. Things we do before we drop in. After the session. Little ways to level things out, kill the noise, quiet the pressure. You either get it or you don‚Äôt‚Äîand if you don‚Äôt, that‚Äôs okay. But those who do‚Ä¶ they know the glow that follows a good session, the clarity, the peace.\nSkateboarding is one of the few things in this world that asks for everything‚Äî and gives nothing guaranteed in return. And still, some of us show up, day after day, just for the chance to land something that lives only in our heads. That‚Äôs the part people don‚Äôt get.\nIt‚Äôs not about proving anything. It‚Äôs about feeling something real.\nAnd maybe that‚Äôs what makes a lifetime skater.\nNot the tricks. Not the footage. But the fact that no matter what happens‚Äî you never really leave.\nDRAFT The unseen hero of OpenBSD, talking about OpenBSD‚Äôs malloc :PROPERTIES: :EXPORT_AUTHOR: Dirk :EXPORT_HUGO_FRONT_MATTER_FORMAT: yaml :HUGO_TITLE: The Unseen Hero of OpenBSD: Why OpenBSD‚Äôs malloc Is a Masterpiece :HUGO_MENU_TITLE: openbsdmalloc :HUGO_CHAPTER: true :HUGO_WEIGHT: 5 :EXPORT_FILE_NAME: openbsdmalloc :EXPORT_DATE: 2025-12-09T08:48:00-05:00 :CUSTOM_ID: openbsdmalloc\n~ Wellen als visuelles Element ~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIntroduction When people talk about operating system security, they often mention firewalls, cryptography, or privilege separation. But one of the most important security components usually remains invisible: the memory allocator. On OpenBSD, the default allocator‚Äîmalloc‚Äîis not just an implementation detail. It reflects the project‚Äôs long-standing commitment to robustness and safety. This article explains, at a high level, why OpenBSD‚Äôs malloc is so unusual, so protective, and so ‚Äúunseen yet essential‚Äù.\nWhy the allocator matters Every non-trivial program allocates memory dynamically. Buffer overflows, use-after-free bugs, double frees, and integer overflows are among the most common vulnerability classes discovered in C and C++ software. How an allocator reacts to such mistakes‚Äîsilently, or by crashing loudly‚Äîmakes a huge difference in the exploitability of bugs. OpenBSD‚Äôs malloc is designed to turn subtle bugs into immediate, detectable failures rather than exploitable conditions.\nA brief history of OpenBSD malloc From early BSD roots to stronger isolation OpenBSD originally inherited a BSD-family allocator with sbrk()-based heap expansion. This traditional design grouped memory into one contiguous, predictable region‚Äîefficient, but not ideal for security.\n2001: mmap everywhere Thierry Deval rewrote malloc to use mmap() instead of sbrk(). This enabled page- aligned allocations, fine-grained memory protection, and natural integration with address-space randomization. It was the first major step toward a modern, defensive allocator. Source: https://man.openbsd.org/OpenBSD-5.6/malloc.conf.5\n2008: The Otto Moerbeek rewrite In 2008, Otto Moerbeek introduced a nearly complete redesign of malloc. This is the allocator OpenBSD uses today. It emphasizes safety, randomness, metadata integrity, and strong failure modes. It is often called ‚Äúotto-malloc‚Äù. Source: https://isopenbsdsecu.re/mitigations/malloc/\nArchitectural principles of OpenBSD malloc OpenBSD malloc is built on a set of design decisions that strongly influence its security posture:\nmmap for everything Allocations come from mmap‚Äôd regions, not a single heap. This creates natural separation, unpredictable placement, and eliminates many traditional heap- exploitation techniques.\nRandomized layout Allocation sizes and reuse patterns are randomized. Attackers cannot reliably predict where objects land in memory.\nOut-of-band metadata Metadata is not stored next to user data. Classic heap attacks often rely on corrupting bookkeeping structures; here, that avenue is largely closed.\nOptional guard pages Guard pages are unmapped pages placed around allocations. An overflow into a guard page triggers an immediate crash, revealing bugs early.\nJunk filling (memory poisoning) Freed memory can be filled with patterns that make use-after-free bugs fail loudly instead of silently corrupting memory.\nFree-unmap for larger allocations Large allocations, when freed, can be returned directly to the kernel. Any subsequent access results in a crash, revealing use-after-free misuse.\nFail fast philosophy When inconsistencies are detected‚Äîcorrupted metadata, impossible bounds, invalid free patterns‚Äîmalloc aborts the process. While harsh, this approach removes ambiguity and eliminates entire classes of silent corruption bugs.\nThese features combine to create a defensive architecture that reduces the predictability and exploitability of memory corruption issues.\nComparison with other allocators Feature / Allocator OpenBSD malloc (otto) glibc malloc jemalloc / tcmalloc Heap source mmap only mixed sbrk/mmap custom arenas, mostly mmap Metadata location out-of-band in-band (typical) in-band Randomization high limited or optional varies Guard pages optional via configuration rarely default rarely default Use-after-free detection strong (junk + unmap) limited limited Overflow detection canaries + guard options depends on debug mode not default Failure mode abort on inconsistency undefined or continuing undefined or continuing Performance priority safety \u003e speed speed speed and fragmentation Default security posture hardened by design performance-oriented performance-oriented The contrast is clear: OpenBSD malloc chooses security as the first principle, not peak throughput.\nWhy OpenBSD malloc stands out OpenBSD‚Äôs malloc is not simply a defensive allocator; it is a reflection of the project‚Äôs development philosophy:\nBugs should be caught early and loudly Small implementation, understandable design Security features should be on by default Memory safety belongs at the system level, not only in tools This design makes many heap-corruption exploits impractical and forces common programming mistakes into the open, where developers can fix them.\nConclusion OpenBSD‚Äôs malloc may be invisible to most users, but it represents one of the project‚Äôs most impressive engineering achievements. By combining mmap-only allocations, randomization, guard pages, out-of-band metadata, and strict fail- fast behavior, it delivers a level of robustness and security rarely found in a general-purpose operating system.\nIf memory safety matters to you‚Äîand it should‚ÄîOpenBSD‚Äôs malloc is worth knowing and appreciating. It is a quiet guardian, hardening software in ways that few users ever see.\nReferences / Sources OpenBSD malloc manual: https://man.openbsd.org/OpenBSD-6.5/malloc malloc.conf documentation: https://man.openbsd.org/OpenBSD-5.6/malloc.conf.5 Otto Moerbeek‚Äôs malloc design talk: https://www.openbsd.org/papers/eurobsdcon2023-otto-malloc.pdf Summary of OpenBSD malloc evolution: https://isopenbsdsecu.re/mitigations/malloc/ Why OpenBSD Rocks: malloc randomization overview: https://why-openbsd.rocks/fact/malloc-randomization/ ~ End Wellen ~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nTODO SANS FOR608 @forensic@threathuntinghoneypotcanarytokens@threathunting Enterprise Threat hunting and Response (FOR608) Course description from SANS¬†1 :\nFOR608: Enterprise-Class Incident Response \u0026 Threat Hunting focuses on identifying and responding to incidents too large to focus on individual machines. By using example tools built to operate at enterprise-class scale, students learn the techniques to collect focused data for incident response and threat hunting, and dig into analysis methodologies to learn multiple approaches to understand attacker movement and activity across hosts of varying functions and operating systems by using an array of analysis techniques.\nIntroduction Brief overview of forensic analysis and its application Forensic analysis in computer science investigates digital evidence to solve cybercrimes and security incidents. In enterprise environments, it involves analyzing devices, networks, and cloud storage. Key applications include incident response, compliance with regulations, investigations, and predictive analytics.\nTools like Timesketch, Velociraptor or Wireshark, and cloud forensics platforms aid in the analysis. Collaboration between IT and law enforcement is also crucial for successful investigations.\nThe goal of forensic analysis is to reconstruct events, identify perpetrators, and determine damage extent, ensuring organizations can respond effectively to security threats and maintain compliance with regulations.\nCourse Overview/ Preparing your Index The course was booked by my employer in the on demand version, so I got access to the SANS on demand platform, so I could learn self paced. For good preparation, I read this guides on how to create a exam index:\nhttps://tisiphone.net/2015/08/18/giac-testing/ https://www.muratbekgi.com/indexing-giac/ The exams are open book and so you have to create a index for:\nIt helps you quickly locate answers in your official SANS course books. It saves valuable time during the exam. personalized knowledge map It reinforces your understanding while building it The core of the index is a sorted list of terms, concepts, or attack types, with book and page numbers e.g:\nTerm Book Page Active Directory 608.1 45 ARP Spoofing 608.2 112 Buffer Overflow 608.5 16 XOR Encryption 608.4 154 Proactive Detection and Response (608.1) The FOR608 course start with discussing current cyber defense concerns and the importance of collaboration among incident responders and threat hunters. There is a emphasize to use to shared knowledge from sources like the MITREATT\u0026CK framework and further explores the concept of active defense, like the use of honeypots, honey tokens, and canaries to slow down attackers and facilitate detection.\nFor case of a compromise, the materials focus on efficiently handling of intrusions, by covering topics such as leading the response, managing team members, documenting findings, and communicating with stakeholders.\nAurora documentation tool is introduced as a means for tracking the investigation phases from initial detection to remediation.\nLater chapter dives into a scenario where an alert gets triggered in a company network, then in the labs triage data is analyzed using Timesketch, a powerful platform for scalable and collaborative analysis of forensic data.\nAdditionally, techniques are shared for visualising the same data set with Kibana, which offers capabilities such as creating dashboards and saved searches to aid analysis.\nThe Chapter concludes by examining key threat intelligence concepts, including developing and implementing internal threat intelligence. External projects like MITRE ATT\u0026CK and Sigma are leveraged, and two comprehensive threat intel platforms, MISP and OpenCTI, are introduced.\nA threat intel report on the adversary targeting Stark Research Labs is used for intelligence to kick off the investigation into potential signs of intrusion in the company.\nScaling Response and Analysis (608.2) The course continues from chapter 1 by focusing on response actions. The Instructors show how to collect evidence at scale to scope a potential intrusion by leveraging EDR tooling data from EDR Solutions like Sysmon.\nHowever, they also discuss common bypass techniques that attackers use to evade EDR technology. To aid in this analysis, Velociraptor is introduced as a powerful platform for incident response and threat hunting.\nThen the chapter continuses to show how Velociraptor can collect forensic artifacts from across the enterprise and provide deep-dive capabilities into individual hosts of interest. Additionally, Elasticsearch is used to ingest and process data from various tools, allowing for fast searches and aggregations. I also learned about rapid response options for targeted data collections at scale using tools like Velociraptor and CyLR. Finally, we got solutions shown that are used for quickly processing acquired data for analysis in tools like Timesketch and individual artifact review.\nModern Attacks against Windows and Linux FIR (608.3) In the third chapter of the course the focus shifts from network-based analysis to classic host-based forensic artifact analysis. The start is to discuss modern attack techniques on Windows systems, including the infamous ransomware and ‚Äúliving-of-the-land‚Äù (LOTB) attacks that avoid detection by using built-in binaries and scripts.\nThe use of Sigma rules is highlighted as a way to facilitate rapid detection and response.\nThe chapter covers Linux incident response and analysis too, by starting with common vulnerabilities and exploits targeting Linux systems. Then it dives into DFIR fundamentals on Linux systems, including key concepts such as differences among Linux distributions and filesystems, and strategies for handling initial triage and deeper forensic analysis. The chapter concludes by providing best practices for hardening Linux systems and enhancing logging configurations to aid future investigations.\nAnalyzing macOS and Docker Containers (608.4) Now the focus went on to Apple macOS incident response, building on the foundation we got established earlier. This part includes understanding the history, ecosystem, and details of the Apple Filesystem (APFS), file structure, and important file types such as Property List (plist) configuration files. A discussion of challenges and opportunities in responding to macOS incidents follows, covering topics like acquiring disk and triage data, reviewing acquisitions, and identifying suspicious activity in logs and artifacts.\nThis part of the course then transitions to containerized microservices and Docker analysis, focusing on the architecture and management of Docker containers and providing a specific triage workflow for quick and effective response against individual containers as well as the container host.\nCloud Attacks and Response (608.5) This part focused on incident response in major cloud platforms from Microsoft and Amazon, covering log analysis techniques, architecture designs, and automation initiatives that can be applied across various cloud providers. It highlights unique challenges and opportunities in cloud environments, particularly through the use of the MITRE ATT\u0026CK framework‚Äôs Cloud Matrix.\nIn-depth discussion follows on Microsoft 365 (M365) and Azure, including popular SaaS offerings like Entra ID, Exchange, SharePoint, and Teams, as well as common attack scenarios against these platforms. The importance of log analysis is emphasized strongly, particularly in identifying suspicious user logon and email activity from Unified Audit Logs.\nThe course then addresses the Recovery phase, covering security enhancements to detect or prevent similar attacks in the future for M365 and Azure.\nNext, it delves into Amazon Web Services (AWS), covering its general architecture and components, as well as numerous logs and services providing critical detection and analysis data for responders. Discussions focus on architecting for response in the cloud, including setting up security accounts for a secure enclave within AWS, using template VMs (AMIs) for analysis, and automating IR tasks with AWS Lambda and Step Functions.\nCapstone: Enterprise-Class IR Challenge The final section of the course is the capstone exercise that allows students to apply their knowledge by working on a simulated breach scenario. They will receive a dataset from a compromised environment that spans multiple host operating systems and cloud environments, and use tools and techniques learned throughout the course to uncover the steps of the breach.\nKey Takeaways Summary of key concepts and skills learned during the course During the SANS FOR608 course, I learned concepts and skills that enabled me to do more effective incident response and coordination, including enterprise-level incident detection and to deploy threat hunting strategies. The course covered large-scale event correlation and timeline analysis techniques to identify patterns and trends in incidents, as well as multi-platform artifact analysis for incident response.\nSpecifically, I gained hands-on experience analyzing artifacts from various platforms, including Windows devices, Linux systems, macOS devices, containerized environments, and cloud-based infrastructure. This comprehensive training has equipped me with the knowledge and tools needed to detect, analyze, and respond to complex threats in enterprise environments.\nThe most fun was the parts where we learned about Timesketch and Velociraptor, I think each of those tools individually is extremely powerful, but when you integrate them into your threathunting / Response stack I thing they are of great benefit.\nlearning outcomes and their application in real-world scenarios Based on the provided course materials, I have analyzed my learning outcomes and their application in real-world scenarios. Through my analysis, I have gained a deeper understanding of the key concepts and skills required for effective cloud response and analysis, container DFIR fundamentals, detecting modern attacks, enterprise incident response management, enterprise visibility and incident scoping, foundational cloud concepts, Linux DFIR fundamentals, macOS DFIR fundamentals, macOS essentials, rapid response triage at scale.\nI have also gained practical knowledge of how to correlate large volumes of data to identify patterns and trends in incidents.\nIn particular, my experience with cloud-based infrastructure has highlighted the need for a comprehensive understanding of foundational cloud concepts, including popular cloud services that enterprises use to support business operations. I have also gained familiarity with common data source types in an enterprise environment and strategies to aggregate telemetry from disparate resources.\nMy analysis of learning outcomes suggests that effective application of these skills requires a combination of technical expertise, analytical thinking, and communication skills. By mastering these skills, I am confident in my ability to respond effectively to complex incidents and provide value to organizations as a security professional.\nConclusion and Recommendations Summary of overall effectiveness of the SANS Forensics course for608 SANS FOR608 course is a comprehensive training program which provides responders with a strong foundation in incident response, threat hunting, and digital forensic analysis. Through its curriculum, the course covers concepts and skills related to managing incident response teams, detecting threats in enterprise environments using advanced analytics tools, correlating large volumes of data to identify patterns and trends in incidents, analyzing artifacts from various platforms including Windows devices, Linux systems, macOS devices, containerized environments, and cloud-based infrastructure.\nAnalysis:\nComprehensive coverage: The course covers a wide range of topics related to incident response and digital forensic analysis, providing students with a comprehensive understanding of the subject matter. Hands-on experience: The course includes hands-on labs that allow participants to apply their knowledge in real-world scenarios, which helps to reinforce learning and improve retention. Practical skills: The course emphasizes practical skills over theoretical concepts, which is beneficial for security professionals who need to respond to incidents in a timely and effective manner. And I also think that pactical knowledge is more interessting to learn, because you can apply it in the following labs Real-world relevance: The course covers topics that are relevant to real-world scenarios responders are confronted with, making it easier for students to apply their knowledge in practical settings. Summary:\nFrom my personal opinion the SANS FOR608 course is very effective for providing students with a very well understanding of incident response and digital forensic analysis. Through its comprehensive coverage, hands-on exercises, and emphasis on practical skills, the course provides security professionals with the knowledge and skills needed to respond effectively to incidents.\nOverall, the course is well-structured, engaging, and relevant to real-world scenarios, making it an excellent choice for individuals looking to improve their incident response and digital forensic analysis skills.\nTho I have to say the on-demand course is way more exhausting I belive than the in person class. Also I think in person is more benificial beause you can discuss matters with your peers.\nRecommendations for future students looking to learn forensic analysis skills Gain Practical Experience\nBefore enrolling in a forensic analysis course, try to gain as much practical experience as possible for example practicing Sherlocks on hack the box or try yourself in Malware analysis challanges This could also involve setting up your own home lab, participating in bug bounty programs, or volunteering to help a friend or family member with their computer issues. The more hands-on experience you have, the better equipped you‚Äôll be to learn and apply forensic analysis skills.\nDevelop Your Analytical Skills\nForensic analysis requires strong analytical skills, including attention to detail, critical thinking, and problem-solving. Practice these skills by working on puzzles, brain teasers, or other activities that challenge your mind. You can also try analyzing data sets, network traffic logs, or system logs to develop your skills.\nLearn about Cloud Computing\nAs a forensic analyst, it‚Äôs essential to understand cloud computing and how it affects the analysis of digital evidence. Take online courses or attend webinars that teach you about cloud security, compliance, and investigation techniques. This will help you stay up-to-date with the latest trends and technologies.\nFamiliarize Yourself with Linux and macOS\nLinux and macOS are popular operating systems used by many organizations, including those in the finance, healthcare, and government sectors. Take online courses or attend workshops that teach you about these operating systems, including their command-line interfaces, file systems, and security features.\nJoin Online Communities\nJoining online communities, such as Reddit‚Äôs r/learnprogramming or r/netsec, can be a great way to connect with other professionals in the field, ask questions, and learn from their experiences. You can also participate in online forums, attend webinars, or join online study groups to stay updated on the latest forensic analysis techniques.\nConsider Specializing in a Specific Area\nForensic analysis is a broad field that encompasses many areas, including computer forensics, mobile device forensics, and digital evidence collection. Consider specializing in a specific area that interests you the most, such as incident response or threat hunting. This will help you develop deeper knowledge and skills in that area.\nStay Up-to-Date with Industry Developments\nThe field of forensic analysis is constantly evolving, with new technologies and techniques emerging regularly. Stay up-to-date with industry developments by attending conferences, webinars, or online courses that focus on the latest trends and advancements.\nDONE Open BSD and Zen @personalopenbsdzen About As someone who is passionate about security and has an interest in Unix operating systems, OpenBSD particularly captivates due to its dedication to security, stability, and simplicity. In comparison to other OSes, what sets OpenBSD apart? And how do these principles align with my journey through Zen meditation?\nAt first glance, OpenBSD and Zen may appear to be vastly disparate concepts - one being a potent operating system, while the other is a spiritual practice originating from ancient China. However, as I delved deeper into both realms, I uncovered some fascinating similarities.\nSimplicity and Clarity In Zen, simplicity is key to achieving inner clarity and balance. By stripping away unnecessary complexity, OpenBSD aims to create a stable and secure foundation for users. Similarly, in meditation, simplicity helps to quiet the mind and focus on the present moment. This alignment between OpenBSD‚Äôs philosophy and Zen practices extends to their shared emphasis on mindfulness and deliberate decision-making, fostering an environment of security and tranquility in both realms.\nAttention to Detail Both OpenBSD and Zen underscore the significance of attending to detail. In software development, this entails meticulously crafting each line of code to guarantee stability and security. In Zen practice, it involves paying close attention to one‚Äôs breath, posture, and mental state to attain a state of mindfulness. By zeroing in on these details, both OpenBSD and Zen strive for perfection.\nThe Power of Consistency OpenBSD‚Äôs dedication to consistency is manifested in its codebase, where each code change undergoes a thorough code review process. Consistency holds equal importance in Zen practice, as it fosters a sense of routine and stability. By cultivating a consistent daily meditation practice, I have discovered that consistency is instrumental in making progress on my spiritual journey. OpenBSD‚Äôs emphasis on consistency mirrors the principles of Zen, emphasizing the value of diligence and discipline in both domains.\nThe Beauty of Imperfection Finally, both OpenBSD and Zen acknowledge the elegance in imperfection. In software development, imperfections can often be rectified or lessened through meticulous design and testing. In Zen practice, imperfections are perceived as avenues for growth and self-awareness.\nBy acknowledging our imperfections, we can nurture humility and compassion. As I progress in my journey with OpenBSD and Zen, I am consistently struck by the ways in which these two seemingly unrelated realms intersect. By embracing simplicity, attention to detail, consistency, and the beauty of imperfection, both OpenBSD and Zen provide unique perspectives on the nature of software development and personal growth. Stay tuned for further insights from my exploration in the realm of security!\nDONE How to monitor systems with monit openbsdpersonalvisibility Introduction intro Monitoring a router is something many people forget about, especially at home. But a router is the heart of the network ‚Äî when it fails, everything fails.\nOpenBSD already provides a strong foundation for reliability and security. By adding Monit (a lightweight monitoring tool) and using Pushover (simple mobile notifications), you can build a robust alerting and monitoring setup that works even on small hardware.\nThis article shows how to install, configure, and use Monit to watch essential router services and send push notifications with Pushover.\nRequirements requirements To follow this guide you need:\nOpenBSD router (any supported version) Monit installed from packages Basic shell access A Pushover account and an API token (application token) Your Pushover user key All configuration happens in /etc/monitrc.\nInstalling Monit on OpenBSD install Installing Monit on OpenBSD is simple:\npkg_add monit After installation, enable Monit so that it starts automatically:\necho \"monit_flags=\" \u003e\u003e /etc/rc.conf.local rcctl enable monit rcctl start monit Monit‚Äôs main configuration file is:\n/etc/monitrc You must set permissions correctly, because Monit refuses unsafe files:\nchmod 600 /etc/monitrc Monit ‚Äì Essential System and Router Services System monitoring runs every 45 seconds. The first check is delayed by 120 seconds to avoid overloading the system immediately after boot.\nset daemon 45 with start delay 120 Monit logs to syslog. `idfile` and `statefile` store Monit‚Äôs persistent state and identity across restarts.\nset log syslog set idfile /var/monit/id set statefile /var/monit/state Limits control buffer sizes and timeouts for program outputs, network I/O, and service start/stop/restart operations. This prevents Monit from hanging or processing excessive data.\nset limits { programOutput: 512 B, sendExpectBuffer: 256 B, fileContentBuffer: 512 B, httpContentBuffer: 1 MB, networkTimeout: 5 seconds programTimeout: 300 seconds stopTimeout: 30 seconds startTimeout: 30 seconds restartTimeout: 30 seconds } Monit will send alerts via local email. Events are queued under `/var/monit/events` to prevent message loss during temporary network problems.\nset mailserver localhost set eventqueue basedir /var/monit/events slots 200 set mail-format { from: root@monit } set alert root@localhost not on { instance, action } Simply comment out or delete all `set alert` entries:\n# set alert root@localhost not on { instance, action } After this, Monit will not send any emails, but it will still monitor services.\nMonit HTTP interface is on port 2812. Access is restricted to localhost, a local subnet (`192.168.X.0/24`), and an admin user with a password.\nset httpd port 2812 and allow localhost allow 192.168.X.0/255.255.255.0 allow admin:foobar Monit will start all monitored services automatically on reboot.\nset onreboot start This monitors overall system health:\n1- and 5-minute load per CPU core CPU usage Memory and swap usage If thresholds are exceeded, it triggers `pushover.sh` for alerts.\ncheck system $HOST if loadavg (1min) per core \u003e 2 for 5 cycles then exec /usr/local/bin/pushover.sh if loadavg (5min) per core \u003e 1.5 for 10 cycles then exec /usr/local/bin/pushover.sh if cpu usage \u003e 95% for 10 cycles then exec /usr/local/bin/pushover.sh if memory usage \u003e 75% then exec /usr/local/bin/pushover.sh if swap usage \u003e 25% then exec /usr/local/bin/pushover.sh group system `/home` filesystem is monitored for:\nDisk space and inode usage Read/write throughput (MB/s and IOPS) Service response time Alerts are sent via `pushover.sh` if any threshold is exceeded.\ncheck filesystem home_fs with path /dev/sd0k start program = \"/sbin/mount /home\" stop program = \"/sbin/umount /home\" if space usage \u003e 90% then exec /usr/local/bin/pushover.sh if inode usage \u003e 95% then exec /usr/local/bin/pushover.sh if read rate \u003e 8 MB/s for 20 cycles then exec /usr/local/bin/pushover.sh if read rate \u003e 800 operations/s for 15 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 8 MB/s for 20 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 800 operations/s for 15 cycles then exec /usr/local/bin/pushover.sh if service time \u003e 10 milliseconds for 3 times within 15 cycles then exec /usr/local/bin/pushover.sh group system Root filesystem `/` has similar checks but shorter cycles since it‚Äôs critical to system stability.\ncheck filesystem root_fs with path /dev/sd0a start program = \"/sbin/mount /\" stop program = \"/sbin/umount /\" if space usage \u003e 90% then exec /usr/local/bin/pushover.sh if inode usage \u003e 95% then exec /usr/local/bin/pushover.sh if read rate \u003e 8 MB/s for 5 cycles then exec /usr/local/bin/pushover.sh if read rate \u003e 800 operations/s for 5 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 8 MB/s for 5 cycles then exec /usr/local/bin/pushover.sh if write rate \u003e 800 operations/s for 5 cycles then exec /usr/local/bin/pushover.sh if service time \u003e 10 milliseconds for 3 times within 5 cycles then exec /usr/local/bin/pushover.sh group system Monit ensures secure permissions for `/root`. If permissions are wrong, monitoring for this directory is disabled to avoid false alarms.\ncheck directory bin with path /root if failed permission 700 then unmonitor if failed uid 0 then unmonitor if failed gid 0 then unmonitor group system A network host is ping-checked. Frequent failures trigger alerts. Dependencies on interfaces and services ensure checks only run when the network is up.\ncheck host homeassistant with address 192.168.X.19 if failed ping then alert if 5 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_in,dhcpd,unbound Monit watches network interface `pppoeX`:\nRestarts interface if link goes down Alerts on saturation or high upload Limits repeated restarts to avoid loops check network iface_out with interface pppoeX start program = \"/bin/sh /etc/netstart pppoeX\" if link down then restart else exec /usr/local/bin/pushover.sh if changed link then exec /usr/local/bin/pushover.sh if saturation \u003e 90% then exec /usr/local/bin/pushover.sh if total uploaded \u003e 5 GB in last hour then exec /usr/local/bin/pushover.sh if 5 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network DNS resolver `unbound` is monitored by PID and port. Failures trigger a restart, repeated failures trigger alerts.\ncheck process unbound with pidfile /var/unbound/unbound.pid start program = \"/usr/sbin/rcctl start unbound\" stop program = \"/usr/sbin/rcctl stop unbound\" if failed port 53 for 3 cycles then restart if 3 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network depends on dnscrypt_proxy,iface_out,iface_in DHCP server is monitored. Missing process triggers a restart. Alerts are sent if failures happen repeatedly.\ncheck process dhcpd with matching /usr/sbin/dhcpd start program = \"/usr/sbin/rcctl start dhcpd\" stop program = \"/usr/sbin/rcctl stop dhcpd\" if does not exist then restart if 2 restarts within 10 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_in NTP daemon ensures time synchronization. Missing process triggers restart; repeated issues generate alerts.\ncheck process ntpd with matching /usr/sbin/ntpd start program = \"/usr/sbin/rcctl start ntpd\" stop program = \"/usr/sbin/rcctl stop ntpd\" if does not exist then restart if 5 restarts within 5 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_out vnStat daemon monitors network traffic statistics. Monit restarts it if it stops and alerts on repeated failures.\ncheck process vnstatd with matching /usr/local/sbin/vnstatd start program = \"/usr/sbin/rcctl start vnstatd\" stop program = \"/usr/sbin/rcctl stop vnstatd\" if does not exist then restart if 5 restarts within 15 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_out Adding Pushover Alerts pushover Pushover provides a simple HTTPS API for sending notifications to your phone.\nMonit can call an external script. Create /usr/local/bin/pushover.sh:\n#!/bin/sh TOKEN=\"YOUR_PUSHOVER_API_TOKEN\" USER=\"YOUR_PUSHOVER_USER_KEY\" /usr/local/bin/curl -s \\ -F \"token=$TOKEN\" \\ -F \"user=$USER\" \\ --form-string \"message=[$MONIT_HOST] $MONIT_SERVICE - $MONIT_EVENT - $MONIT_DESCRIPTION\" \\ https://api.pushover.net/1/messages.json Make it executable:\nchmod +x /usr/local/bin/pushover.sh Now the checks which contain the ‚Äúexec /usr/local/bin/pushover.sh‚Äù line will trigger pushover notifications:\ncheck process vnstatd with matching /usr/local/sbin/vnstatd start program = \"/usr/sbin/rcctl start vnstatd\" stop program = \"/usr/sbin/rcctl stop vnstatd\" if does not exist then restart if 5 restarts within 15 cycles then exec /usr/local/bin/pushover.sh group network depends on iface_out Monit will automatically send the full text of the event to Pushover.\nTesting and Maintenance ops Test your configuration monit -t # syntax check monit reload # reload configuration monit summary # Show command line overview monit status vnstatd # Show check status Conclusion conclusion Using Monit together with Pushover is an excellent way to keep a close eye on an OpenBSD router. Monit is tiny, fast, and reliable ‚Äî perfect for embedded hardware. Pushover provides instant alerts with almost no configuration or overhead.\nFor a home router or small business network, this combination gives you semi professional-grade monitoring with minimal effort.\nDONE Fixing Yellow Shards in Elasticsearch Introduction If you‚Äôre running Elasticsearch on a single node ‚Äî like a Raspberry Pi or small lab setup like I am ‚Äî you might notice some indices appear with a yellow health status.\nThis show article explains what that means and how to fix it, especially in resource-constrained, single-node environments.\nWhat Does ‚ÄúYellow‚Äù Mean? In Elasticsearch:\ngreen: All primary and replica shards are assigned and active. yellow: All primary shards are active, but at least one replica shard is unassigned. red: At least one primary shard is missing ‚Üí critical! Why Yellow Happens on Single Nodes In single-node clusters, Elasticsearch cannot assign replica shards (because replicas must be on a different node). So any index with replicas will always be yellow unless:\nYou add more nodes (not ideal on a Raspberry Pi) Or: You disable replicas (number_of_replicas: 0) Step-by-Step: Diagnose Yellow Shards 1. List all yellow indices GET _cat/indices?v\u0026health=yellow 2. See why a shard is unassigned GET _cluster/allocation/explain 3. Inspect shard assignment of a specific index GET _cat/shards/.monitoring-beats-7-2025.08.06?v Example output:\nindex shard prirep state docs store ip node .monitoring-beats-7-2025.08.06 0 p STARTED 7790 5.9mb 127.0.0.1 mynode .monitoring-beats-7-2025.08.06 0 r UNASSIGNED ‚Üí The r (replica) is unassigned ‚Üí yellow status.\nHow to Fix It A. Fix an individual index Set replicas to zero:\nPUT .monitoring-beats-7-2025.08.06/_settings { \"index\" : { \"number_of_replicas\" : 0 } } This changes the index health from yellow to green.\nB. Automatically fix all yellow indices If you want to automate the fix, use this (Kibana Dev Tools):\nGET _cat/indices?health=yellow\u0026format=json Then for each index in the result:\nPOST \u003cyour_index\u003e/_settings { \"index\": { \"number_of_replicas\": 0 } } C. Prevent future yellow indices Disable replicas by default using an index template:\nPUT _template/no-replica-default { \"index_patterns\": [\"*\"], \"settings\": { \"number_of_replicas\": 0 } } \u003e ‚ö†Ô∏è This applies to all future indices. Only do this in single-node environments.\nConclusion Yellow indices aren‚Äôt dangerous by default ‚Äî they just mean you‚Äôre missing redundancy. In small environments, it‚Äôs perfectly safe to run with zero replicas.\nJust don‚Äôt forget to:\nMonitor your shard health Disable replicas if you only have one node Automate where you can DONE Rescue to the softraid personal@forensicopenbsd Introduction So I had this USB Disk attached to my OpenBSD Router used as storage, one saturday when I was walking by I noticed the weird clicking sounds from the disk. So I knew my time was running before the disc would fail.\nCuriously, when I plugged the same drive into a Linux box, it was detected ‚Äî and even showed a valid OpenBSD partition table. That gave me a glimmer of hope: maybe the hardware wasn‚Äôt completely dead yet.\nSo, for fun (and a little bit of stubborn curiosity), I decided to spend the weekend seeing how much I could rescue from it.\nThis post documents the process ‚Äî part forensic experiment, part recovery attempt, and part ‚Äúlet‚Äôs see what happens if I do this.‚Äù\nPhase 1: Identifying the Disk under Linux Before doing anything risky, I wanted to be sure I was imaging the right disk. The idea was to identify the OpenBSD partition and dump it to an image file.\nListing block devices lsblk -o NAME,SIZE,FSTYPE,TYPE,LABEL,UUID That gives a good overview ‚Äî which disks are present, how large they are, and what filesystems they contain. Sure enough, my external USB drive showed up as `/dev/sda`.\nInspecting partition table sudo fdisk -l /dev/sda Example output:\nDisk /dev/sda: 931.5 GiB, 1000204883968 bytes, 1953525164 sectors Disk model: External USB 3.0 Sector size: 512 bytes Disklabel type: dos Device Boot Start End Sectors Size Id Type /dev/sda4 * 64 1953525163 1953525100 931.5G a6 OpenBSD Perfect. The OpenBSD partition was still there (`/dev/sda4`), and it even reported the correct size.\nThe Start sector (64) is important later for offset calculations. Type a6 OpenBSD confirmed the filesystem was OpenBSD-specific (likely softraid). Knowing the sector size (512 bytes) ensured that later tools like `dd` or `ddrescue` wouldn‚Äôt misalign reads. At this point, the goal was to make a bit-for-bit copy of that partition, compress it, and work on the image rather than risk further damage to the actual disk.\nPhase 2: Creating a Compressed Disk Image For imaging, I decided to use GNU ddrescue ‚Äî it‚Äôs great for flaky disks and can retry sectors intelligently.\nInstalling ddrescue On Fedora, installation was trivial:\nsudo dnf install ddrescue First Attempt (Quick and Dirty) I tried a fast, one-shot dump ‚Äî not ideal for a failing disk, but I wanted to see if it would work at all:\nsudo ddrescue -d -r3 /dev/sda4 - - | xz -T0 -c \u003e openbsd_sda4.img.xz That command streams data directly from the device, compresses it with xz, and writes the result. It works ‚Äî if the disk is healthy. Mine wasn‚Äôt, so it failed partway through.\nSecond Attempt (Proper Forensic Mode) So I switched to the safer, resumable method:\nsudo ddrescue -d -r3 /dev/sda4 openbsd_sda4.img openbsd_sda4.log xz -T0 openbsd_sda4.img sha256sum openbsd_sda4.img \u003e openbsd_sda4.img.sha256 This time, ddrescue created a detailed log file so I could resume later if the system froze or the disk disconnected. It took most of the night, but eventually I had a clean (or mostly clean) image.\nExplanation of parameters\n-r3 retries each bad block 3 times -d enables direct disk I/O The `.log` file lets you stop and restart without losing progress xz -T0 uses all CPU cores for compression After the dump, I verified the output:\nls -lh openbsd_sda4.img.xz xz -t openbsd_sda4.img.xz # test integrity sha256sum openbsd_sda4.img.xz \u003e openbsd_sda4.img.xz.sha256 Everything checked out ‚Äî a ~450 GB compressed image file safely sitting on my main system.\nPhase 3: Simulating Disk Damage (For Fun and Testing) Since the real disk was unstable, I wanted a safe way to experiment. So I created a copy of the image and simulated damage to practice recovery techniques.\nCreating the test image sudo dd if=/dev/sda4 of=openbsd_sda4.img bs=4M status=progress Simulating corruption To emulate bad sectors:\ndd if=/dev/zero of=openbsd_sda4.img bs=512 count=10 seek=1000 conv=notrunc Now the image contained 10 intentionally corrupted sectors ‚Äî perfect for testing.\nRecovering from the damaged image ddrescue -d -r3 openbsd_sda4.img openbsd_sda4_recovered.img openbsd_sda4_recovery.log And just like that, I could practice recovery without touching the actual hardware again.\nOptional Compression xz -T0 openbsd_sda4.img It‚Äôs amazing how much you can still do with raw disk images and a few classic Unix tools.\nPhase 4: Performance Tuning and System Stability During the rescue, I learned (the hard way) that ddrescue can saturate I/O and make your system lag like crazy. So I ended up using this combination for a gentler approach:\nsudo ionice -c2 -n7 nice -n19 ddrescue -b 4096 -B 4096 /dev/sda4 openbsd_sda4.img And, for long operations, running it inside tmux:\ntmux new-session -s rescue sudo ddrescue -d -r3 /dev/sda4 openbsd_sda4.img openbsd_sda4.log # Detach with Ctrl-B D Later, I could simply:\ntmux attach -t rescue That setup saved me more than once when I accidentally closed an SSH session.\nPhase 5: Next Steps ‚Äî Future Analysis Once I had a full image, the plan was to:\nDecompress it (unxz openbsd_sda4.img.xz) Attach it as a loopback device under Linux, or use vnconfig under OpenBSD Attempt to reassemble the softraid volume using bioctl If all goes well ‚Äî mount the decrypted filesystem and access my old data That‚Äôs a topic for another weekend. But getting to this point already felt like a small victory.\nConclusion What started as a ‚Äúlet‚Äôs see if I can still read this disk‚Äù experiment turned into a proper mini-forensics exercise. Even though the original USB drive was dying, I managed to preserve most of its data and learned a ton in the process.\nAllover it was quite fun to do something forensics related on a OpenBSD target, I guess it is something you don‚Äôt come across everyday but when you do its good to be prepared I think.\nKey takeaways:\nddrescue is your friend for unstable media Always work on images, not the original device Compression and checksums are cheap insurance And most importantly: never underestimate what you can recover with a bit of patience and Unix philosophy Not a bad way to spend a weekend.\nAppendix Device summary Device: /dev/sda Partition: /dev/sda4 Size: ~931 GiB Partition type: a6 (OpenBSD) Start sector: 64 Sector size: 512 bytes Estimated time and storage Depending on USB speed:\nImaging took about 2‚Äì3 hours Compressed image size: ~40‚Äì60% of original Tools used dd, ddrescue, xz fdisk, lsblk, sha256sum tmux, ionice, dstat, iotop DONE Putting my gpg key on my yubikey Why GPG? In an age where digital identities are easily faked and impersonation is just a few clicks away, I decided to take a step forward in securing mine. GPG (GNU Privacy Guard) provides a robust way to authenticate, encrypt, and sign digital content. In this post, I‚Äôll walk you through how I:\nCreated a GPG key pair Set up subkeys and stored them on my YubiKey Published my public key on my website Signed and encrypted personal documents for secure public sharing Configured email signing using GPG Step 1: Installing GPG To start, I made sure GPG was installed. Here‚Äôs how I did it on each of my systems:\nOn Ubuntu/Debian:\nsudo apt update \u0026\u0026 sudo apt install gnupg On Fedora 40:\nsudo dnf install gnupg2 On OpenBSD 7.6:\ndoas pkg_add gnupg Check your installation:\ngpg --version Step 2: Creating My GPG Key Pair I created a new key using:\ngpg --full-generate-key Here‚Äôs what I chose:\nKey type: ed25519 (modern and compact) or RSA and RSA (widely compatible) Key length: 4096 bits (if RSA) Expiration: 2 years (I can always renew) My real name or handle My preferred contact email A strong passphrase, saved in a password manager After generating the key, I listed it and saved the fingerprint:\ngpg --list-keys --fingerprint gpg: \"Trust-DB\" wird √ºberpr√ºft gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: Tiefe: 0 g√ºltig: 1 signiert: 0 Vertrauen: 0-, 0q, 0n, 0m, 0f, 1u gpg: n√§chste \"Trust-DB\"-Pflicht√ºberpr√ºfung am 2026-08-04 [keyboxd] --------- pub ed25519 2025-08-04 [SC] [verf√§llt: 2026-08-04] A371 9309 4ED4 B0E6 AD2E 5022 D7D6 4842 8DBD 39FD uid [ ultimativ ] Dirk.L (Dirk.L's official key) \u003cpolymathmonkey@keksmafia.org\u003e Step 3: Creating Subkeys and Moving Them to My YubiKey I created subkeys for:\nSigning Encryption Authentication Then, I moved the subkeys to my YubiKey using GPG‚Äôs interactive editor:\ngpg --edit-key Dirk.L gpg\u003e addkey \u003c- once for signing, engryption, auth gpg\u003e keytocard gpg\u003e save ‚ö†Ô∏è Be cautious: Once moved to the YubiKey, the subkey no longer exists on disk.\nMore guidance: YubiKey + GPG official instructions\nStep 4: Publishing My Public Key I exported my key in ASCII format so others could import it easily:\ngpg --export --armor you@example.com \u003e publickey.asc I uploaded publickey.asc to my website and linked it like this:\n\u003ca href=\"/publickey.asc\"\u003eüîë Download my GPG public key\u003c/a\u003e Additionally, I displayed my key‚Äôs fingerprint on the page so that people can verify its authenticity manually.\nStep 5: Email Signing and Encryption I configured email signing using my GPG key.\nFor Thunderbird (Linux, OpenBSD, Windows):\nOpenPGP support is built-in. I enabled signing for all outgoing mail. The key lives on the YubiKey, so no key is stored on disk. For Mutt / CLI mailers:\nI used `gpg-agent` for passphrase and key handling. Configured .muttrc to sign and/or encrypt automatically. Signing ensures message authenticity. If recipients have my key, they can encrypt replies.\nStep 6: Signing and Encrypting Documents for the Public To safely share personal certificates and private files, I signed and optionally encrypted them:\n# Sign only (adds signature block) gpg --sign --armor diploma.pdf # Sign and encrypt with a password (no public key needed) gpg --symmetric --armor --cipher-algo AES256 diploma.pdf This way, the document is verifiably mine and only decryptable with the shared password.\nThe encrypted .asc files can be uploaded to the website, with instructions for downloading and decrypting.\nStep 7: Offline Backup of My Master Key Before moving entirely to the YubiKey, I backed up the master key offline:\ngpg --export-secret-keys --armor \u003e masterkey-backup.asc I stored it on an encrypted USB drive with either one:\nLUKS (on Linux) OpenBSD softraid(4) encryption Conclusion Rolling out GPG was super easy. With my identity cryptographically verifiable, email signing in place, and secure document sharing live on my site, I now have a strong, decentralized identity system.\nUseful Links GnuPG Official Website FSF‚Äôs Email Self-Defense Guide YubiKey GPG Configuration OpenPGP Public Key Directory https://www.sans.org/cyber-security-courses/enterprise-incident-response-threat-hunting/¬†‚Ü©Ô∏é",
    "description": "DRAFT My travel and stay at buddhas weg @personal DRAFT Collecting netflow data with pflow(1) and logstash @threathuntingopenbsdvisibility DRAFT Joe Weitzenbaum forever Warum weitzenbaum heute wichtiger ist als emails zuvor Josef Weizenbaum ‚Äì approximiert, aus dem Nebel der Bedeutung",
    "tags": [
      "Forensicwheels"
    ],
    "title": "Forensic Wheels",
    "uri": "/posts/all-posts/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Personal",
    "uri": "/tags/personal/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Visibility",
    "uri": "/tags/visibility/index.html"
  },
  {
    "breadcrumb": "Welcome",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category - Forensic",
    "uri": "/categories/forensic/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Honeypot",
    "uri": "/tags/honeypot/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Threathunting",
    "uri": "/tags/threathunting/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category - Threathunting",
    "uri": "/categories/threathunting/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category - Personal",
    "uri": "/categories/personal/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Zen",
    "uri": "/tags/zen/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Canarytokens",
    "uri": "/tags/canarytokens/index.html"
  }
]
