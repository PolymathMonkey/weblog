#+hugo_base_dir: ../
#+hugo_section: ./posts/
#+hugo_weight: auto
#+hugo_auto_set_lastmod: t
#+title: Adventures in threathunting
#+seq_todo: DRAFT TODO DONE
#+FILETAGS: :threathunting:
#+TAGS: @personal @forensic @zen @threathunting
#+TAGS: openbsd honeypot zen personal canarytokens skateboarding visibility

* DONE Threathunting I: Network setup                                                     :@threathunting:honeypot:visibility:
CLOSED: [2025-07-08 Di 09:15]
:PROPERTIES:
:EXPORT_AUTHOR: Dirk
:EXPORT_HUGO_FRONT_MATTER_FORMAT: yaml
:EXPORT_OPTIONS: toc:2
:HUGO_TITLE: Threathunting at home
:EXPORT_FILE_NAME: threathuntingnet
:EXPORT_DATE: 2025-05-26T16:21:00-05:00
:CUSTOM_ID: threathuntingathome
:END:
** Introduction

This is a small series I wanted to start, where I write about my small
threathunting setup and describe a little what I build and what I am doing
with it.

In this part, I will describe the Network setup for my Environment, more about
how I build the honeypots and the ELK Server I will describe in the follow up
articles about threathunting.

Keep in mind this is for Education and fun, no serious stuff going on here.

*** Why I Built a Home Lab for Threat Hunting  üïµ
The threat landscape is constantly evolving, with new attack vectors, tools,
and tactics appearing almost daily.  

And to keep my skills current with real-world threats, I built a home lab dedicated
to threat hunting. This environment allows me to safely observe attacks and
develop detection and defense methods. I deployed web and shell honeypots,
and collect real threat data in a controlled setting.

It‚Äôs a practical, hands-on way to explore the behavior of adversaries and its a
lot of fun!

** Network Setup
*** Topology, Hardware and Tools üõ†

[[../img/mynet.png]]

For the **hardware setup**, I kept things lightweight and affordable by using
Raspberry Pi devices and open-source tools. The honeypot is based on the
well-known [[https://docs.cowrie.org/en/latest/][Cowrie SSH honeypot]] and the [[https://github.com/bocajspear1/honeyhttpd][honeyhttpd HTTP honeypot]] .
It runs on a **Raspberry Pi 4 with 8GB of RAM**, hosted inside a Docker üê≥
container. On the honeypot host, **Filebeat** is running to ingest the Cowrie
logs into the ELK stack. @@comment: Write about honeypot setup @@

For the **ELK stack**, I used a **Raspberry Pi 5 with 16GB of RAM**, running
Debian. The ELK services are also containerized using Docker. The stack is
based on the [[https://github.com/bruneaug/DShield-SIEM][DShield-SIEM]] project, which I customized to better fit
my needs. I‚Äôll dive deeper into those modifications and the ELK setup in
a follow-up article.

The network topology is straightforward but deliberately segmented. The router
is connected to a managed switch, which is responsible for handling VLAN
separation. Both the honeypot and the ELK server are connected to this switch
and are placed in an **isolated VLAN (VLAN210)**. This VLAN is dedicated
exclusively to **threat hunting**, ensuring that any potentially malicious
traffic remains fully contained and cannot interfere with the rest of the
home network.

My client system üíª is the only machine allowed to connect from outside the
VLAN to both the ELK server and the honeypot. This connection is strictly
for maintenance and administrative purposes. The ELK server is allowed to
access the internet, primarily to pull threat intelligence data from
external sources and security feeds.

In contrast, the **honeypot** is completely **blocked from internet access**,
with the exception of **SSH and HTTP traffic** going in and out of it. These
are the only services deliberately exposed to simulate vulnerable endpoints.
Communication between the honeypot and the ELK server is allowed for log
ingestion and analysis. However, I intend to introduce stricter controls on
this internal traffic in the future to further reduce the attack surface.

*** Firewall configurationüß± 
For the pf(1) configuration It was as always with UNIX fairly easy to get to work:
#+begin_src sh
match in quick log on egress proto tcp from any to any port 22 flags S/SA rdr-to $honeypot port 2222
match in quick log on egress proto tcp from any to any port 443 flags S/SA rdr-to $honeypot port 4433 
#+end_src

This rule makes sure any incoming TCP connection attempt to port 22 (SSH) and
port 443 (HTTPS) is immediately intercepted, logged, and transparently
redirected to the $honeypot server listening on port 2222 or 4433 for HTTPS
Traffic.@@comment: Link to article with full ruleset@@

*** Switch configuration
@@comment: TBD@@
Here you can see my managed switch configuration. Port 5 (honeypot) and port 3
(ELK) is assigned to VLAN210, port 2 is the router it needs to talk into both
networks and at port 1 is my workstation to access the theathunting
environment.

[[../img/switch.png]]


** What I Learned
Building and maintaining this lightweight honeypot and monitoring setup on
Raspberry Pi devices has been an insightful experience. Here are some key takeaways:

- **Resource Efficiency**: Raspberry Pis provide a surprisingly capable
  platform for running complex services like Cowrie honeypot and the ELK stack
  in Docker containers, keeping costs and power consumption low.

- **Network Segmentation Matters**: Isolating the honeypot and ELK server in a
  dedicated VLAN (VLAN210) effectively contains malicious traffic, protecting
  the rest of the home network from potential threats.

- **Controlled Access Is Crucial**: Restricting external access to only
  authorized    clients and limiting the honeypot's internet connectivity
  reduces the attack surface while still enabling useful data collection.

- **Logging and Data Collection**: Using Filebeat to ship logs from the
  honeypot to the ELK stack provides real-time visibility into attacker
  behavior, which is essential for threat hunting and incident response.

- **Customization Pays Off**: Adapting existing tools and SIEM projects
  (like DShield) to specific needs improves effectiveness and allows for
  tailored threat detection.

- **Future Improvements**: There is always room to tighten internal
  communication rules and harden the setup further to minimize risk and
  improve operational security.

This project highlights the balance between practical constraints and security
needs, demonstrating that even modest hardware can contribute significantly
to threat intelligence and network defense.

I drew inspiration for this setup from the DShield SIEM project by SANS and
would like to express my gratitude for their valuable work.

** Whats next
Next I had to build the [[~/../theathuntinghoneypot][ssh honeypot]] and the [[~/../honeyhttpd][HTTP Honeypot]], stay tuned for the
follow up!

#+BEGIN_EXPORT md
{{< giscus >}}
#+END_EXPORT



* DONE Threat hunting II: SSH Honeypot setup                                            :@threathunting:honeypot:
CLOSED: [2025-09-29 Mo 07:18]
:PROPERTIES:
:EXPORT_AUTHOR: Dirk
:EXPORT_HUGO_FRONT_MATTER_FORMAT: yaml
:EXPORT_OPTIONS: toc:2
:HUGO_TITLE: Threathunting at home
:EXPORT_FILE_NAME: theathuntinghoneypot
:EXPORT_DATE: 2025-05-26T16:21:00-05:00
:CUSTOM_ID: theathuntingathome
:END:
** Introduction
This post provides a brief walkthrough of how to deploy a lightweight,
containerized SSH honeypot using Cowrie and Podman, with the goal of
capturing and analyzing malicious activity as part of my threat hunting
strategy.

** What is Cowrie?
Cowrie is an interactive SSH and Telnet honeypot designed to emulate a
real system, capturing attacker behavior in a controlled environment.
It allows defenders and researchers to observe malicious activity without
exposing actual infrastructure.

_Key capabilities of Cowrie include_

- *Full session logging*: Records all commands entered by the attacker,
  along with input/output streams and timing data. Sessions can be saved
  as plaintext or in formats suitable for replay.

- *Fake file system and shell environment*: Emulates a basic Linux shell
  with a user-modifiable file system. Attackers can navigate directories,
  read/write fake files, or attempt to download/upload payloads.

- *Command emulation*: Supports a large set of common Unix commands (`ls`,
  `cat`, `wget`, etc.), allowing attackers to interact naturally, as
  if on a real system. And can be extended with more commands

- *Credential logging*: Captures usernames and passwords used in
  brute-force login attempts or interactive logins.

- *File download capture*: Logs and optionally stores any files attackers
  attempt to retrieve via `wget`, `curl`, or similar tools.

- *JSON-formatted logging and integration's*: Outputs structured logs that
  are easy to parse and ingest into systems like ELK, Splunk, or custom
  analysis pipelines.

Cowrie is widely used in research, threat intelligence, and proactive defense
efforts to gather Indicators of Compromise (IOCs) and understand attacker
tactics,techniques, and procedures (TTPs).

** Why Podman over Docker?
Podman offers several advantages over Docker, particularly in terms of security
and system integration. It supports rootless containers, allowing users to run
containers without elevated privileges, which reduces the attack surface.  

Podman is daemon-less, integrating more seamlessly with systemd and existing
Linux workflows. Additionally, Podman is fully compatible with the Open
Container Initiative (OCI) standards, ensuring interoperability and
flexibility across container ecosystems.


** Preconditions / System setup
Before I proceed with the cowrie setup, I made sure the following preconditions are met:

*** Ubuntu Installed on Raspberry Pi 4+

I am using a Raspberry Pi 4+ running Ubuntu

*** System Fully Updated

After installation, I made sure system is up to date:

#+begin_src bash
sudo apt update && sudo apt upgrade -y
#+end_src

*** Podman installed and working
#+begin_src sh
# Ubuntu 20.10 and newer
sudo apt-get -y install podman
#+end_src

Run the Hello World Container.In this moment I did not had the cowrie user yet
setup so I used my system user to test

#+begin_src bash
  podman run hello-world
  Trying to pull docker.io/library/hello-world:latest...
  ...
  Hello from Docker!
  This message shows that your installation appears to be working correctly.
#+end_src

tho sometimes the pulling fails like that then I had to put `docker.io` in
front of the container name like:
#+begin_src sh
  podman run docker.io/hello-world  
#+end_src

then it would work for sure.

*** VLAN Tagging Configured on Network Interface

In my network setup for threathunting the honeypot requires VLAN tagging to
configured to reachable from the outside, VLAN210 is my restricted Network.
Therefore i needed to configure the vlan using =nmcli= so it's persistent across reboots.

**** Example: Create a VLAN interface (e.g., VLAN ID 210 on main if)

#+begin_src bash
sudo nmcli con add type vlan con-name vlan210 dev mainif id 210 ip4 192.168.210.3/24 gw4 192.168.210.1
sudo nmcli con up vlan210
#+end_src

- =con-name vlan210=: Name of the new VLAN connection.
- =dev mainif=: Physical interface to tag.
- =id 210=: VLAN ID.
- =ip4=, =gw4=: Optional IP and gateway assignment.

This will persist the configuration and activate the VLAN interface
immediately. Next I moved on to Install the honeypot.

---

** Setup environment, install cowrie as container and adjust configuration
*** üêß Create a Dedicated User for Cowrie (No Login Shell)
Running the Podman container under a dedicated system user with no login shell
is a recommended security best practice. Reasons include:

- **Privilege Separation:**  
  Isolates the container from other system processes and users, limiting
  the potential impact of a compromise.

- **Reduced Attack Surface:**  
  The user has no login shell (e.g., =/usr/sbin/nologin=), meaning it can't be
  used to log into the system interactively.

- **Auditing & Logging:**  
  Helps distinguish container activity in system logs and process lists,
  making monitoring easier.

- **Least Privilege Principle:**  
  The user has only the permissions necessary to run the container ‚Äî nothing more.

**1. Create the 'cowrie' user (no home directory, no login shell)**
#+begin_src sh
sudo useradd --system --no-create-home --shell /usr/sbin/nologin cowrie
#+end_src

**2. Create necessary directories and set ownership**
#+begin_src sh
  sudo mkdir -p /opt/cowrie/etc
  sudo mkdir -p /opt/cowrie/var
  sudo mkdir -p /opt/cowrie/var/log/cowrie
  sudo chown -R cowrie:cowrie /opt/cowrie
#+end_src

*** üê≥ Pull and Configure Cowrie with Podman

**3. As the cowrie user, pull the container image**
#+begin_src bash
sudo -u cowrie podman pull docker.io/cowrie/cowrie
#+end_src

**4. Copy default config file into persistent volume**
#+begin_src bash
sudo -u cowrie podman run --rm localhost/cowrie_honeypot:latest \
  cat /cowrie/cowrie-git/etc/cowrie.cfg.dist > /opt/cowrie/etc/cowrie.cfg
#+end_src


*** üõ† cowrie.cfg ‚Äì Basic Overview

The `cowrie.cfg` file is the main configuration for **Cowrie**, the SSH/Telnet
honeypot we use. It uses INI-style syntax and is divided into sections. Each section
begins with a header like *[section_name]*.

***** üìÅ Key Sections & Settings
**[ssh]**
- Enable or disable SSH/Telnet and set the port to listen on::
  #+begin_src sh
  enabled = true
  listen_port = 2222
  #+end_src

**[honeypot]**
- Set honeypot host name and logpath properties:
  #+begin_src sh
    hostname = cowrie-host
 
    # Directory where to save log files in.
    log_path = var/log/cowrie
  #+end_src

- Define login behavior:
  #+begin_src sh
  auth_class = AuthRandom
  auth_class_parameters = 1, 5, 10
  #+end_src

  I use AuthRandom here which causes to allow access after "randint(2,5)"
  attempts. This means the threat actor will fail with some logins and some
  will be logged in immediately. 

**[output_jsonlog]**
- Configure logging and output plugins:
  #+begin_src sh
  [output_jsonlog]
  enabled = true
  logfile = ${honeypot:log_path}/cowrie.json
  epoch_timestamp = false
  #+end_src
  This sets the default log location in the file-system, this is important so that file beat later can
  pickup on the juicy honeypot log files.

This is the whole configuration needed to run the honeypot.

_üìå Notes_
- Restart Cowrie after configuration changes.
- The configuration can be split across multiple `.cfg` files in `cowrie.cfg.d/` for modular setup.

*** üöÄ Run Cowrie Container as 'cowrie' User

Once I had created the dedicated system user (see earlier section), I
was able to run the Cowrie container with Podman using =sudo -u= and UID mapping.

**** Step-by-Step Command explanation

#+begin_src bash
sudo -u cowrie podman run -d --name cowrie \
  --uidmap 0:999:1001 \
  -v /opt/cowrie/etc:/cowrie/cowrie-git/etc:Z \
  -v /opt/cowrie/var:/cowrie/cowrie-git/var:Z \
  -p 2222:2222 \
  cowrie/cowrie
#+end_src

**** Explanation

- =sudo -u cowrie=: Runs the Podman command as the unprivileged =cowrie= user.
- =--uidmap 0:999:1001=: Maps root (UID 0) **inside** the container to the =cowrie= UID on the host.
- =-v /opt/cowrie/etc= and =/opt/cowrie/var=: Mounts configuration and data volumes from the host with `:Z` to apply correct SELinux labels (optional on systems without SELinux).
- =-p 2222:2222=: Forwards port 2222 from host to container (Cowrie's SSH honeypot port).
- =cowrie/cowrie=: The container image name (use latest or specific tag as needed).
  
**** Benefits:

- **Container runs as non-root on the host:**  
  Even if a process inside the container thinks it's root, it's actually limited to the unprivileged =cowrie= user outside the container.

- **Enhanced security:**  
  If the container is compromised, the attacker only gets access as the =cowrie= user ‚Äî not real root.

- **Avoids root-equivalent risks:**  
  Prevents privilege escalation or access to sensitive host files and devices.
  
*** üéØ Operating the Honeypot

- **View logs**
  I think to know how to debug the container is important so we start first
  with the logs: 

  #+begin_src sh
    sudo -u cowrie podman logs -f cowrie
    ...snip...
    [HoneyPotSSHTransport,14,10.0.2.100] Closing TTY Log: var/lib/cowrie/tty/e52d9c508c502347344d8c07ad91cbd6068afc75ff6292f062a09ca381c89e71 after 0.8 seconds
    [cowrie.ssh.connection.CowrieSSHConnection#info] sending close 0
    [cowrie.ssh.session.HoneyPotSSHSession#info] remote close
    [HoneyPotSSHTransport,14,10.0.2.100] Got remote error, code 11 reason: b'disconnected by user'
    [HoneyPotSSHTransport,14,10.0.2.100] avatar root logging out
    [cowrie.ssh.transport.HoneyPotSSHTransport#info] connection lost
    [HoneyPotSSHTransport,14,10.0.2.100] Connection lost after 2.8 seconds
    ...snip...
  #+end_src

- **Restart container**
  If things go left just restart that thing: 
  #+begin_src bash
  sudo -u cowrie podman restart cowrie
  #+end_src

  In the logs you can see that cowrie is running and accepting SSH connections:
  #+begin_src sh
    ...snip...
    [-] CowrieSSHFactory starting on 2222
    [cowrie.ssh.factory.CowrieSSHFactory#info] Starting factory <cowrie.ssh.factory.CowrieSSHFactory object at 0x7fb66f26d0>
    [-] Ready to accept SSH connections
    ...snip...
  #+end_src
  When the log says "Ready to accept SSH connections" I tested if I could login:

  #+begin_src sh
    ssh 192.168.210.3 -p 2222 -l root
    root@192.168.210.3 password: 

    The programs included with the Debian GNU/Linux system are free software;
    the exact distribution terms for each program are described in the
    individual files in /usr/share/doc/*/copyright.

    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
    permitted by applicable law.
    root@svr04:~# uname -a
    Linux svr04 3.2.0-4-amd64 #1 SMP Debian 3.2.68-1+deb7u1 x86_64 GNU/Linux
    root@svr04:~#
  #+end_src

- **Stop container**
  Nothing special here:
  #+begin_src bash
    sudo -u cowrie podman stop cowrie
  #+end_src
---
*** üîÑ Automatically Restart Cowrie Podman Container with systemd

To keep your Cowrie container running reliably and restart it if it stops, use a systemd service with restart policies.
Please make sure to double check this part on your side as I am no systemd
expert at all, for me this just worked.

**** Step 1: Generate a systemd Service File

Create `/etc/systemd/system/cowrie-container.service` with the following
content:
You can create the systemd file with the command:
#+begin_src sh
sudo -u cowrie podman generate systemd --name cowrie --files --restart-policy=on-failure  
#+END_SRC

The resulting file looks somewhat like this
#+begin_src sh
  # container-cowrie.service
  # autogenerated by Podman 4.3.1
  # Fri Sep 19 10:27:47 CEST 2025

  [Unit]
  Description=Podman container-cowrie.service
  Documentation=man:podman-generate-systemd(1)
  Wants=network-online.target
  After=network-online.target
  RequiresMountsFor=/run/user/1001/containers

  [Service]
  User=cowrie
  Group=cowrie
  Restart=on-failure
  Environment=PODMAN_SYSTEMD_UNIT=%n
  Restart=on-failure
  TimeoutStopSec=70
  ExecStart=/usr/bin/podman start -a cowrie
  ExecStop=/usr/bin/podman stop -t 10 cowrie
  ExecStopPost=/usr/bin/podman stop -t 10 cowrie
  Type=forking

  [Install]
  WantedBy=default.target
#+end_src

- The `--restart-policy=on-failure` makes systemd restart the container if it exits with a failure.

**** Step 2: Enable the Service
#+begin_src bash
  sudo systemctl daemon-reload
  sudo systemctl enable --now cowrie-container.service
#+end_src

**** Step 3: (Optional) Add a Health Check Script

To detect if Cowrie stops accepting connections even if the container is still running, create a health check script running as =cowrie=:

Create `/usr/local/bin/check_cowrie.sh`:

#+begin_src bash
  #!/bin/bash
  if ! nc -z localhost 2222; then
    echo "Cowrie not responding, restarting container"
    /usr/bin/podman restart cowrie
    /usr/local/bin/pushover.sh "Cowrie was restarted!"
  fi
#+end_src

This restarts the service and sends out a notification via pushover.

Make it executable:

#+begin_src bash
sudo chmod +x /usr/local/bin/check_cowrie.sh
sudo chown cowrie:cowrie /usr/local/bin/check_cowrie.sh
#+end_src

Create systemd service `/etc/systemd/system/check_cowrie.service`:

#+begin_src ini
[Unit]
Description=Check Cowrie honeypot health

[Service]
User=cowrie
Group=cowrie
Type=oneshot
ExecStart=/usr/local/bin/check_cowrie.sh
#+end_src

Create systemd timer `/etc/systemd/system/check_cowrie.timer`:

#+begin_src ini
[Unit]
Description=Run Cowrie health check every minute

[Timer]
OnBootSec=1min
OnUnitActiveSec=1min
Unit=check_cowrie.service

[Install]
WantedBy=timers.target
#+end_src

Enable and start the timer:

#+begin_src bash
sudo systemctl daemon-reload
sudo systemctl enable --now check_cowrie.timer
#+end_src

---

**** Summary

- Used Podman‚Äôs systemd integration for automatic restart on container failure.
- Added a health check timer to detect if Cowrie stops accepting connections and restart proactively.

*** üîí Security Notes
- The `cowrie` user has no login shell (`/usr/sbin/no login`)
- Running Cowrie isolated via Podman increases containment
- All files are owned by `cowrie`, no root access required for normal operation

  ---

** Log Forwarding with Filebeat
*** üì¶ Install Filebeat on Ubuntu

**1. Add Elastic‚Äôs GPG key and repository**
#+begin_src bash
curl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elastic.gpg

echo "deb [signed-by=/usr/share/keyrings/elastic.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | \
  sudo tee /etc/apt/sources.list.d/elastic-8.x.list
#+end_src

**2. Update APT and install Filebeat**
#+begin_src bash
sudo apt install filebeat
#+end_src

*** ‚öô Configure and test Filebeat

**3. Edit Filebeat config**
#+begin_src bash
sudo mg /etc/filebeat/filebeat.yml
#+end_src
The filebeat config is straight forward. You have to write a filebeat.input
block which contains the path where the logfiles are you need to ingest. And
at the end the log-destination (logstash) so that filebeat knows where to send
the logs to: 
#+begin_src yaml
  filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /opt/cowrie/var/log/cowrie/cowrie.json
    json.keys_under_root: true
    json.add_error_key: true
    fields:
      source: cowrie
    fields_under_root: true

  output.logstash:
    hosts: ["192.168.123.5:5044"]
#+end_src

**4. (Optional) Test Filebeat config**
#+begin_src bash
  sudo filebeat test config
  logstash: 192.168.210.5:5044...
  connection...
    parse host... OK
    dns lookup... OK
    addresses: 192.168.210.5
    dial up... OK
  TLS... WARN secure connection disabled
  talk to server... OK
#+end_src

*** üöÄ Start and Enable Filebeat

**5. Enable and start Filebeat**
#+begin_src bash
  sudo systemctl enable filebeat
  sudo systemctl daemon-reload
  sudo systemctl start filebeat
#+end_src

**6. Check Filebeat status and logs**
#+begin_src bash
sudo systemctl status filebeat
sudo journalctl -u filebeat -f
#+end_src
---
** üéØ TL;DR ‚Äì What Did We Just Do?

**1. We deployed Cowrie like pros.**
- Ran it safely in a Podman container under a non-login user.
- No mess, no root, no regrets.

**2. Logs? Sorted.**
- Filebeat scooped up Cowrie‚Äôs logs and shipped them to Elasticsearch.
- Now we can actually *see* who's knocking on the honeypot door.

**3. Everything‚Äôs persistent.**
- Configs and logs live outside the container. Cowrie forgets nothing‚Äîeven after a reboot.

**4. Setup is clean and modular.**
- Each part (Cowrie, Filebeat, Elasticsearch) does its job. 
- Break one, fix one‚Äîno domino disasters.

**5. It‚Äôs nerdy, useful, and kinda fun.**
- Now I built a mini threat intel system.
- Now I can sit back, sip coffee, and watch the kiddies play.

** Whats next
Next I build the [[~/../honeyhttpd][HTTP Honeypot]]

#+BEGIN_EXPORT md
{{< giscus >}}
#+END_EXPORT


* DONE Threathunting III: HTTP Honeypot develop and setup                               :honeypot:@forensic:
CLOSED: [2025-08-13 Mi 06:35]
:PROPERTIES:
:EXPORT_AUTHOR: Dirk
:EXPORT_HUGO_FRONT_MATTER_FORMAT: yaml
:HUGO_TITLE: honeyhttpd
:EXPORT_OPTIONS: toc:2
:EXPORT_FILE_NAME: honeyhttpd
:EXPORT_DATE: 2019-01-11T16:00:00-05:00
:CUSTOM_ID: honeyhttpd
:END:

** Introduction
:PROPERTIES:
:CUSTOM_ID: introduction
:END:
*** Brief overview of the use case
:PROPERTIES:
:CUSTOM_ID: brief-overview-of-the-use-case
:END:
I recently set out to ingest web traffic data into my SIEM solution,
which requires data to be ingested in a specific format. After
researching various options, I sought an easy-to-use solution that could
integrate with our existing Elasticsearch setup. One tool that caught my
attention was HoneyPot HTTPD.

As I researched potential solutions, I realized that many of them
required manual configuration and scripting to ingest web data into
Elasticsearch. However, HoneyPot HTTPD offered a simple and elegant way
to do so through its built-in ingestion feature. This was especially
appealing since I wanted to integrate the web traffic data with our
existing SIEM setup that utilized Elasticsearch.

In particular, I needed a tool that could collect web traffic data and
forward it to a centralized location for analysis and processing.
Honeypot HTTPD's ability to ingest web data into Elasticsearch made it
an attractive choice, as it would allow me to leverage our existing
Elasticsearch infrastructure and integrate the data with our SIEM
solution seamlessly.

With this in mind, I set out to explore how to use HoneyPot HTTPD to
ingest web traffic data into Elasticsearch. In the following sections,
I'll walk you through the steps I took to configure HoneyPot HTTPD for
ingestion, including the Dockerfile used to build the container and any
additional configuration settings required.

** Setting up HoneyPot HTTPD for Web Data
:PROPERTIES:
:CUSTOM_ID: setting-up-honeypot-httpd-for-web-data-ingestion
:END:
*** Containerizing the application to run inside docker
:PROPERTIES:
:CUSTOM_ID: containerizing-the-application-to-run-inside-docker
:END:

- Creating a Dockerfile
  
  I started by creating a Dockerfile that would build the HoneHTTPD
  image. The Dockerfile included the following instructions:

  #+begin_src sh
    # Use python base image
    FROM python:3

    # Set environment 
    ARG APP_NAME=honeyhttpd
    ENV APP_NAME=${APP_NAME}

    ARG USER_ID="10001"
    ARG GROUP_ID="app"
    ARG HOME="/app"

    ENV HOME=${HOME}

    # Create user and environment
    RUN groupadd --gid ${USER_ID} ${GROUP_ID} && \
        useradd --create-home --uid ${USER_ID} --gid ${GROUP_ID} --home-dir /app ${GROUP_ID}


    # Install dependencies 
    RUN apt-get update && \
        apt-get install -y --no-install-recommends \
            file        \
            gcc         \
            libwww-perl curl unzip && \
        apt-get autoremove -y && \
        apt-get clean

    # Set workdir 
    WORKDIR ${HOME}

    # Copy config files and certs into container
    COPY ./requirements.txt .
    COPY ./config.json .
    COPY ./server*.pem .
    COPY ./ca.crt . 
    COPY honeyhttpd logs servers util .
    COPY start.py .

    # Upgrade python packages and install dependencies
    RUN pip3 install --upgrade pip
    RUN pip3 install virtualenv
    RUN python3 -m virtualenv ${HOME} && \
    virtualenv ${HOME}
    RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel elasticsearch==8.13.0 && \
    pip3 install --no-cache-dir --upgrade -r ./requirements.txt && pip3 install -r ./requirements.txt

    ADD . ${HOME}

    # Remove compilers
    RUN apt-get remove gcc --purge -y

        # Drop root and change ownership of the application folder to the user
    RUN chown -R ${USER_ID}:${GROUP_ID} ${HOME}
    USER ${USER_ID}

    # Expose Honeypot ports to outside world
     EXPOSE 8443:8443

    # run cowrie with config
    CMD ["python3", "start.py", "--config", "config.json"]
  #+end_src

  In this Dockerfile, I:

  - Installed necessary dependencies, including Python and pip
  - Installed the required packages, including HoneyPot HTTPD
  - Set the working directory to /usr/local/bin to run the application
    from
  - Exposed port 80 for HTTP traffic
  - Copied the configuration file (config.yaml) into the container
  - Specified the command to run HoneyPot HTTPD with the -c option,
    which points
  - to the configuration file

- Building and Running the Container

  Once I had created the Dockerfile, I built the image by running the
  following command:

  #+begin_src sh
    sudo docker build -t honeyhttpd .
  #+end_src

  This command told Docker to create an image with the tag honeyhttpd
  using the instructions in the Dockerfile.To run the container, I used
  the following command:

  #+begin_src sh
    sudo docker run --hostname honeyhttpd -p 8443:8443 honeyhttpd
  #+end_src

  This command started a new container from the honeyhttpd image and
  mapped port 8443 on the host machine to the port 8443 in the container.
  
- Configuring the Container
  
  To configure the honeypot, I updated the config.yaml file to point to
  my Elasticsearch instance. Here's an example of what the configuration
  file might look like:

  #+begin_src json
    "loggers": {
        "ElasticSearchLogger": {
            "active": true,
            "config": {
                "server": "https://192.168.210.95:9200",
                "verify_certs": true,
                "username": "elastic",
                "password": "SecretPassword",
                "index": "cowrie.webhoneypot",
            }
        }
  #+end_src

  the server config itself is quite simplistic:

  #+BEGIN_SRC json
  "servers" : [
	{"handler": "ApachePasswordServer", "mode": "https", "port": 8443, "domain": "cooldomain.com", "timeout": 10, "cert_path": "server_cert.pem", "key_path": "server_key.pem"},
    ],
    "user": "nobody",
    "group": "nogroup"
}
  #+end_src

  This configuration told HoneyPot HTTPD to forward web traffic data to
  my Elasticsearch instance, where it could be processed and stored.

  For the cert_path and key_path we earlier copied the self signed cert and
  key to the container. 

  With the container running and configured, I was now ready to test
  HoneyPot HTTPD's ability to ingest web traffic data into
  Elasticsearch.

  Which I did with just opening https://honeypot.home.arpa:8443 in my
  webbrowser. Which gave me the htpasswd auth prompt. 
  
*** üöÄ Code adjustments for our environment

I started from the original ApachePasswordServer in honeyhttpd, which was
fairly minimal‚Äîit simply responded with a 401 on selected paths and captured
credentials in a rudimentary way. I overhauled it to better structure logging,
extract metadata, and sanitize inputs before sending logs to Elasticsearch.

Below, I explain each change with commentary.

**** üîê Improvements in ApachePasswordServer.py:

This update significantly extends the functionality of ApachePasswordServer.py.
It builds on the original honeyhttpd implementation by enhancing its ability to simulate
Basic Authentication, extract and decode credentials from the `Authorization` header,
and log structured metadata about each HTTP request and response.

It now integrates tightly with an `ElasticSearchLogger`, providing enriched, sanitized logs
for further analysis or visualization. Custom helper functions ensure safe parsing,
while connection and client metadata offer greater context to the captured events.

***** üìä Summary of Changes

- 2 files changed: ApachePasswordServer.py and Elasticsearchlogger.py
- Key improvements:
  - Simulation of Basic Auth (401 challenge on sensitive paths)
  - Credential extraction and decoding from `Authorization` header
  - Connection metadata collection (IP, port, useragent etc.)
  - Header parsing with case-insensitive lookup
  - Structured request and response logging
  - Integration with `ElasticSearchLogger`
  - Safer JSON serialization and error handling
Here I describe how I extended the honeypot server to improve credential
logging and integrate with Elasticsearch for structured logging.

Starting from a basic server that simply issued 401 responses,
I added features to parse HTTP requests, decode Basic Auth headers,
and enrich logs with request and connection metadata. This makes the
server far more useful for DFIR research and threat hunting.

---

**** Original baseline (for reference)

Big thanks to the great ground wrok from bocajspear1
over at github with the [[https://github.com/bocajspear1/honeyhttpd][honeyHTTPD]] Server. That way
I did not had to write all from scratch. But I still had to
make some improvements in order to use the honeypot in my Environment.
#+begin_src python
from servers.ApacheServer import ApacheServer
import honeyhttpd.lib.encode as encode

class ApachePasswordServer(ApacheServer):
    def on_request(self, handler):
        return None, None

    def on_GET(self, path, headers):
        if path == "/" or path == "/index.php" or path == "/admin":
            return 401, [],  "Basic realm=\"Secure Area\""
        return 404, [], ""

    def on_POST(self, path, headers, post_data):
        return 404, [], ""

    def on_error(self, code, headers, message):
        return code, [("Connection", "close"), ("Content-Type", "text/html; charset=iso-8859-1")], message

    def on_complete(self, client, code, req_headers, res_headers, request, response):
        extra = {}
        for header in req_headers:
            if header[0].lower() == "authorization":
                auth_split = header[1].split(" ")
                if len(auth_split) > 1:
                    auth_data = auth_split[1]
                    extra['creds'] = encode.decode_base64(auth_data)
        self.log(client, request, response, extra)

    def default_headers(self):
        return []
#+end_src


**** üÜï Auto-Injection of ElasticSearchLogger in __init__()
To ensure consistent structured logging, `ElasticSearchLogger` is
now injected into the logger stack if not already present.

#+begin_src diff
+ if loggers is None:
+     loggers = []
+ if not any(isinstance(logger, ElasticSearchLogger) for logger in loggers):
+     loggers.append(ElasticSearchLogger())
#+end_src

This avoids missing logs if the user forgets to pass a logger during instantiation.

---

**** üîê New GET Handler Simulates Apache Basic Auth Challenge
The server now returns `401 Unauthorized` and prompts for credentials on common admin paths.

#+begin_src diff
+ def on_GET(self, path, headers):
+     if path in ["/", "/index.php", "/admin"]:
+         return 401, [], 'Basic realm="Secure Area"'
+     return 404, [], ""
#+end_src

This turns the honeypot into a credential trap for automated brute-forcers and scanners.

---

**** üß∞ New Helper Functions for Header Parsing and Auth Decoding
I had to introduce two utility functions:

- parse_to_json() transforms header tuples into a JSON dictionary.
- decode_basic_auth() decodes Base64 credentials and validates them.

#+begin_src python
def parse_to_json(data):
    return json.dumps({key: value for key, value in data})

def decode_basic_auth(b64_string):
    try:
        decoded_bytes = base64.b64decode(b64_string, validate=True)
        decoded_str = decoded_bytes.decode('utf-8')
        if ':' in decoded_str:
            return decoded_str
        else:
            return "[invalid format: missing colon]"
    except Exception as e:
        return f"[decode error: {e}]"
#+end_src

These enable safe and consistent parsing for incoming HTTP headers.

---

**** üì¶ Structured Request Parsing & Credential Extraction in on_complete()
I had to completely rework the on_complete() method to:

- Parse the HTTP request line
- Convert headers to a JSON object
- Extract relevant metadata and credentials
- Store all data in `req_dict`, passed to the logger

#+begin_src diff
-        extra = {}
+        req_dict = {}
...
+        lines = request.split('\n')
+        first_line = lines[0].strip()
+        parts = first_line.split()
+        requested_url = parts[1] if len(parts) > 1 else ""
+        method = parts[0] if len(parts) > 0 else ""
+        req_dict['request_body'] = requested_url
+        req_dict['method'] = method
+        req_dict['code'] = code
+
+        try:
+            req_output = parse_to_json(req_headers)
+            parsed_req = json.loads(req_output)
+        except Exception as e:
+            parsed_req = {}
+
+        for key in ['Host', 'User-Agent', 'Accept', 'Accept-Language',
+                    'Accept-Encoding', 'Authorization']:
+            req_dict[key] = parsed_req.get(key, '')
+
+        auth = parsed_req.get('Authorization', '')
+        if auth.startswith("Basic "):
+            try:
+                auth_data = auth.split(" ", 1)[1]
+                decoded_creds = encode.decode_base64(auth_data)
+                req_dict['creds'] = decoded_creds
+            except Exception as e:
+                req_dict['creds'] = f"[decode error: {e}]"
#+end_src

This prepares the logs to include useful hunting metadata for later analysis.

---

**** üåê Enriched Connection Metadata Logging
Additional context is logged to req_dict, including:

- Remote IP and port
- SSL usage @@comment: this can be removed as it is not needed@@
- Listening port
- HTTP response code
- Response headers

#+begin_src diff
+        remote_ip = client[0] if isinstance(client, tuple) else ''
+        remote_port = client[1] if isinstance(client, tuple) else ''
+        is_ssl = getattr(self, 'is_ssl', False)
+        port = getattr(self, 'port', '8843')
+
+        req_dict['remote_ip'] = remote_ip
+        req_dict['remote_port'] = remote_port
+        req_dict['is_ssl'] = is_ssl
+        req_dict['port'] = port
+        req_dict['response_headers'] = res_dict
+
+        self.log(client, request, response, res_dict, req_dict)
#+end_src

This provides rich forensic data for Elasticsearch or Splunk pipelines.

---

**** üîë Highlights

- **üÜï Auto-injection of `ElasticSearchLogger`**  
  Ensures logs are never silently dropped, even if no logger is passed explicitly.

- **üîê Basic Auth Simulation with 401 Challenge**  
  Returns `401 Unauthorized` on suspicious paths to bait scanners.

- **üß∞ Safe Parsing & Decoding with Helpers**  
  New functions parse_to_json() and decode_basic_auth() added for reliability.

- **üì¶ Structured Logging in on_complete()**  
  Fully rewritten to extract metadata, decode credentials, and prepare logs.

- **üåê Rich Connection Context**  
  Logs IP, port, and full response headers for correlation.

**** üß† Use Case

These changes upgrade ApachePasswordServer.py from a toy honeypot to a serious
data source for threat hunting. It can now be safely deployed in research
environments, logging attack metadata in structured formats ideal for analysis
pipelines like Elasticsearch + Kibana or Splunk.

*** üìù Conclusion
:PROPERTIES:
:CUSTOM_ID: v-dot-conclusion
:END:
**** üîë Key points about the changes in honeyhttpd
:PROPERTIES:
:CUSTOM_ID: recap-of-key-points-about-using-honeypot-httpd-to-ingest-web-data-into-elasticsearch
:END:

This article detailed how I enhanced ApachePasswordServer.py and
ElasticSearchLogger to make the simple honeypot into a powerful
tool for capturing attacker behavior. 

I added Structured parsing of HTTP requests and responses combined with
integration into ElasticSearchLogger allowed to generate rich, queryable logs,
complete with client metadata, HTTP headers, and decoded credentials.

This setup not only captures raw data but also organizes it in a way that
facilitates downstream analysis using Elasticsearch or similar log management
platforms.

**** üí° Final thoughts

Deploying this enhanced honeypot within your environment equips you with
detailed, actionable insights into attacker tactics and techniques.
The ability to collect and analyze credential attempts and associated metadata
improves detection and supports incident response efforts.

By bridging the gap between mere detection and detailed forensic logging, this
solution empowers proactive threat hunting and accelerates the identification
of emerging attack patterns. Integrating it into your security operations stack
can thus significantly boost your detection and defense capabilities.

_TODO_ - Extensive testing of the honeypot, there will be an article coming up
how to do automated honeypot testing with Zap proxy, stay tuned!

_TODO_ - Still need to deploy the honeypot to my honeypot server so that I can
see how it operates in a live scenario. If you faster than me let me know you
results 

_TODO_ - Upload the new honeyhttpd code to a git repo and ask honeyhttpd author
if they interessted in a merge request from my code.

Next, we will talk about Setting up the ELK Server and how to ingest data.
Stay tuned!

* DRAFT Threathunting IV: Setup ELK Server and ingest data                                :@threathunting:visibility:
* DRAFT Testing webhoneypots with ZAP in server mode                                    :@threathunting:
