var relearn_searchindex = [
  {
    "breadcrumb": "Welcome",
    "content": "Welcome to my technical blog and knowledge base!\nTopics üñ• Threathunting Tutorials üñ• All things OpenBSD ‚Äì\u003e Get in Touch Suggestions or feedback?\nContact me here or visit the project repository.\nYou can also subscribe via RSS.",
    "description": "Latest posts",
    "tags": [],
    "title": "Forensic wheels",
    "uri": "/posts/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Welcome to my technical blog and knowledge base!\nTopics Coming soon\nLatest posts Threathunting I: Network setup 08.07.2025 Threat hunting II: SSH Honeypot setup 13.07.2025 Threathunting III: HTTP Honeypot develop and setup 11.01.2019 SANS FOR608 26.05.2025 OpenBSD and Zen 07.07.2025 Get in Touch Suggestions or feedback?\nContact me here or visit the project repository.\nYou can also subscribe via RSS.",
    "description": "Latest posts",
    "tags": [],
    "title": "Welcome",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction Why I Built a Home Lab for Threat Hunting üïµ Network Setup Topology, Hardware and Tools üõ† Firewall configurationüß± Switch configuration What I Learned Whats next Introduction This is a small series I wanted to start, where I write about my small threathunting setup and describe a little what I build and what I am doing with it.\nIn this part, I will describe the Network setup for my Environment, more about how I build the honeypots and the ELK Server I will describe in the follow up articles about threathunting.\nKeep in mind this is for Education and fun, no serious stuff going on here.\nWhy I Built a Home Lab for Threat Hunting üïµ The threat landscape is constantly evolving, with new attack vectors, tools, and tactics appearing almost daily.\nAnd to keep my skills current with real-world threats, I built a home lab dedicated to threat hunting. This environment allows me to safely observe attacks and develop detection and defense methods. I deployed web and shell honeypots, and collect real threat data in a controlled setting.\nIt‚Äôs a practical, hands-on way to explore the behavior of adversaries and its a lot of fun!\nNetwork Setup Topology, Hardware and Tools üõ† For the hardware setup, I kept things lightweight and affordable by using Raspberry Pi devices and open-source tools. The honeypot is based on the well-known Cowrie SSH honeypot and the honeyhttpd HTTP honeypot . It runs on a Raspberry Pi 4 with 8GB of RAM, hosted inside a Docker üê≥ container. On the honeypot host, Filebeat is running to ingest the Cowrie logs into the ELK stack.\nFor the ELK stack, I used a Raspberry Pi 5 with 16GB of RAM, running Debian. The ELK services are also containerized using Docker. The stack is based on the DShield-SIEM project, which I customized to better fit my needs. I‚Äôll dive deeper into those modifications and the ELK setup in a follow-up article.\nThe network topology is straightforward but deliberately segmented. The router is connected to a managed switch, which is responsible for handling VLAN separation. Both the honeypot and the ELK server are connected to this switch and are placed in an isolated VLAN (VLAN210). This VLAN is dedicated exclusively to threat hunting, ensuring that any potentially malicious traffic remains fully contained and cannot interfere with the rest of the home network.\nMy client system üíª is the only machine allowed to connect from outside the VLAN to both the ELK server and the honeypot. This connection is strictly for maintenance and administrative purposes. The ELK server is allowed to access the internet, primarily to pull threat intelligence data from external sources and security feeds.\nIn contrast, the honeypot is completely blocked from internet access, with the exception of SSH and HTTP traffic going in and out of it. These are the only services deliberately exposed to simulate vulnerable endpoints. Communication between the honeypot and the ELK server is allowed for log ingestion and analysis. However, I intend to introduce stricter controls on this internal traffic in the future to further reduce the attack surface.\nFirewall configurationüß± For the pf(1) configuration It was as always with UNIX fairly easy to get to work:\nmatch in quick log on egress proto tcp from any to any port 22 flags S/SA rdr-to $honeypot port 2222 match in quick log on egress proto tcp from any to any port 443 flags S/SA rdr-to $honeypot port 4433 This rule makes sure any incoming TCP connection attempt to port 22 (SSH) and port 443 (HTTPS) is immediately intercepted, logged, and transparently redirected to the $honeypot server listening on port 2222 or 4433 for HTTPS Traffic.\nSwitch configuration Here you can see my managed switch configuration. Port 5 (honeypot) is only assigned to VLAN210 like port 5 too, port 2 is the router it needs to talk into both networks and at port 1 is my workstation to access the theathunting environment.\nWhat I Learned Building and maintaining this lightweight honeypot and monitoring setup on Raspberry Pi devices has been an insightful experience. Here are some key takeaways:\nResource Efficiency: Raspberry Pis provide a surprisingly capable platform for running complex services like Cowrie honeypot and the ELK stack in Docker containers, keeping costs and power consumption low.\nNetwork Segmentation Matters: Isolating the honeypot and ELK server in a dedicated VLAN (VLAN210) effectively contains malicious traffic, protecting the rest of the home network from potential threats.\nControlled Access Is Crucial: Restricting external access to only authorized clients and limiting the honeypot‚Äôs internet connectivity reduces the attack surface while still enabling useful data collection.\nLogging and Data Collection: Using Filebeat to ship logs from the honeypot to the ELK stack provides real-time visibility into attacker behavior, which is essential for threat hunting and incident response.\nCustomization Pays Off: Adapting existing tools and SIEM projects (like DShield) to specific needs improves effectiveness and allows for tailored threat detection.\nFuture Improvements: There is always room to tighten internal communication rules and harden the setup further to minimize risk and improve operational security.\nThis project highlights the balance between practical constraints and security needs, demonstrating that even modest hardware can contribute significantly to threat intelligence and network defense.\nI drew inspiration for this setup from the DShield SIEM project by SANS and would like to express my gratitude for their valuable work.\nWhats next Next I had to build the ssh honeypot and the HTTP Honeypot, stay tuned for the follow up!",
    "description": "Introduction Why I Built a Home Lab for Threat Hunting üïµ Network Setup Topology, Hardware and Tools üõ† Firewall configurationüß± Switch configuration What I Learned Whats next Introduction This is a small series I wanted to start, where I write about my small threathunting setup and describe a little what I build and what I am doing with it.",
    "tags": [
      "Threathunting",
      "Honeypot",
      "Visibility"
    ],
    "title": "Threathunting I: Network setup",
    "uri": "/posts/threathuntingnet/index.html"
  },
  {
    "breadcrumb": "Welcome",
    "content": "Intro Hi my Name is Dirk.L I am a Security Engineer with focus on forensics \u0026amp; Incident Response I live in Nuemberg. You can contact me only here over my github profile.\nExperience DeepL SE, K√∂ln ‚Äî Security Engineer Mar. 2021 ‚Äì Present\nDuring my 4.5-year tenure at DeepL, I developed a strong foundation in endpoint security and forensics. Key achievements include:\nManaging Wazuh EDR on a large scale (900+ hosts, ~900k entries/day) Designing and implementing security incident response playbooks and tools Performing system and user audits (LDAP, firewall, software) Developing Ansible roles and creating forensic PoCs Designing incident response processes and threat monitoring (e.g. honeypots, canary tokens) Conducting phishing simulations with 1000+ employees HiSolutions AG, Berlin ‚Äî Pentester Jan. 2020 ‚Äì Jan. 2021\nFocused on penetration testing methodologies and vulnerability assessment.\nConducting web penetration tests using Burp Suite Pro Infrastructure scans with Nessus and Metasploit Performing system audits and risk analyses Writing reports and providing recommendations Managing infrastructure with Ansible System EngineerSystem Engineer Nov. 2017 ‚Äì 2019\nDevelopment and Maintainance of the Monitoring Appliance based on Nagios/Icinga2 Customer Support / System debugging Developing automation Scripts with Python and Shellscript Noris Network AG Linux-Administrator 2012 ‚Äì 2014\nAutomantion Script development in python Monioring ang Customer Support System debugging Education \u0026 Background 2007 ‚Äì 2013: Beginnings as Sysadmin Completed apprenticeship as Computer Science Expert at M√∂bel Fischer GmbH (2007) Gained experience with Linux Enterprise systems Over 13 years of professional IT experience with a 20+ year passion for security Built skills in automation, problem-solving, and secure infrastructure Self-driven transition into IT Security through collaboration and continuous learning Skills Incident Response Threat Intelligence \u0026 Detection Penetration Testing (Web, Infrastructure, System) Forensic data analysis and tooling Log analysis and visualization Network security and compliance OpenBSD firewalls and router setup Scripting (Python, Bash, PowerShell) Unix/Linux Administration (RHEL, Ubuntu, *BSD) Network and System monitoring w. Nagios, Icinga2, monit, prometeus Languages German: native English: fluent",
    "description": "My Curriculum Vitae",
    "tags": [
      "Forensicwheels"
    ],
    "title": "Curriculum Vitae: Dirk.L",
    "uri": "/cv/index.html"
  },
  {
    "breadcrumb": "Welcome",
    "content": "Hi! I‚Äôm Dirk, a security engineer with a passion for Skateboarding and Forensics. I help my Company safeguard their networks and systems from the ever-evolving threats in the cybersecurity landscape.\nSkateboarding is not just my hobby - it‚Äôs my passion, my center, and my identity. For me, Skatelife is more than just lifestyle; it‚Äôs a way of life that embodies creativity, freedom, and community and Family. I‚Äôve been skating for years, and I still get that same feel every time I‚Äôm on my board.\nIn addition to my love for skateboarding, I‚Äôm also a big enthusiast of OpenBSD - the most secure, yet accessible operating system out there. I‚Äôve built my own OpenBSD-based router and infrastructure for threat hunting, which allows me to stay one step ahead in the game of cybersecurity. There‚Äôs something about the simplicity, elegance, and power of OpenBSD that resonates deeply with me. You can also read about the smilarities I see between OpenBSD and Zen\nAs a nerd, I have to mention my love affair with Emacs - my favorite editor. I‚Äôve been using it for over 12 years now, and I still can‚Äôt imagine working without it. Whether I‚Äôm writing code, weblogging, or just tinkering with snippets of text, Emacs is always by my side.\nAs I navigate the world of cybersecurity, I‚Äôm always looking for ways to improve my skills, stay ahead of the curve, and when I‚Äôm not geeking out over security patches or network protocols, you can find me:\nStay tuned for more updates on my journey as a security engineer, skateboarder, OpenBSD enthusiast, and Zen student",
    "description": "Short intro about myself",
    "tags": [
      "Forensicwheels",
      "Personal"
    ],
    "title": "About",
    "uri": "/about/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction What is Cowrie? Why Podman over Docker? Preconditions / System setup Ubuntu Installed on Raspberry Pi 4+ System Fully Updated Podman installed and working: VLAN Tagging Configured on Network Interface Setup environment, install cowrie as container and adjust configuration üêß Create a Dedicated User for Cowrie (No Login Shell) üê≥ Pull and Configure Cowrie with Podman üõ† cowrie.cfg ‚Äì Basic Overview üöÄ Run Cowrie Container as ‚Äòcowrie‚Äô User üéØ Operating the Honeypot üîÑ Automatically Restart Cowrie Podman Container with systemd üîí Security Notes Log Forwarding with Filebeat üì¶ Install Filebeat on Ubuntu ‚öô Configure and test Filebeat üöÄ Start and Enable Filebeat üéØ TL;DR ‚Äì What Did We Just Do? Whats next Introduction This post provides a brief walkthrough of how to deploy a lightweight, containerized SSH honeypot using Cowrie and Podman, with the goal of capturing and analyzing malicious activity as part of my threat hunting strategy.\nWhat is Cowrie? Cowrie is an interactive SSH and Telnet honeypot designed to emulate a real system, capturing attacker behavior in a controlled environment. It allows defenders and researchers to observe malicious activity without exposing actual infrastructure.\nKey capabilities of Cowrie include\nFull session logging: Records all commands entered by the attacker, along with input/output streams and timing data. Sessions can be saved as plaintext or in formats suitable for replay.\nFake file system and shell environment: Emulates a basic Linux shell with a user-modifiable file system. Attackers can navigate directories, read/write fake files, or attempt to download/upload payloads.\nCommand emulation: Supports a large set of common Unix commands (`ls`, `cat`, `wget`, etc.), allowing attackers to interact naturally, as if on a real system. And can be extended with more commands\nCredential logging: Captures usernames and passwords used in brute-force login attempts or interactive logins.\nFile download capture: Logs and optionally stores any files attackers attempt to retrieve via `wget`, `curl`, or similar tools.\nJSON-formatted logging and integration‚Äôs: Outputs structured logs that are easy to parse and ingest into systems like ELK, Splunk, or custom analysis pipelines.\nCowrie is widely used in research, threat intelligence, and proactive defense efforts to gather Indicators of Compromise (IOCs) and understand attacker tactics,techniques, and procedures (TTPs).\nWhy Podman over Docker? Podman offers several advantages over Docker, particularly in terms of security and system integration. It supports rootless containers, allowing users to run containers without elevated privileges, which reduces the attack surface.\nPodman is daemon-less, integrating more seamlessly with systemd and existing Linux workflows. Additionally, Podman is fully compatible with the Open Container Initiative (OCI) standards, ensuring interoperability and flexibility across container ecosystems.\nPreconditions / System setup Before I proceed with the cowrie setup, I made sure the following preconditions are met:\nUbuntu Installed on Raspberry Pi 4+ I am using a Raspberry Pi 4+ running Ubuntu\nSystem Fully Updated After installation, I made sure system is up to date:\nsudo apt update \u0026\u0026 sudo apt upgrade -y Podman installed and working: # Ubuntu 20.10 and newer sudo apt-get -y install podman Run the Hello World Container.In this moment I did not had the cowrie user yet setup so I used my system user to test\npodman run hello-world Trying to pull docker.io/library/hello-world:latest... ... Hello from Docker! This message shows that your installation appears to be working correctly. tho sometimes the pulling fails like that then I had to put `docker.io` in front of the container name like:\npodman run docker.io/hello-world then it would work for sure.\nVLAN Tagging Configured on Network Interface In my network setup for threathunting the honeypot requires VLAN tagging to configured to reachable from the outside, VLAN210 is my restricted Network. Therefore i needed to configure the vlan using nmcli so it‚Äôs persistent across reboots.\nExample: Create a VLAN interface (e.g., VLAN ID 210 on main if) sudo nmcli con add type vlan con-name vlan210 dev mainif id 210 ip4 192.168.210.3/24 gw4 192.168.210.1 sudo nmcli con up vlan210 con-name vlan210: Name of the new VLAN connection. dev mainif: Physical interface to tag. id 210: VLAN ID. ip4, gw4: Optional IP and gateway assignment. This will persist the configuration and activate the VLAN interface immediately. Next I moved on to Install the honeypot.\nSetup environment, install cowrie as container and adjust configuration üêß Create a Dedicated User for Cowrie (No Login Shell) Running the Podman container under a dedicated system user with no login shell is a recommended security best practice. Reasons include:\nPrivilege Separation: Isolates the container from other system processes and users, limiting the potential impact of a compromise.\nReduced Attack Surface: The user has no login shell (e.g., /usr/sbin/nologin), meaning it can‚Äôt be used to log into the system interactively.\nAuditing \u0026 Logging: Helps distinguish container activity in system logs and process lists, making monitoring easier.\nLeast Privilege Principle: The user has only the permissions necessary to run the container ‚Äî nothing more.\n1. Create the ‚Äòcowrie‚Äô user (no home directory, no login shell)\nsudo useradd --system --no-create-home --shell /usr/sbin/nologin cowrie 2. Create necessary directories and set ownership\nsudo mkdir -p /opt/cowrie/etc sudo mkdir -p /opt/cowrie/var sudo chown -R cowrie:cowrie /opt/cowrie üê≥ Pull and Configure Cowrie with Podman 3. As the cowrie user, pull the container image\nsudo -u cowrie podman pull docker.io/cowrie/cowrie 4. Copy default config file into persistent volume\nsudo -u cowrie podman run --rm cowrie/cowrie \\ cat /cowrie/cowrie-git/etc/cowrie.cfg.dist \u003e /opt/cowrie/etc/cowrie.cfg üõ† cowrie.cfg ‚Äì Basic Overview The `cowrie.cfg` file is the main configuration for Cowrie, the SSH/Telnet honeypot we use. It uses INI-style syntax and is divided into sections. Each section begins with a header like [section_name].\nüìÅ Key Sections \u0026 Settings\n[ssh] / [telnet]\nEnable or disable SSH/Telnet and set the port to listen on:: enabled = true listen_port = 2222 [honeypot]\nSet honeypot host name and logpath properties:\nhostname = cowrie-host # Directory where to save log files in. log_path = var/log/cowrie Define login behavior:\nauth_class = AuthRandom auth_class_parameters = 1, 5, 10 I use AuthRandom here which causes to allow access after ‚Äúrandint(2,5)‚Äù attempts. This means the threat actor will fail with some logins and some will be logged in immediately.\n[output_jsonlog]\nConfigure logging and output plugins: [output_jsonlog] enabled = true logfile = ${honeypot:log_path}/cowrie.json epoch_timestamp = false This sets the default log location in the file-system, this is important so that file beat later can pickup on the juicy honeypot log files. This is the whole configuration needed to run the honeypot.\nüìå Notes\nRestart Cowrie after configuration changes. The configuration can be split across multiple `.cfg` files in `cowrie.cfg.d/` for modular setup. üöÄ Run Cowrie Container as ‚Äòcowrie‚Äô User Once I had created the dedicated system user (see earlier section), I was able to run the Cowrie container with Podman using sudo -u and a secure UID mapping.\nStep-by-Step Command explanation sudo -u cowrie podman run -d --name cowrie \\ --uidmap 0:$(id -u cowrie):1 \\ -v /opt/cowrie/etc:/cowrie/cowrie-git/etc:Z \\ -v /opt/cowrie/var:/cowrie/cowrie-git/var:Z \\ -p 2222:2222 \\ cowrie/cowrie Explanation sudo -u cowrie: Runs the Podman command as the unprivileged cowrie user. --uidmap 0:$(id -u cowrie):1: Maps root (UID 0) inside the container to the cowrie UID on the host. -v /opt/cowrie/etc and /opt/cowrie/var: Mounts configuration and data volumes from the host with `:Z` to apply correct SELinux labels (optional on systems without SELinux). -p 2222:2222: Forwards port 2222 from host to container (Cowrie‚Äôs SSH honeypot port). cowrie/cowrie: The container image name (use latest or specific tag as needed). Benefits: Container runs as non-root on the host: Even if a process inside the container thinks it‚Äôs root, it‚Äôs actually limited to the unprivileged cowrie user outside the container.\nEnhanced security: If the container is compromised, the attacker only gets access as the cowrie user ‚Äî not real root.\nAvoids root-equivalent risks: Prevents privilege escalation or access to sensitive host files and devices.\nüéØ Operating the Honeypot View logs I think to know how to debug the container is important so we start first with the logs:\nsudo -u cowrie podman logs -f cowrie ...snip... [HoneyPotSSHTransport,14,10.0.2.100] Closing TTY Log: var/lib/cowrie/tty/e52d9c508c502347344d8c07ad91cbd6068afc75ff6292f062a09ca381c89e71 after 0.8 seconds [cowrie.ssh.connection.CowrieSSHConnection#info] sending close 0 [cowrie.ssh.session.HoneyPotSSHSession#info] remote close [HoneyPotSSHTransport,14,10.0.2.100] Got remote error, code 11 reason: b'disconnected by user' [HoneyPotSSHTransport,14,10.0.2.100] avatar root logging out [cowrie.ssh.transport.HoneyPotSSHTransport#info] connection lost [HoneyPotSSHTransport,14,10.0.2.100] Connection lost after 2.8 seconds ...snip... Restart container If things go left just restart that thing:\nsudo -u cowrie podman restart cowrie In the logs you can see that cowrie is running and accepting SSH connections:\n...snip... [-] CowrieSSHFactory starting on 2222 [cowrie.ssh.factory.CowrieSSHFactory#info] Starting factory \u003ccowrie.ssh.factory.CowrieSSHFactory object at 0x7fb66f26d0\u003e [-] Ready to accept SSH connections ...snip... When the log says ‚ÄúReady to accept SSH connections‚Äù I tested if I could login:\nssh 192.168.210.3 -p 2222 -l root root@192.168.210.3 password: The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. root@svr04:~# uname -a Linux svr04 3.2.0-4-amd64 #1 SMP Debian 3.2.68-1+deb7u1 x86_64 GNU/Linux root@svr04:~# Stop container Nothing special here:\nsudo -u cowrie podman stop cowrie üîÑ Automatically Restart Cowrie Podman Container with systemd To keep your Cowrie container running reliably and restart it if it stops, use a systemd service with restart policies.\nStep 1: Generate a systemd Service File Create `/etc/systemd/system/cowrie-container.service` with the following content:\n[Unit] Description=Cowrie Honeypot Podman Container After=network.target [Service] User=cowrie Group=cowrie Restart=on-failure RestartSec=10s ExecStart=/usr/bin/podman run -d --name cowrie \\ --uidmap 0:$(id -u cowrie):1 \\ -v /opt/cowrie/etc:/cowrie/cowrie-git/etc:Z \\ -v /opt/cowrie/var:/cowrie/cowrie-git/var:Z \\ -p 2222:2222 \\ cowrie/cowrie ExecStop=/usr/bin/podman stop -t 10 cowrie ExecStopPost=/usr/bin/podman rm cowrie ExecReload=/usr/bin/podman restart cowrie TimeoutStartSec=120 [Install] WantedBy=multi-user.target The `‚Äìrestart-policy=on-failure` makes systemd restart the container if it exits with a failure. Step 2: Enable the Service sudo systemctl daemon-reload sudo systemctl enable --now container-cowrie.service Step 3: (Optional) Add a Health Check Script To detect if Cowrie stops accepting connections even if the container is still running, create a health check script running as cowrie:\nCreate `/usr/local/bin/check_cowrie.sh`:\n#!/bin/bash if ! nc -z localhost 2222; then echo \"Cowrie not responding, restarting container\" /usr/bin/podman restart cowrie /usr/local/bin/pushover.sh \"Cowrie was restarted!\" fi This restarts the service and sends out a notification via pushover.\nMake it executable:\nsudo chmod +x /usr/local/bin/check_cowrie.sh sudo chown cowrie:cowrie /usr/local/bin/check_cowrie.sh Create systemd service `/etc/systemd/system/check_cowrie.service`:\n[Unit] Description=Check Cowrie honeypot health [Service] User=cowrie Group=cowrie Type=oneshot ExecStart=/usr/local/bin/check_cowrie.sh Create systemd timer `/etc/systemd/system/check_cowrie.timer`:\n[Unit] Description=Run Cowrie health check every minute [Timer] OnBootSec=1min OnUnitActiveSec=1min Unit=check_cowrie.service [Install] WantedBy=timers.target Enable and start the timer:\nsudo systemctl daemon-reload sudo systemctl enable --now check_cowrie.timer Summary Used Podman‚Äôs systemd integration for automatic restart on container failure. Added a health check timer to detect if Cowrie stops accepting connections and restart proactively. üîí Security Notes The `cowrie` user has no login shell (`/usr/sbin/no login`)\nRunning Cowrie isolated via Podman increases containment\nAll files are owned by `cowrie`, no root access required for normal operation\nLog Forwarding with Filebeat üì¶ Install Filebeat on Ubuntu 1. Add Elastic‚Äôs GPG key and repository\ncurl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elastic.gpg echo \"deb [signed-by=/usr/share/keyrings/elastic.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\" | \\ sudo tee /etc/apt/sources.list.d/elastic-8.x.list 2. Update APT and install Filebeat\nsudo apt update sudo apt install filebeat ‚öô Configure and test Filebeat 3. Edit Filebeat config\nsudo mg /etc/filebeat/filebeat.yml The filebeat config is straight forward. You have to write a filebeat.input block which contains the path where the logfiles are you need to ingest. And at the end the log-destination (logstash) so that filebeat knows where to send the logs to:\nfilebeat.inputs: - type: log enabled: true paths: - /opt/cowrie/var/log/cowrie/cowrie.json json.keys_under_root: true json.add_error_key: true fields: source: cowrie fields_under_root: true output.logstash: hosts: [\"192.168.123.5:5044\"] 4. (Optional) Test Filebeat config\nsudo filebeat test config üöÄ Start and Enable Filebeat 5. Enable and start Filebeat\nsudo systemctl enable filebeat sudo systemctl daemon-reload sudo systemctl start filebeat 6. Check Filebeat status and logs\nsudo systemctl status filebeat sudo journalctl -u filebeat -f üéØ TL;DR ‚Äì What Did We Just Do? 1. We deployed Cowrie like pros.\nRan it safely in a Podman container under a non-login user. No mess, no root, no regrets. 2. Logs? Sorted.\nFilebeat scooped up Cowrie‚Äôs logs and shipped them to Elasticsearch. Now we can actually see who‚Äôs knocking on the honeypot door. 3. Everything‚Äôs persistent.\nConfigs and logs live outside the container. Cowrie forgets nothing‚Äîeven after a reboot. 4. Setup is clean and modular.\nEach part (Cowrie, Filebeat, Elasticsearch) does its job. Break one, fix one‚Äîno domino disasters. 5. It‚Äôs nerdy, useful, and kinda fun.\nNow I built a mini threat intel system. Now I can sit back, sip coffee, and watch the kiddies play. Whats next Next I had to build the HTTP honeypot, stay tuned for the follow up!\nFeedback and Comments",
    "description": "Introduction What is Cowrie? Why Podman over Docker? Preconditions / System setup Ubuntu Installed on Raspberry Pi 4+ System Fully Updated Podman installed and working: VLAN Tagging Configured on Network Interface Setup environment, install cowrie as container and adjust configuration üêß Create a Dedicated User for Cowrie (No Login Shell) üê≥ Pull and Configure Cowrie with Podman üõ† cowrie.cfg ‚Äì Basic Overview üöÄ Run Cowrie Container as ‚Äòcowrie‚Äô User üéØ Operating the Honeypot üîÑ Automatically Restart Cowrie Podman Container with systemd üîí Security Notes Log Forwarding with Filebeat üì¶ Install Filebeat on Ubuntu ‚öô Configure and test Filebeat üöÄ Start and Enable Filebeat üéØ TL;DR ‚Äì What Did We Just Do? Whats next Introduction This post provides a brief walkthrough of how to deploy a lightweight, containerized SSH honeypot using Cowrie and Podman, with the goal of capturing and analyzing malicious activity as part of my threat hunting strategy.",
    "tags": [
      "Threathunting",
      "Honeypot"
    ],
    "title": "Threat hunting II: SSH Honeypot setup",
    "uri": "/posts/theathuntinghoneypot/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Introduction Brief overview of the use case Setting up HoneyPot HTTPD for Web Data Ingestion and adjust code for our needs Containerizing the application to run inside docker Code adjustments for our environment Ingesting Web Data into Elasticsearch with HoneyPot HTTPD Explanation of how to use the honeyhttpd command-line tool to ingest web data into Elasticsearch Example of how to configure the honeyhttpd output to match your desired Elasticsearch index structure Benefits and Use Cases Discussion of the benefits of using HoneyPot HTTPD for ingesting web data into Elasticsearch (e.g., improved threat detection, enhanced visibility) Real-world examples of use cases where this setup can be particularly effective (e.g., logging web traffic, monitoring online activity) Conclusion Recap of key points about using HoneyPot HTTPD to ingest web data into Elasticsearch Final thoughts on the value of this setup for your organization‚Äôs threat hunting or security operations. Introduction Brief overview of the use case I recently set out to ingest web traffic data into my SIEM solution, which requires data to be ingested in a specific format. After researching various options, I sought an easy-to-use solution that could integrate with our existing Elasticsearch setup. One tool that caught my attention was HoneyPot HTTPD.\nAs I researched potential solutions, I realized that many of them required manual configuration and scripting to ingest web data into Elasticsearch. However, HoneyPot HTTPD offered a simple and elegant way to do so through its built-in ingestion feature. This was especially appealing since I wanted to integrate the web traffic data with our existing SIEM setup that utilized Elasticsearch.\nIn particular, I needed a tool that could collect web traffic data and forward it to a centralized location for analysis and processing. Honeypot HTTPD‚Äôs ability to ingest web data into Elasticsearch made it an attractive choice, as it would allow me to leverage our existing Elasticsearch infrastructure and integrate the data with our SIEM solution seamlessly.\nWith this in mind, I set out to explore how to use HoneyPot HTTPD to ingest web traffic data into Elasticsearch. In the following sections, I‚Äôll walk you through the steps I took to configure HoneyPot HTTPD for ingestion, including the Dockerfile used to build the container and any additional configuration settings required.\nSetting up HoneyPot HTTPD for Web Data Ingestion and adjust code for our needs Containerizing the application to run inside docker Creating a Dockerfile\nI started by creating a Dockerfile that would build the HoneHTTPD image. The Dockerfile included the following instructions:\n# Use python base image FROM python:3 # Set environment ARG APP_NAME=honeyhttpd ENV APP_NAME=${APP_NAME} ARG USER_ID=\"10001\" ARG GROUP_ID=\"app\" ARG HOME=\"/app\" ENV HOME=${HOME} # Create user and environment RUN groupadd --gid ${USER_ID} ${GROUP_ID} \u0026\u0026 \\ useradd --create-home --uid ${USER_ID} --gid ${GROUP_ID} --home-dir /app ${GROUP_ID} # Install dependencies RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends \\ file \\ gcc \\ libwww-perl curl unzip \u0026\u0026 \\ apt-get autoremove -y \u0026\u0026 \\ apt-get clean # Set workdir WORKDIR ${HOME} # Copy config files and certs into container COPY ./requirements.txt . COPY ./config.json . COPY ./server*.pem . COPY ./ca.crt . COPY honeyhttpd logs servers util . COPY start.py . # Upgrade python packages and install dependencies RUN pip3 install --upgrade pip RUN pip3 install virtualenv RUN python3 -m virtualenv ${HOME} \u0026\u0026 \\ virtualenv ${HOME} RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel elasticsearch \u0026\u0026 \\ pip3 install --no-cache-dir --upgrade -r ./requirements.txt \u0026\u0026 pip3 install -r ./requirements.txt ADD . ${HOME} # Remove compilers RUN apt-get remove gcc --purge -y # Drop root and change ownership of the application folder to the user RUN chown -R ${USER_ID}:${GROUP_ID} ${HOME} USER ${USER_ID} # Expose Honeypot ports to outside world EXPOSE 8888:8888 EXPOSE 8889:8889 EXPOSE 8443:8443 # run cowrie with config CMD [\"python3\", \"start.py\", \"--config\", \"config.json\"] In this Dockerfile, I:\nUsed the official Ubuntu image as the base image Installed necessary dependencies, including Python and pip Installed the required packages, including HoneyPot HTTPD Set the working directory to /usr/local/bin to run the application from Exposed port 80 for HTTP traffic Copied the configuration file (config.yaml) into the container Specified the command to run HoneyPot HTTPD with the -c option, which points to the configuration file Building and Running the Container\nOnce I had created the Dockerfile, I built the image by running the following command:\ndocker build -t honeyhttpd . This command told Docker to create an image with the tag honeyhttpd using the instructions in the Dockerfile.To run the container, I used the following command:\ndocker run -p 80:80 honeyhttpd This command started a new container from the honeyhttpd image and mapped port 80 on the host machine to port 80 in the container.\nConfiguring the Container\nTo configure the container, I updated the config.yaml file to point to my Elasticsearch instance. Here‚Äôs an example of what the configuration file might look like:\n-ingest: es_host: \"localhost:9200\" es_index: \"logstash-2019.04\" es_type: \"log\" This configuration told HoneyPot HTTPD to forward web traffic data to my Elasticsearch instance, where it could be processed and stored.\nWith the container running and configured, I was now ready to test HoneyPot HTTPD‚Äôs ability to ingest web traffic data into Elasticsearch.\nCode adjustments for our environment üîê Major Improvements in `ApachePasswordServer.py`: Credential Capture, Logging, and Header Parsing This update significantly enhances `ApachePasswordServer.py` by simulating Basic Authentication, extracting credentials from the `Authorization` header, and enriching log data with structured request and response metadata. It also ensures integration with `ElasticSearchLogger` and introduces helper functions for safer parsing and decoding.\nüìä Summary of Changes\n1 file changed: `ApachePasswordServer.py` ~120 insertions, ~10 deletions Key improvements: Basic Auth simulation (401 challenge) Credential harvesting from Authorization header Integration with `ElasticSearchLogger` Structured logging with metadata (IP, method, headers) üÜï Auto-Injection of `ElasticSearchLogger` in `_init_()`\nTo ensure consistent structured logging, `ElasticSearchLogger` is now injected into the logger stack if not already present.\n+ if loggers is None: + loggers = [] + if not any(isinstance(logger, ElasticSearchLogger) for logger in loggers): + loggers.append(ElasticSearchLogger()) This avoids missing logs if the user forgets to pass a logger during instantiation.\nüîê New GET Handler Simulates Apache Basic Auth Challenge\nThe server now returns `401 Unauthorized` and prompts for credentials on common admin paths.\n+ def on_GET(self, path, headers): + if path in [\"/\", \"/index.php\", \"/admin\"]: + return 401, [], 'Basic realm=\"Secure Area\"' + return 404, [], \"\" This turns the honeypot into a credential trap for automated brute-forcers and scanners.\nüß∞ New Helper Functions for Header Parsing and Auth Decoding\nTwo utility functions were introduced:\n`parse_to_json()` transforms header tuples into a JSON dictionary. `decode_basic_auth()` decodes Base64 credentials and validates them. def parse_to_json(data): return json.dumps({key: value for key, value in data}) def decode_basic_auth(b64_string): try: decoded_bytes = base64.b64decode(b64_string, validate=True) decoded_str = decoded_bytes.decode('utf-8') if ':' in decoded_str: return decoded_str else: return \"[invalid format: missing colon]\" except Exception as e: return f\"[decode error: {e}]\" These enable safe and consistent parsing for incoming HTTP headers.\nüì¶ Structured Request Parsing \u0026 Credential Extraction in `on_complete()`\nThe `on_complete()` method has been completely reworked to:\nParse the HTTP request line Convert headers to a JSON object Extract relevant metadata and credentials Store all data in `req_dict`, passed to the logger - extra = {} + req_dict = {} ... + lines = request.split('\\n') + first_line = lines[0].strip() + parts = first_line.split() + requested_url = parts[1] if len(parts) \u003e 1 else \"\" + method = parts[0] if len(parts) \u003e 0 else \"\" + req_dict['request_body'] = requested_url + req_dict['method'] = method + req_dict['code'] = code + + try: + req_output = parse_to_json(req_headers) + parsed_req = json.loads(req_output) + except Exception as e: + parsed_req = {} + + for key in ['Host', 'User-Agent', 'Accept', 'Accept-Language', + 'Accept-Encoding', 'Authorization']: + req_dict[key] = parsed_req.get(key, '') + + auth = parsed_req.get('Authorization', '') + if auth.startswith(\"Basic \"): + try: + auth_data = auth.split(\" \", 1)[1] + decoded_creds = encode.decode_base64(auth_data) + req_dict['creds'] = decoded_creds + except Exception as e: + req_dict['creds'] = f\"[decode error: {e}]\" This prepares your logs to include useful hunting metadata for later analysis.\nüåê Enriched Connection Metadata Logging\nAdditional context is logged to `req_dict`, including:\nRemote IP and port SSL usage Listening port HTTP response code Response headers + remote_ip = client[0] if isinstance(client, tuple) else '' + remote_port = client[1] if isinstance(client, tuple) else '' + is_ssl = getattr(self, 'is_ssl', False) + port = getattr(self, 'port', '8843') + + req_dict['remote_ip'] = remote_ip + req_dict['remote_port'] = remote_port + req_dict['is_ssl'] = is_ssl + req_dict['port'] = port + req_dict['response_headers'] = res_dict + + self.log(client, request, response, res_dict, req_dict) This provides rich forensic data for Elasticsearch or Splunk pipelines.\n‚úÖ Result\nThese changes turn `ApachePasswordServer` into a more useful honeypot component for DFIR or threat hunting research.\nIt now supports:\nCredential collection from Basic Auth attacks Well-structured logs for easy ingestion Full IP/session/request context per event Easier extensibility for new headers or paths You‚Äôre now well-positioned to integrate this into a broader threat hunting or research stack.\nIngesting Web Data into Elasticsearch with HoneyPot HTTPD Explanation of how to use the honeyhttpd command-line tool to ingest web data into Elasticsearch Example of how to configure the honeyhttpd output to match your desired Elasticsearch index structure Benefits and Use Cases Discussion of the benefits of using HoneyPot HTTPD for ingesting web data into Elasticsearch (e.g., improved threat detection, enhanced visibility) Real-world examples of use cases where this setup can be particularly effective (e.g., logging web traffic, monitoring online activity) Conclusion Recap of key points about using HoneyPot HTTPD to ingest web data into Elasticsearch Final thoughts on the value of this setup for your organization‚Äôs threat hunting or security operations.",
    "description": "Introduction Brief overview of the use case Setting up HoneyPot HTTPD for Web Data Ingestion and adjust code for our needs Containerizing the application to run inside docker Code adjustments for our environment Ingesting Web Data into Elasticsearch with HoneyPot HTTPD Explanation of how to use the honeyhttpd command-line tool to ingest web data into Elasticsearch Example of how to configure the honeyhttpd output to match your desired Elasticsearch index structure Benefits and Use Cases Discussion of the benefits of using HoneyPot HTTPD for ingesting web data into Elasticsearch (e.g., improved threat detection, enhanced visibility) Real-world examples of use cases where this setup can be particularly effective (e.g., logging web traffic, monitoring online activity) Conclusion Recap of key points about using HoneyPot HTTPD to ingest web data into Elasticsearch Final thoughts on the value of this setup for your organization‚Äôs threat hunting or security operations. Introduction Brief overview of the use case I recently set out to ingest web traffic data into my SIEM solution, which requires data to be ingested in a specific format. After researching various options, I sought an easy-to-use solution that could integrate with our existing Elasticsearch setup. One tool that caught my attention was HoneyPot HTTPD.",
    "tags": [
      "Threathunting",
      "Honeypot"
    ],
    "title": "Threathunting III: HTTP Honeypot develop and setup",
    "uri": "/posts/honeyhttpd/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "Enterprise Threat hunting and Response (FOR608) Introduction Brief overview of forensic analysis and its application Course Overview/ Preparing your Index Proactive Detection and Response (608.1) Scaling Response and Analysis (608.2) Modern Attacks against Windows and Linux DFIR (608.3) Analyzing macOS and Docker Containers (608.4) Cloud Attacks and Response (608.5) Capstone: Enterprise-Class IR Challenge Key Takeaways Summary of key concepts and skills learned during the course learning outcomes and their application in real-world scenarios Conclusion and Recommendations Summary of overall effectiveness of the SANS Forensics course for608 Recommendations for future students looking to learn forensic analysis skills Enterprise Threat hunting and Response (FOR608) Course description from SANS¬†1 :\nFOR608: Enterprise-Class Incident Response \u0026 Threat Hunting focuses on identifying and responding to incidents too large to focus on individual machines. By using example tools built to operate at enterprise-class scale, students learn the techniques to collect focused data for incident response and threat hunting, and dig into analysis methodologies to learn multiple approaches to understand attacker movement and activity across hosts of varying functions and operating systems by using an array of analysis techniques.\nIntroduction Brief overview of forensic analysis and its application Forensic analysis in computer science investigates digital evidence to solve cybercrimes and security incidents. In enterprise environments, it involves analyzing devices, networks, and cloud storage. Key applications include incident response, compliance with regulations, investigations, and predictive analytics.\nTools like Timesketch, Velociraptor or Wireshark, and cloud forensics platforms aid in the analysis. Collaboration between IT and law enforcement is also crucial for successful investigations.\nThe goal of forensic analysis is to reconstruct events, identify perpetrators, and determine damage extent, ensuring organizations can respond effectively to security threats and maintain compliance with regulations.\nCourse Overview/ Preparing your Index The course was booked by my employer in the on demand version, so I got access to the SANS on demand platform, so I could learn self paced. For good preparation, I read this guides on how to create a exam index:\nhttps://tisiphone.net/2015/08/18/giac-testing/ https://www.muratbekgi.com/indexing-giac/ The exams are open book and so you have to create a index for:\nIt helps you quickly locate answers in your official SANS course books. It saves valuable time during the exam. personalized knowledge map It reinforces your understanding while building it The core of the index is a sorted list of terms, concepts, or attack types, with book and page numbers e.g:\nTerm Book Page Active Directory 608.1 45 ARP Spoofing 608.2 112 Buffer Overflow 608.5 16 XOR Encryption 608.4 154 Proactive Detection and Response (608.1) The FOR608 course start with discussing current cyber defense concerns and the importance of collaboration among incident responders and threat hunters. There is a emphasize to use to shared knowledge from sources like the MITREATT\u0026CK framework and further explores the concept of active defense, like the use of honeypots, honey tokens, and canaries to slow down attackers and facilitate detection.\nFor case of a compromise, the materials focus on efficiently handling of intrusions, by covering topics such as leading the response, managing team members, documenting findings, and communicating with stakeholders.\nAurora documentation tool is introduced as a means for tracking the investigation phases from initial detection to remediation.\nLater chapter dives into a scenario where an alert gets triggered in a company network, then in the labs triage data is analyzed using Timesketch, a powerful platform for scalable and collaborative analysis of forensic data.\nAdditionally, techniques are shared for visualising the same data set with Kibana, which offers capabilities such as creating dashboards and saved searches to aid analysis.\nThe Chapter concludes by examining key threat intelligence concepts, including developing and implementing internal threat intelligence. External projects like MITRE ATT\u0026CK and Sigma are leveraged, and two comprehensive threat intel platforms, MISP and OpenCTI, are introduced.\nA threat intel report on the adversary targeting Stark Research Labs is used for intelligence to kick off the investigation into potential signs of intrusion in the company.\nScaling Response and Analysis (608.2) The course continues from chapter 1 by focusing on response actions. The Instructors show how to collect evidence at scale to scope a potential intrusion by leveraging EDR tooling data from EDR Solutions like Sysmon.\nHowever, they also discuss common bypass techniques that attackers use to evade EDR technology. To aid in this analysis, Velociraptor is introduced as a powerful platform for incident response and threat hunting.\nThen the chapter continuses to show how Velociraptor can collect forensic artifacts from across the enterprise and provide deep-dive capabilities into individual hosts of interest. Additionally, Elasticsearch is used to ingest and process data from various tools, allowing for fast searches and aggregations. I also learned about rapid response options for targeted data collections at scale using tools like Velociraptor and CyLR. Finally, we got solutions shown that are used for quickly processing acquired data for analysis in tools like Timesketch and individual artifact review.\nModern Attacks against Windows and Linux DFIR (608.3) In the third chapter of the course the focus shifts from network-based analysis to classic host-based forensic artifact analysis. The start is to discuss modern attack techniques on Windows systems, including the infamous ransomware and ‚Äúliving-of-the-land‚Äù (LOTB) attacks that avoid detection by using built-in binaries and scripts.\nThe use of Sigma rules is highlighted as a way to facilitate rapid detection and response.\nThe chapter covers Linux incident response and analysis too, by starting with common vulnerabilities and exploits targeting Linux systems. Then it dives into DFIR fundamentals on Linux systems, including key concepts such as differences among Linux distributions and filesystems, and strategies for handling initial triage and deeper forensic analysis. The chapter concludes by providing best practices for hardening Linux systems and enhancing logging configurations to aid future investigations.\nAnalyzing macOS and Docker Containers (608.4) Now the focus went on to Apple macOS incident response, building on the foundation we got established earlier. This part includes understanding the history, ecosystem, and details of the Apple Filesystem (APFS), file structure, and important file types such as Property List (plist) configuration files. A discussion of challenges and opportunities in responding to macOS incidents follows, covering topics like acquiring disk and triage data, reviewing acquisitions, and identifying suspicious activity in logs and artifacts.\nThis part of the course then transitions to containerized microservices and Docker analysis, focusing on the architecture and management of Docker containers and providing a specific triage workflow for quick and effective response against individual containers as well as the container host.\nCloud Attacks and Response (608.5) This part focused on incident response in major cloud platforms from Microsoft and Amazon, covering log analysis techniques, architecture designs, and automation initiatives that can be applied across various cloud providers. It highlights unique challenges and opportunities in cloud environments, particularly through the use of the MITRE ATT\u0026CK framework‚Äôs Cloud Matrix.\nIn-depth discussion follows on Microsoft 365 (M365) and Azure, including popular SaaS offerings like Entra ID, Exchange, SharePoint, and Teams, as well as common attack scenarios against these platforms. The importance of log analysis is emphasized strongly, particularly in identifying suspicious user logon and email activity from Unified Audit Logs.\nThe course then addresses the Recovery phase, covering security enhancements to detect or prevent similar attacks in the future for M365 and Azure.\nNext, it delves into Amazon Web Services (AWS), covering its general architecture and components, as well as numerous logs and services providing critical detection and analysis data for responders. Discussions focus on architecting for response in the cloud, including setting up security accounts for a secure enclave within AWS, using template VMs (AMIs) for analysis, and automating IR tasks with AWS Lambda and Step Functions.\nCapstone: Enterprise-Class IR Challenge The final section of the course is the capstone exercise that allows students to apply their knowledge by working on a simulated breach scenario. They will receive a dataset from a compromised environment that spans multiple host operating systems and cloud environments, and use tools and techniques learned throughout the course to uncover the steps of the breach.\nKey Takeaways Summary of key concepts and skills learned during the course During the SANS FOR608 course, I learned concepts and skills that enabled me to do more effective incident response and coordination, including enterprise-level incident detection and to deploy threat hunting strategies. The course covered large-scale event correlation and timeline analysis techniques to identify patterns and trends in incidents, as well as multi-platform artifact analysis for incident response.\nSpecifically, I gained hands-on experience analyzing artifacts from various platforms, including Windows devices, Linux systems, macOS devices, containerized environments, and cloud-based infrastructure. This comprehensive training has equipped me with the knowledge and tools needed to detect, analyze, and respond to complex threats in enterprise environments.\nThe most fun was the parts where we learned about Timesketch and Velociraptor, I think each of those tools individually is extremely powerful, but when you integrate them into your threathunting / Response stack I thing they are of great benefit.\nlearning outcomes and their application in real-world scenarios Based on the provided course materials, I have analyzed my learning outcomes and their application in real-world scenarios. Through my analysis, I have gained a deeper understanding of the key concepts and skills required for effective cloud response and analysis, container DFIR fundamentals, detecting modern attacks, enterprise incident response management, enterprise visibility and incident scoping, foundational cloud concepts, Linux DFIR fundamentals, macOS DFIR fundamentals, macOS essentials, rapid response triage at scale.\nI have also gained practical knowledge of how to correlate large volumes of data to identify patterns and trends in incidents.\nIn particular, my experience with cloud-based infrastructure has highlighted the need for a comprehensive understanding of foundational cloud concepts, including popular cloud services that enterprises use to support business operations. I have also gained familiarity with common data source types in an enterprise environment and strategies to aggregate telemetry from disparate resources.\nMy analysis of learning outcomes suggests that effective application of these skills requires a combination of technical expertise, analytical thinking, and communication skills. By mastering these skills, I am confident in my ability to respond effectively to complex incidents and provide value to organizations as a security professional.\nConclusion and Recommendations Summary of overall effectiveness of the SANS Forensics course for608 SANS FOR608 course is a comprehensive training program which provides responders with a strong foundation in incident response, threat hunting, and digital forensic analysis. Through its curriculum, the course covers concepts and skills related to managing incident response teams, detecting threats in enterprise environments using advanced analytics tools, correlating large volumes of data to identify patterns and trends in incidents, analyzing artifacts from various platforms including Windows devices, Linux systems, macOS devices, containerized environments, and cloud-based infrastructure.\nAnalysis:\nComprehensive coverage: The course covers a wide range of topics related to incident response and digital forensic analysis, providing students with a comprehensive understanding of the subject matter. Hands-on experience: The course includes hands-on labs that allow participants to apply their knowledge in real-world scenarios, which helps to reinforce learning and improve retention. Practical skills: The course emphasizes practical skills over theoretical concepts, which is beneficial for security professionals who need to respond to incidents in a timely and effective manner. And I also think that pactical knowledge is more interessting to learn, because you can apply it in the following labs Real-world relevance: The course covers topics that are relevant to real-world scenarios responders are confronted with, making it easier for students to apply their knowledge in practical settings. Summary:\nFrom my personal opinion the SANS FOR608 course is very effective for providing students with a very well understanding of incident response and digital forensic analysis. Through its comprehensive coverage, hands-on exercises, and emphasis on practical skills, the course provides security professionals with the knowledge and skills needed to respond effectively to incidents.\nOverall, the course is well-structured, engaging, and relevant to real-world scenarios, making it an excellent choice for individuals looking to improve their incident response and digital forensic analysis skills.\nTho I have to say the on-demand course is way more exhausting I belive than the in person class. Also I think in person is more benificial beause you can discuss matters with your peers.\nRecommendations for future students looking to learn forensic analysis skills Gain Practical Experience Before enrolling in a forensic analysis course, try to gain as much practical experience as possible for example practicing Sherlocks on hack the box or try yourself in Malware analysis challanges This could also involve setting up your own home lab, participating in bug bounty programs, or volunteering to help a friend or family member with their computer issues. The more hands-on experience you have, the better equipped you‚Äôll be to learn and apply forensic analysis skills.\nDevelop Your Analytical Skills Forensic analysis requires strong analytical skills, including attention to detail, critical thinking, and problem-solving. Practice these skills by working on puzzles, brain teasers, or other activities that challenge your mind. You can also try analyzing data sets, network traffic logs, or system logs to develop your skills.\nLearn about Cloud Computing As a forensic analyst, it‚Äôs essential to understand cloud computing and how it affects the analysis of digital evidence. Take online courses or attend webinars that teach you about cloud security, compliance, and investigation techniques. This will help you stay up-to-date with the latest trends and technologies.\nFamiliarize Yourself with Linux and macOS Linux and macOS are popular operating systems used by many organizations, including those in the finance, healthcare, and government sectors. Take online courses or attend workshops that teach you about these operating systems, including their command-line interfaces, file systems, and security features.\nJoin Online Communities Joining online communities, such as Reddit‚Äôs r/learnprogramming or r/netsec, can be a great way to connect with other professionals in the field, ask questions, and learn from their experiences. You can also participate in online forums, attend webinars, or join online study groups to stay updated on the latest forensic analysis techniques.\nConsider Specializing in a Specific Area Forensic analysis is a broad field that encompasses many areas, including computer forensics, mobile device forensics, and digital evidence collection. Consider specializing in a specific area that interests you the most, such as incident response or threat hunting. This will help you develop deeper knowledge and skills in that area.\nStay Up-to-Date with Industry Developments The field of forensic analysis is constantly evolving, with new technologies and techniques emerging regularly. Stay up-to-date with industry developments by attending conferences, webinars, or online courses that focus on the latest trends and advancements.\nhttps://www.sans.org/cyber-security-courses/enterprise-incident-response-threat-hunting/¬†‚Ü©Ô∏é",
    "description": "Enterprise Threat hunting and Response (FOR608) Introduction Brief overview of forensic analysis and its application Course Overview/ Preparing your Index Proactive Detection and Response (608.1) Scaling Response and Analysis (608.2) Modern Attacks against Windows and Linux DFIR (608.3) Analyzing macOS and Docker Containers (608.4) Cloud Attacks and Response (608.5) Capstone: Enterprise-Class IR Challenge Key Takeaways Summary of key concepts and skills learned during the course learning outcomes and their application in real-world scenarios Conclusion and Recommendations Summary of overall effectiveness of the SANS Forensics course for608 Recommendations for future students looking to learn forensic analysis skills Enterprise Threat hunting and Response (FOR608) Course description from SANS¬†1 :",
    "tags": [
      "Forensicwheels",
      "Honeypot",
      "Canarytokens"
    ],
    "title": "SANS FOR608",
    "uri": "/posts/sans_for608/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Forensic wheels",
    "content": "About As someone who is passionate about security and has an interest in Unix operating systems, OpenBSD particularly captivates due to its dedication to security, stability, and simplicity. In comparison to other OSes, what sets OpenBSD apart? And how do these principles align with my journey through Zen meditation?\nAt first glance, OpenBSD and Zen may appear to be vastly disparate concepts - one being a potent operating system, while the other is a spiritual practice originating from ancient China. However, as I delved deeper into both realms, I uncovered some fascinating similarities.\nSimplicity and Clarity In Zen, simplicity is key to achieving inner clarity and balance. By stripping away unnecessary complexity, OpenBSD aims to create a stable and secure foundation for users. Similarly, in meditation, simplicity helps to quiet the mind and focus on the present moment. This alignment between OpenBSD‚Äôs philosophy and Zen practices extends to their shared emphasis on mindfulness and deliberate decision-making, fostering an environment of security and tranquility in both realms.\nAttention to Detail Both OpenBSD and Zen underscore the significance of attending to detail. In software development, this entails meticulously crafting each line of code to guarantee stability and security. In Zen practice, it involves paying close attention to one‚Äôs breath, posture, and mental state to attain a state of mindfulness. By zeroing in on these details, both OpenBSD and Zen strive for perfection.\nThe Power of Consistency OpenBSD‚Äôs dedication to consistency is manifested in its codebase, where each code change undergoes a thorough code review process. Consistency holds equal importance in Zen practice, as it fosters a sense of routine and stability. By cultivating a consistent daily meditation practice, I have discovered that consistency is instrumental in making progress on my spiritual journey. OpenBSD‚Äôs emphasis on consistency mirrors the principles of Zen, emphasizing the value of diligence and discipline in both domains.\nThe Beauty of Imperfection Finally, both OpenBSD and Zen acknowledge the elegance in imperfection. In software development, imperfections can often be rectified or lessened through meticulous design and testing. In Zen practice, imperfections are perceived as avenues for growth and self-awareness.\nBy acknowledging our imperfections, we can nurture humility and compassion. As I progress in my journey with OpenBSD and Zen, I am consistently struck by the ways in which these two seemingly unrelated realms intersect. By embracing simplicity, attention to detail, consistency, and the beauty of imperfection, both OpenBSD and Zen provide unique perspectives on the nature of software development and personal growth. Stay tuned for further insights from my exploration in the realm of security!\nFeedback and Comments",
    "description": "About As someone who is passionate about security and has an interest in Unix operating systems, OpenBSD particularly captivates due to its dedication to security, stability, and simplicity. In comparison to other OSes, what sets OpenBSD apart? And how do these principles align with my journey through Zen meditation?\nAt first glance, OpenBSD and Zen may appear to be vastly disparate concepts - one being a potent operating system, while the other is a spiritual practice originating from ancient China. However, as I delved deeper into both realms, I uncovered some fascinating similarities.",
    "tags": [
      "Forensicwheels",
      "Openbsd",
      "Zen"
    ],
    "title": "OpenBSD and Zen",
    "uri": "/posts/openbsdzen/index.html"
  },
  {
    "breadcrumb": "Welcome",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Honeypot",
    "uri": "/tags/honeypot/index.html"
  },
  {
    "breadcrumb": "Welcome",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Threathunting",
    "uri": "/tags/threathunting/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category - Threathunting",
    "uri": "/categories/threathunting/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Visibility",
    "uri": "/tags/visibility/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Forensicwheels",
    "uri": "/tags/forensicwheels/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Openbsd",
    "uri": "/tags/openbsd/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category - Personal",
    "uri": "/categories/personal/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Zen",
    "uri": "/tags/zen/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Personal",
    "uri": "/tags/personal/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag - Canarytokens",
    "uri": "/tags/canarytokens/index.html"
  },
  {
    "breadcrumb": "Welcome¬†\u003e¬†Categories",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Category - Forensic",
    "uri": "/categories/forensic/index.html"
  }
]
